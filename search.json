[
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "devtools::load_all()\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(performance) # diagnostic\nlibrary(car) # general utilities\nlibrary(MuMIn) # model selection\nlibrary(patchwork)"
  },
  {
    "objectID": "labs/lab1.html#footnotes",
    "href": "labs/lab1.html#footnotes",
    "title": "Lab 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe script has been adapted from the Prof. Paolo Girardi (A.Y. 2021/2022) document↩︎"
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "devtools::load_all() # if using the rproject dowloaded from the slides\n# source(\"utils-glm.R\") # if using a standard setup\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(car) # general utilities\nlibrary(effects) # for extracting and plotting effects \nlibrary(emmeans) # for marginal means\nlibrary(patchwork)\n\n\ndata(\"drop\")\ndat &lt;- drop\n\n\nOverview\nThis dataset dropout.csv contains data about dropouts during high school for nrow(dat) adolescents. We want to understand the impact of the parenting style (permissive, neglectful, authoritative, authoritarian) and the academic performance (high, low) on the probability of dropout (0 = no dropout, 1 = dropout).\n\nImporting data and overall check\nExploratory data analysis of predictors and the relationships between predictors and the number of words\nCompute the odds ratio manually comparing the academic performances for each parenting style\nModel fitting with glm() using the dataset in the binary form\nModel fitting with glm() using the dataset in the aggregated form\nPlotting and interpreting effects of both models\n\nis there any difference? try to understand why\n\nWrite a brief paragraph reporting the effects with your interpretation\n\n\n\n1. Importing data and overall check\n\nstr(dat)\n\n'data.frame':   500 obs. of  4 variables:\n $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ parenting: chr  \"permissive\" \"neglectful\" \"authoritative\" \"neglectful\" ...\n $ academic : chr  \"high\" \"low\" \"high\" \"low\" ...\n $ drop     : int  0 0 0 1 0 0 0 0 0 0 ...\n\n\nCheck for NA values:\n\nsapply(dat, function(x) sum(is.na(x)))\n\n       id parenting  academic      drop \n        0         0         0         0 \n\n\nEverything seems good, we do not have NA values.\nLet’s convert categorical variables into factor setting the appropriate order:\n\nparenting: neglectful, permissive, authoritative, authoritarian\nacademic: low, high\n\n\ndat$parenting &lt;- factor(dat$parenting, levels = c(\"neglectful\",\n                                                  \"permissive\",\n                                                  \"authoritative\",\n                                                  \"authoritarian\"))\ndat$academic &lt;- factor(dat$academic, levels = c(\"low\", \"high\"))\n\nlevels(dat$parenting)\n\n[1] \"neglectful\"    \"permissive\"    \"authoritative\" \"authoritarian\"\n\nlevels(dat$academic)\n\n[1] \"low\"  \"high\"\n\n\n\n\n2. Exploratory data analysis\n\nsummary(dat) # not really meaningful\n\n       id                parenting   academic        drop      \n Min.   :  1.0   neglectful   :119   low :237   Min.   :0.000  \n 1st Qu.:125.8   permissive   :152   high:263   1st Qu.:0.000  \n Median :250.5   authoritative:124              Median :0.000  \n Mean   :250.5   authoritarian:105              Mean   :0.166  \n 3rd Qu.:375.2                                  3rd Qu.:0.000  \n Max.   :500.0                                  Max.   :1.000  \n\n\nWith categorical variables we need to use absolute/relative frequencies and contingency tables.\nLet’s start by univariate EDA:\n\n# distribution of parenting styles\ntable(dat$parenting)\n\n\n   neglectful    permissive authoritative authoritarian \n          119           152           124           105 \n\ntable(dat$parenting)/nrow(dat)\n\n\n   neglectful    permissive authoritative authoritarian \n        0.238         0.304         0.248         0.210 \n\n# distribution of academic performance\ntable(dat$academic)\n\n\n low high \n 237  263 \n\ntable(dat$academic)/nrow(dat)\n\n\n  low  high \n0.474 0.526 \n\n# overall dropout rate\ntable(dat$drop)\n\n\n  0   1 \n417  83 \n\ntable(dat$drop)/nrow(dat)\n\n\n    0     1 \n0.834 0.166 \n\n# mean(dat$drop) # directly\n\nLet’s create an overall plot:\n\nplt_par &lt;- dat |&gt; \n  ggplot(aes(x = parenting)) +\n  geom_bar()\n\nplt_academic &lt;- dat |&gt; \n  ggplot(aes(x = academic)) +\n  geom_bar()\n\nplt_drop &lt;- dat |&gt; \n  ggplot(aes(x = factor(drop))) +\n  geom_bar()\n\nplt_par / plt_academic / plt_drop\n\n\n\n\nHow to interpret?\nLet’s now explore the bivariate relationships:\n\ntable(dat$parenting, dat$academic)\n\n               \n                low high\n  neglectful     95   24\n  permissive     77   75\n  authoritative  20  104\n  authoritarian  45   60\n\ntable(dat$academic, dat$parenting)\n\n      \n       neglectful permissive authoritative authoritarian\n  low          95         77            20            45\n  high         24         75           104            60\n\n\nWe can create tables with relative frequencies:\n\nprop.table(table(dat$parenting, dat$academic), 1) # by row\n\n               \n                      low      high\n  neglectful    0.7983193 0.2016807\n  permissive    0.5065789 0.4934211\n  authoritative 0.1612903 0.8387097\n  authoritarian 0.4285714 0.5714286\n\nprop.table(table(dat$parenting, dat$academic), 2) # by column\n\n               \n                       low       high\n  neglectful    0.40084388 0.09125475\n  permissive    0.32489451 0.28517110\n  authoritative 0.08438819 0.39543726\n  authoritarian 0.18987342 0.22813688\n\n\n…and some plots:\n\ndat |&gt; \n  ggplot(aes(x = academic, fill = parenting)) +\n  geom_bar(position = position_dodge(),\n           col = \"black\")\n\n\n\n\nOf course, we can compute the relative frequencies in multiple ways (total, row or column wise).\nThen the bivariate relationships with the drop variable:\n\ntable(dat$parenting, dat$drop)\n\n               \n                  0   1\n  neglectful     76  43\n  permissive    136  16\n  authoritative 117   7\n  authoritarian  88  17\n\nprop.table(table(dat$parenting, dat$drop), 1)\n\n               \n                         0          1\n  neglectful    0.63865546 0.36134454\n  permissive    0.89473684 0.10526316\n  authoritative 0.94354839 0.05645161\n  authoritarian 0.83809524 0.16190476\n\ntable(dat$academic, dat$drop)\n\n      \n         0   1\n  low  174  63\n  high 243  20\n\nprop.table(table(dat$academic, dat$drop), 1)\n\n      \n                0          1\n  low  0.73417722 0.26582278\n  high 0.92395437 0.07604563\n\n\nAnd the plots:\n\nbarplot(prop.table(table(dat$parenting, dat$drop), 1), \n        beside = TRUE,\n        col = c(\"firebrick\", \"lightblue\", \"darkgreen\", \"pink\"))\n\nlegend(7, 0.8, legend = levels(dat$parenting), \n       fill = c(\"firebrick\", \"lightblue\", \"darkgreen\", \"pink\"))\n\n\n\nbarplot(prop.table(table(dat$parenting, dat$drop), 1), \n        beside = TRUE,\n        col = c(\"firebrick\", \"lightblue\", \"darkgreen\", \"pink\"))\n\nlegend(7, 0.8, legend = levels(dat$parenting), \n       fill = c(\"firebrick\", \"lightblue\", \"darkgreen\", \"pink\"))\n\n\n\nbarplot(prop.table(table(dat$academic, dat$drop), 1), \n        beside = TRUE,\n        col = c(\"red\", \"blue\"))\n\nlegend(4, 0.5, legend = levels(dat$academic), \n       fill =  c(\"red\", \"blue\"))\n\n\n\n\nFinally we can represent the full relationship:\n\ndat |&gt; \n  group_by(parenting, academic) |&gt; \n  summarise(drop = mean(drop)) |&gt; \n  ggplot(aes(x = parenting, y = drop, color = academic, group = academic)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nComments? Main effects? Interactions?\n\n\n3. Compute the odds ratio manually comparing the academic performances for each parenting style\nFirstly we compute the probability of dropout for each category:\n\nagg &lt;- aggregate(drop ~ parenting + academic, FUN = mean, data = dat)\nagg\n\n      parenting academic       drop\n1    neglectful      low 0.37894737\n2    permissive      low 0.19480519\n3 authoritative      low 0.15000000\n4 authoritarian      low 0.20000000\n5    neglectful     high 0.29166667\n6    permissive     high 0.01333333\n7 authoritative     high 0.03846154\n8 authoritarian     high 0.13333333\n\n\nThen we can compute the odds of the probabilities and the odds ratios\n\nodds &lt;- function(p) p / (1 - p)\nagg$odds &lt;- odds(agg$drop)\n\nors &lt;- agg$odds[agg$academic == \"high\"] / agg$odds[agg$academic == \"low\"]\nnames(ors) &lt;- unique(agg$parenting)\nors\n\n   neglectful    permissive authoritative authoritarian \n   0.67483660    0.05585586    0.22666667    0.61538462 \n\n\nComments?\n\n\n4. Model fitting with glm() using the dataset in the binary form\nLet’s start fitting the null model:\n\nfit0 &lt;- glm(drop ~ 1, data = dat, family = binomial(link = \"logit\"))\nsummary(fit0)\n\n\nCall:\nglm(formula = drop ~ 1, family = binomial(link = \"logit\"), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6025  -0.6025  -0.6025  -0.6025   1.8951  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.6142     0.1202  -13.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 449.49  on 499  degrees of freedom\nResidual deviance: 449.49  on 499  degrees of freedom\nAIC: 451.49\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe intercept is the overall odds of dropout:\n\nexp(coef(fit0))\n\n(Intercept) \n  0.1990408 \n\nplogis(coef(fit0))\n\n(Intercept) \n      0.166 \n\nmean(dat$drop)\n\n[1] 0.166\n\n\nLet’s now fit a model with the two main effects:\n\nfit1 &lt;- glm(drop ~ academic + parenting, data = dat, family = binomial(link = \"logit\"))\nsummary(fit1)\n\n\nCall:\nglm(formula = drop ~ academic + parenting, family = binomial(link = \"logit\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0161  -0.5696  -0.3506  -0.3036   2.4902  \n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -0.3921     0.1985  -1.975 0.048216 *  \nacademichigh            -1.0221     0.3030  -3.373 0.000742 ***\nparentingpermissive     -1.3446     0.3335  -4.031 5.55e-05 ***\nparentingauthoritative  -1.6402     0.4670  -3.512 0.000444 ***\nparentingauthoritarian  -0.7550     0.3412  -2.212 0.026935 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 449.49  on 499  degrees of freedom\nResidual deviance: 392.66  on 495  degrees of freedom\nAIC: 402.66\n\nNumber of Fisher Scoring iterations: 5\n\n\nComments?\nLet’s now fit the interaction model:\n\nfit2 &lt;- glm(drop ~ academic * parenting, data = dat, family = binomial(link = \"logit\"))\nsummary(fit2)\n\n\nCall:\nglm(formula = drop ~ academic * parenting, family = binomial(link = \"logit\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.9760  -0.6583  -0.2801  -0.1638   2.9385  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                         -0.49402    0.21149  -2.336  0.01950 * \nacademichigh                        -0.39328    0.49639  -0.792  0.42820   \nparentingpermissive                 -0.92507    0.35710  -2.590  0.00958 **\nparentingauthoritative              -1.24058    0.66097  -1.877  0.06053 . \nparentingauthoritarian              -0.89228    0.42850  -2.082  0.03731 * \nacademichigh:parentingpermissive    -2.49170    1.15811  -2.152  0.03144 * \nacademichigh:parentingauthoritative -1.09099    0.94793  -1.151  0.24976   \nacademichigh:parentingauthoritarian -0.09222    0.72769  -0.127  0.89915   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 449.49  on 499  degrees of freedom\nResidual deviance: 384.58  on 492  degrees of freedom\nAIC: 400.58\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n5. Model fitting with glm() using the dataset in the aggregated form\nIn this case we can easily fit the same model using the aggregated form. The aggregated form is a dataset without 1s and 0s but counting the number of 1s for each condition.\n\ndat_agg &lt;- dat |&gt; \n    group_by(academic, parenting) |&gt; \n    summarise(drop_1 = sum(drop),\n              drop_0 = sum(drop == 0)) |&gt; \n    data.frame()\n\ndat_agg$drop_tot &lt;- dat_agg$drop_1 + dat_agg$drop_0\n\nNow we have a column with the number of 1s and a column with the total. Then we can also compute the number of 0s:\n\ndat_agg\n\n  academic     parenting drop_1 drop_0 drop_tot\n1      low    neglectful     36     59       95\n2      low    permissive     15     62       77\n3      low authoritative      3     17       20\n4      low authoritarian      9     36       45\n5     high    neglectful      7     17       24\n6     high    permissive      1     74       75\n7     high authoritative      4    100      104\n8     high authoritarian      8     52       60\n\n\nThe two dataset (dat and dat_agg) contains the same information. Let’s now fit the same models as before:\n\nfit0_agg &lt;- glm(cbind(drop_1, drop_0) ~ 1, data = dat_agg, family = binomial(link = \"logit\"))\nsummary(fit0)\n\n\nCall:\nglm(formula = drop ~ 1, family = binomial(link = \"logit\"), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6025  -0.6025  -0.6025  -0.6025   1.8951  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.6142     0.1202  -13.43   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 449.49  on 499  degrees of freedom\nResidual deviance: 449.49  on 499  degrees of freedom\nAIC: 451.49\n\nNumber of Fisher Scoring iterations: 3\n\n\nLet’s now fit a model with the two main effects:\n\nfit1_agg &lt;- glm(cbind(drop_1, drop_0) ~ academic + parenting, data = dat_agg, family = binomial(link = \"logit\"))\nsummary(fit1_agg)\n\n\nCall:\nglm(formula = cbind(drop_1, drop_0) ~ academic + parenting, family = binomial(link = \"logit\"), \n    data = dat_agg)\n\nDeviance Residuals: \n      1        2        3        4        5        6        7        8  \n-0.4840   1.0677   0.4590  -0.6573   1.1269  -2.0280  -0.3309   0.7549  \n\nCoefficients:\n                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -0.3921     0.1985  -1.975 0.048216 *  \nacademichigh            -1.0221     0.3030  -3.373 0.000742 ***\nparentingpermissive     -1.3446     0.3335  -4.031 5.54e-05 ***\nparentingauthoritative  -1.6402     0.4670  -3.512 0.000444 ***\nparentingauthoritarian  -0.7550     0.3412  -2.212 0.026935 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 64.902  on 7  degrees of freedom\nResidual deviance:  8.079  on 3  degrees of freedom\nAIC: 46.507\n\nNumber of Fisher Scoring iterations: 4\n\n\nComments?\nLet’s now fit the interaction model:\n\nfit2_agg &lt;- glm(cbind(drop_1, drop_0) ~ academic * parenting, data = dat_agg, family = binomial(link = \"logit\"))\nsummary(fit2_agg)\n\n\nCall:\nglm(formula = cbind(drop_1, drop_0) ~ academic * parenting, family = binomial(link = \"logit\"), \n    data = dat_agg)\n\nDeviance Residuals: \n[1]  0  0  0  0  0  0  0  0\n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                         -0.49402    0.21149  -2.336  0.01950 * \nacademichigh                        -0.39328    0.49639  -0.792  0.42820   \nparentingpermissive                 -0.92507    0.35710  -2.590  0.00958 **\nparentingauthoritative              -1.24058    0.66097  -1.877  0.06053 . \nparentingauthoritarian              -0.89228    0.42850  -2.082  0.03731 * \nacademichigh:parentingpermissive    -2.49170    1.15876  -2.150  0.03153 * \nacademichigh:parentingauthoritative -1.09099    0.94793  -1.151  0.24976   \nacademichigh:parentingauthoritarian -0.09222    0.72769  -0.127  0.89915   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.4902e+01  on 7  degrees of freedom\nResidual deviance: 1.1546e-14  on 0  degrees of freedom\nAIC: 44.428\n\nNumber of Fisher Scoring iterations: 4\n\n\nDo you notice any difference with the previous models?\n\n\n6. Plotting and interpreting effects of both models\nLet’s start by plotting the full model (in both forms):\n\nplot(allEffects(fit2))\n\n\n\nplot(allEffects(fit2_agg))\n\n\n\n\nLet’s compare the coefficients:\n\ncar::compareCoefs(fit2, fit2_agg)\n\nCalls:\n1: glm(formula = drop ~ academic * parenting, family = binomial(link = \n  \"logit\"), data = dat)\n2: glm(formula = cbind(drop_1, drop_0) ~ academic * parenting, family = \n  binomial(link = \"logit\"), data = dat_agg)\n\n                                    Model 1 Model 2\n(Intercept)                          -0.494  -0.494\nSE                                    0.211   0.211\n                                                   \nacademichigh                         -0.393  -0.393\nSE                                    0.496   0.496\n                                                   \nparentingpermissive                  -0.925  -0.925\nSE                                    0.357   0.357\n                                                   \nparentingauthoritative               -1.241  -1.241\nSE                                    0.661   0.661\n                                                   \nparentingauthoritarian               -0.892  -0.892\nSE                                    0.429   0.429\n                                                   \nacademichigh:parentingpermissive      -2.49   -2.49\nSE                                     1.16    1.16\n                                                   \nacademichigh:parentingauthoritative  -1.091  -1.091\nSE                                    0.948   0.948\n                                                   \nacademichigh:parentingauthoritarian -0.0922 -0.0922\nSE                                   0.7277  0.7277\n                                                   \n\n\nNow let’s interpret the effects. The “new” component is the interaction between two categorical variable. If the coefficients with one categorical variable is the log(Odds Ratio), the interaction is the difference between the two odds ratios. When transformed on the probability scale, the parameter is the ratio between odds ratios.\nThis is the odds ratio for the academic effect with neglectful parenting (i.e., the reference level):\n\ncoefs &lt;- coef(fit2_agg)\n\ncoefs[\"academichigh\"] # log odds ratio\n\nacademichigh \n  -0.3932847 \n\nexp(coefs[\"academichigh\"]) # odds ratio\n\nacademichigh \n   0.6748366 \n\nagg\n\n      parenting academic       drop       odds\n1    neglectful      low 0.37894737 0.61016949\n2    permissive      low 0.19480519 0.24193548\n3 authoritative      low 0.15000000 0.17647059\n4 authoritarian      low 0.20000000 0.25000000\n5    neglectful     high 0.29166667 0.41176471\n6    permissive     high 0.01333333 0.01351351\n7 authoritative     high 0.03846154 0.04000000\n8 authoritarian     high 0.13333333 0.15384615\n\nlow &lt;- agg$odds[agg$parenting == \"neglectful\" & agg$academic == \"low\"]\nhigh &lt;- agg$odds[agg$parenting == \"neglectful\" & agg$academic == \"high\"]\n\nhigh/low\n\n[1] 0.6748366\n\nlog(high/low)\n\n[1] -0.3932847\n\n\nThen the academichigh:parentingpermissive is the difference of the log odds ratios for low vs high for neglectful and permissive parenting styles.\n\ncoefs[\"academichigh:parentingpermissive\"]\n\nacademichigh:parentingpermissive \n                       -2.491696 \n\nexp(coefs[\"academichigh:parentingpermissive\"])\n\nacademichigh:parentingpermissive \n                      0.08276945 \n\nlow_neg &lt;- agg$odds[agg$parenting == \"neglectful\" & agg$academic == \"low\"]\nhigh_neg &lt;- agg$odds[agg$parenting == \"neglectful\" & agg$academic == \"high\"]\nlow_per &lt;- agg$odds[agg$parenting == \"permissive\" & agg$academic == \"low\"]\nhigh_per &lt;- agg$odds[agg$parenting == \"permissive\" & agg$academic == \"high\"]\n\nlog((high_per / low_per)) - log((high_neg / low_neg))\n\n[1] -2.491696\n\n(high_per / low_per) / (high_neg / low_neg)\n\n[1] 0.08276945\n\n\nSimilarly to the odds ratio, the ratio between two odds ratios can be interpreted in the same way:\n\nOR1 / OR2 &gt; 1: the odds ratio for the numerator condition is x times higher than the odds ratio for the denominator condition\nOR1 / OR2 &lt; 1: the odds ratio for the numerator condition is x times lower than the odds ratio for the denominator condition\n\nOf course, the best way is using the predict() function:\n\npreds &lt;- expand.grid(parenting = c(\"neglectful\", \"permissive\"),\n                     academic = c(\"low\", \"high\"))\npreds$pr &lt;- predict(fit2_agg, newdata = preds)\n\nwith(preds, (exp(pr)[4] / exp(pr)[2]) / (exp(pr)[3] / exp(pr)[1]))\n\n[1] 0.08276945\n\n\nWhy the residual deviance is different between the aggregated and the binary model?\n\ndeviance(fit2)\n\n[1] 384.5843\n\ndeviance(fit2_agg)\n\n[1] 1.154632e-14\n\n\nThis is the main difference between the two approaches. Actually we do not have to compare the deviance of the two models e.g., the aggregated form is better because it is closer to 0 but we always need to compare the model with the null deviance.\n\nanova(fit0, fit2, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: drop ~ 1\nModel 2: drop ~ academic * parenting\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       499     449.49                          \n2       492     384.58  7   64.902 1.573e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(fit0_agg, fit2_agg, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: cbind(drop_1, drop_0) ~ 1\nModel 2: cbind(drop_1, drop_0) ~ academic * parenting\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1         7     64.902                          \n2         0      0.000  7   64.902 1.573e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see the ratio is the same, thus the two deviances are on a different scale. The two models explains the same amount of (relative) deviance.\nWhy?\nThe reason is that we are computing the residual deviance from observed 0 and 1 vs observed counts.\n\n# aggregated model deviance\n-2*(sum(log(dbinom(dat_agg$drop_1, dat_agg$drop_tot, fitted(fit2_agg))) - log(dbinom(dat_agg$drop_1, dat_agg$drop_tot, dat_agg$drop_1/dat_agg$drop_tot))))\n\n[1] 0\n\n# binary model deviance\n-2*(sum(log(dbinom(dat$drop, 1, fitted(fit2))) - log(dbinom(dat$drop, 1, dat$drop))))\n\n[1] 384.5843\n\n\nIn a way it is more difficult to predict 0 and 1 compared to counts thus the residuals and the residual deviance will be always higher. Model coefficients, standard error and tests are the same.\nWhere the two models are not the same? Depends on the type of variables. Let’s add a new column to our binary dataset with the age of each student:\n\ndat$age &lt;- round(runif(nrow(dat), 12, 18))\n\nNow, if we want to include the age as predictor, we need to use the binary form because we have one value for each student. We are including a predictor at the level of the 0-1 values.\n\nfit3 &lt;- glm(drop ~ academic * parenting + age , data = dat, family = binomial(link = \"logit\"))\nsummary(fit3)\n\n\nCall:\nglm(formula = drop ~ academic * parenting + age, family = binomial(link = \"logit\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0596  -0.6609  -0.3093  -0.1633   2.8641  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)                          0.62071    1.08855   0.570   0.5685  \nacademichigh                        -0.38830    0.49748  -0.781   0.4351  \nparentingpermissive                 -0.89558    0.35858  -2.498   0.0125 *\nparentingauthoritative              -1.18035    0.66408  -1.777   0.0755 .\nparentingauthoritarian              -0.90057    0.42917  -2.098   0.0359 *\nage                                 -0.07536    0.07233  -1.042   0.2974  \nacademichigh:parentingpermissive    -2.51731    1.15902  -2.172   0.0299 *\nacademichigh:parentingauthoritative -1.16406    0.95166  -1.223   0.2213  \nacademichigh:parentingauthoritarian -0.07272    0.72905  -0.100   0.9205  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 449.49  on 499  degrees of freedom\nResidual deviance: 383.50  on 491  degrees of freedom\nAIC: 401.5\n\nNumber of Fisher Scoring iterations: 6\n\n\nWhen we have predictors at the 0-1 levels, we need to use the binary form.\nA little (visual) demonstration:\n\nx &lt;- seq(0, 1, 0.01)\ndat &lt;- data.frame(\n    x = rep(x, 10)\n)\n\ndat$lp &lt;- plogis(qlogis(0.01) + 8*dat$x)\ndat$y &lt;- rbinom(nrow(dat), 1, dat$lp)\nhead(dat)\n\n     x         lp y\n1 0.00 0.01000000 0\n2 0.01 0.01082386 0\n3 0.02 0.01171478 0\n4 0.03 0.01267810 0\n5 0.04 0.01371954 0\n6 0.05 0.01484523 0\n\n\nLet’s fit the model in the binary form:\n\n# model prediction\nfit &lt;- glm(y ~ x, data = dat, family = binomial())\n\n# equivalent to predict()\npi &lt;- plogis(coef(fit)[1] + coef(fit)[2]*unique(dat$x))\n\nLet’s fit the mode in the binomial form:\n\n# aggregated form\ndat_agg &lt;- aggregate(y ~ x, FUN = sum, data = dat)\ndat_agg$n &lt;- 10 # total trials\ndat_agg$f &lt;- dat_agg$n - dat_agg$y\ndat_agg$p &lt;- dat_agg$y / dat_agg$n\n\nhead(dat_agg)\n\n     x y  n  f   p\n1 0.00 0 10 10 0.0\n2 0.01 0 10 10 0.0\n3 0.02 1 10  9 0.1\n4 0.03 0 10 10 0.0\n5 0.04 1 10  9 0.1\n6 0.05 0 10 10 0.0\n\nfit2 &lt;- glm(cbind(y, f) ~ x, data = dat_agg, family = binomial())\npi &lt;- plogis(coef(fit2)[1] + coef(fit2)[2]*dat_agg$x)\n\nThe residuals (thus the residual deviance) will be always larger in the binary model (but the coefficients are the same):\n\npar(mfrow = c(1,2))\n\njit &lt;- runif(nrow(dat), -0.03, 0.03)\nplot((y + jit) ~ x, data = dat, ylab = \"y\", xlab = \"x\",\n     main = \"Binary Form\")\nlines(unique(dat$x), pi, lwd = 2, col = \"red\")\n\nplot(y/n ~ x, data = dat_agg, ylab = \"y\",\n     main = \"Binomial Form\")\nlines(dat_agg$x, pi, lwd = 2, col = \"red\")\n\n\n\n\nThis is the reason why binary model have also strange residuals:\n\n# also residuals\npar(mfrow = c(1,2))\nplot(fitted(fit), residuals(fit), main = \"Binary Form\")\nplot(fitted(fit2), residuals(fit2), main = \"Binomial Form\")"
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "devtools::load_all()\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(performance) # diagnostic\nlibrary(car) # general utilities\nlibrary(MuMIn) # model selection\nlibrary(patchwork)\n\n\nOverview\nThe volunt dataset contains data from 1000 people on volunteering.\nThe dataset contains:\n\nid: identifier for each person\nage: age of the participants in years\nses: socioeconomic status, from 1 (low) to 4 (high)\nsconnection: a scale measuring the social connection within the community. From 1 (low) to 10 (high)\nvol: is the person involved in volunteering? 1 = yes, 0 = no\n\n\n\nSteps\n\nImport the data\nExplore the dataset\n\nunivariate distributions of the variables\nbivariate relationships\n\nfit a model testing all main effects and interpret the parameters\nfit a model testing the interaction between ses and social connection. Compare the model with the previous model and interpret the result."
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "devtools::load_all()\nlibrary(tidyverse)\nlibrary(ggeffects)"
  },
  {
    "objectID": "labs/lab6.html#loading-data",
    "href": "labs/lab6.html#loading-data",
    "title": "Lab 6",
    "section": "Loading data",
    "text": "Loading data\n\ndata(\"psych\")\ndat &lt;- psych\n\nGiven that we did not introduced random-effects models, we select a single subject to analyze.\n\ncc &lt;- seq(0, 1, 0.1)\n\ndat$contrast_c &lt;- cut(dat$contrast, cc, include.lowest = TRUE)\n\ndat |&gt; \n  filter(id %in% sample(unique(dat$id), 5)) |&gt; \n  group_by(id, cond, contrast_c) |&gt; \n  summarise(y = mean(y)) |&gt; \n  ggplot(aes(x = contrast_c, y = y, color = cond, group = cond)) +\n  geom_point() +\n  geom_line() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  facet_wrap(~id)\n\n`summarise()` has grouped output by 'id', 'cond'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\ndat &lt;- filter(dat, id == 6)\n\nWe have several interesting stuff to estimate. Let’s start by fitting a simple model:\n\nfit1 &lt;- glm(y ~ contrast, data = dat, family = binomial(link = \"logit\"))\nsummary(fit1)\n\n\nCall:\nglm(formula = y ~ contrast, family = binomial(link = \"logit\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8826  -0.6003   0.2343   0.5974   2.1113  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.11482    0.09237  -22.90   &lt;2e-16 ***\ncontrast     6.41786    0.22165   28.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4038.1  on 2999  degrees of freedom\nResidual deviance: 2500.5  on 2998  degrees of freedom\nAIC: 2504.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe parameters (Intercept) and contrast are respectively the probability of saying yes for stimuli with 0 contrast. Seems odd but in Psychophysics this is a very interesting information. We can call it the false alarm rate. We usually expect this rate to be low, ideally 0.\n\nplogis(coef(fit1)[1])\n\n(Intercept) \n   0.107665 \n\n\nThe contrast is the slope i.e. the increase in the log odds of saying yes for a unit increase in contrast. In this case this parameter is hard to intepret, let’s change the scale of the contrast:\n\ndat$contrast10 &lt;- dat$contrast * 10\nfit1 &lt;- glm(y ~ contrast10, data = dat, family = binomial(link = \"logit\"))\nsummary(fit1)\n\n\nCall:\nglm(formula = y ~ contrast10, family = binomial(link = \"logit\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8826  -0.6003   0.2343   0.5974   2.1113  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -2.11482    0.09237  -22.90   &lt;2e-16 ***\ncontrast10   0.64179    0.02217   28.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4038.1  on 2999  degrees of freedom\nResidual deviance: 2500.5  on 2998  degrees of freedom\nAIC: 2504.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nNow the contrast10 is the increase in the log odds of saying yes for an increase of 10% contrast. Using the divide-by-4 rule we obtain an maximal increase of 0.1604465 of probability of saying yes.\nAnother interesting parameter is the threshold. In psychophysics the threshold is the required \\(x\\) level (in this case contrast) to obtain a certain proportions of \\(y\\) response.\nFor a logistic distribution (see Knoblauch and Maloney 2012) the 50% threshold can be estimated as \\(-\\frac{\\beta_0}{\\beta_1}\\) thus:\n\n-(coef(fit1)[1]/coef(fit1)[2])\n\n(Intercept) \n   3.295206 \n\n\nThen the slope is simply the inverse of the regression slope and represent the increase in performance/visibility for a unit increase in \\(x\\):\n\n1/coef(fit1)[2]\n\ncontrast10 \n  1.558152 \n\n\nIn fact, we can use these parameters to plot a logistic distribution:\n\ncurve(plogis(x, -(coef(fit1)[1]/coef(fit1)[2]), 1/coef(fit1)[2]),\n      0, 10)\n\n\n\n\nThat is very similar to the effects estimated by our model:\n\nplot(ggeffect(fit1))\n\n$contrast10"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "devtools::load_all() # if using the rproject dowloaded from the slides\n# source(\"utils-glm.R\") # if using a standard setup\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(car) # general utilities\nlibrary(effects) # for extracting and plotting effects \nlibrary(emmeans) # for marginal means\n\n\ndata(\"nwords\")\ndat &lt;- nwords\n\n\nOverview\nThis dataset nwords represent a developmental psychology study investigating the factors that influence children language development. The dataset includes information about the number of words in a a language task, and some potential predictors: caregiver behavior, socioeconomic status, amount of time spent with the child during a week and the presence of a baby-sitter or not.\n\nchild: numeric variable representing the child ID number.\ntimebonding: numeric variable representing the average hours per week of child-parent activities\nnwords: numeric variable representing the number of words produced by the child in a language task.\ncaregiving: numeric variable representing the caregiver’s behavior in a parent-child interaction task, measured on a scale from 0 to 10\nses: categorical variable representing the family socioeconomic status, with three levels: “Low”, “Middle”, and “High”\n\n\nImporting data and overall check\n\nthink about factors levels, scale of the numerical variables, etc.\n\nExploratory data analysis of predictors and the relationships between predictors and the number of words\nModel fitting with glm() and poisson family starting from additive models and then adding the interactions.\nModel diagnostic of the chosen model\n\noverdispersion\nresiduals\noutliers and influential points\n\nInterpreting the effects and plotting\nFit a quasi-poisson and negative binomial version of the chosen model, and check how parameters are affected\n\n\n\n1. Importing data and overall check\n\ndat &lt;- read.csv(\"data/nwords.csv\")\n\n\nstr(dat)\n\n'data.frame':   150 obs. of  6 variables:\n $ id         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ timebonding: num  23 18 20 23 11 5 15 18 29 22 ...\n $ caregiving : num  7 8 7 11 8 5 4 5 14 9 ...\n $ ses        : Factor w/ 3 levels \"low\",\"middle\",..: 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"contrasts\")= num [1:3, 1:2] 0 1 0 0 0 1\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:3] \"low\" \"middle\" \"high\"\n  .. .. ..$ : chr [1:2] \"2\" \"3\"\n $ babysitter : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 1 2 1 2 1 1 ...\n  ..- attr(*, \"contrasts\")= num [1:2, 1] 0 1\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:2] \"no\" \"yes\"\n  .. .. ..$ : chr \"2\"\n $ nwords     : int  24 20 16 38 14 17 17 35 30 13 ...\n\n\nCheck for NA values:\n\nsapply(dat, function(x) sum(is.na(x)))\n\n         id timebonding  caregiving         ses  babysitter      nwords \n          0           0           0           0           0           0 \n\n\nEverything seems good, we do not have NA values.\nLet’s convert categorical variables into factor setting the appropriate order:\n\nses: low, middle, high\nbabysitter: no, yes\n\n\ndat$ses &lt;- factor(dat$ses, levels = c(\"low\", \"middle\", \"high\"))\ndat$babysitter &lt;- factor(dat$babysitter, levels = c(\"no\", \"yes\"))\n\nlevels(dat$ses)\n\n[1] \"low\"    \"middle\" \"high\"  \n\nlevels(dat$babysitter)\n\n[1] \"no\"  \"yes\"\n\n\ntimebonding and caregiving are the two numerical predictors. Given that we are going to fit main effects and interactions and we want to interpret the intercept and test the interaction on a meaningful point, we create two centered versions of these variables:\n\ndat$timebonding0 &lt;- dat$timebonding - mean(dat$timebonding)\ndat$caregiving0 &lt;- dat$caregiving - mean(dat$caregiving)\n\nThen we can see what happen to the variables:\n\ncols &lt;- grepl(\"timebonding|caregiving\", names(dat))\nlapply(dat[, cols], function(x) round(c(mean = mean(x), sd = sd(x)), 3))\n\n$timebonding\n  mean     sd \n14.807  8.736 \n\n$caregiving\n mean    sd \n5.420 2.841 \n\n$timebonding0\n mean    sd \n0.000 8.736 \n\n$caregiving0\n mean    sd \n0.000 2.841 \n\n\n\n\n2. Exploratory data analysis\n\nsummary(dat)\n\n       id          timebonding      caregiving        ses     babysitter\n Min.   :  1.00   Min.   : 0.00   Min.   : 0.00   low   :45   no :83    \n 1st Qu.: 38.25   1st Qu.: 8.00   1st Qu.: 4.00   middle:75   yes:67    \n Median : 75.50   Median :14.00   Median : 5.00   high  :30             \n Mean   : 75.50   Mean   :14.81   Mean   : 5.42                         \n 3rd Qu.:112.75   3rd Qu.:21.00   3rd Qu.: 7.00                         \n Max.   :150.00   Max.   :37.00   Max.   :14.00                         \n     nwords       timebonding0       caregiving0   \n Min.   : 2.00   Min.   :-14.8067   Min.   :-5.42  \n 1st Qu.:10.00   1st Qu.: -6.8067   1st Qu.:-1.42  \n Median :17.00   Median : -0.8067   Median :-0.42  \n Mean   :19.96   Mean   :  0.0000   Mean   : 0.00  \n 3rd Qu.:27.00   3rd Qu.:  6.1933   3rd Qu.: 1.58  \n Max.   :54.00   Max.   : 22.1933   Max.   : 8.58  \n\n\nLet’s do some plotting of predictors:\n\npar(mfrow = c(2,2))\nhist(dat$timebonding)\nhist(dat$caregiving)\nbarplot(table(dat$babysitter))\nbarplot(table(dat$ses))\n\n\n\n\nComments?\nAlso the response variable:\n\nhist(dat$nwords, probability = TRUE)\nlines(density(dat$nwords), col = \"salmon\", lwd = 2)\n\n\n\n\nLet’s plot the theoretical distributions, Poisson and Gaussian:\n\nm &lt;- mean(dat$nwords)\ns &lt;- sd(dat$nwords)\nxs &lt;- seq(0, 50, 1)\n\nhist(dat$nwords, probability = TRUE, xlim = c(-10, 50), ylim = c(0, 0.1))\ncurve(dnorm(x, m, s), add = TRUE, col = \"green\", lwd = 2)\nlines(xs, dpois(xs, m), col = \"red\", lwd = 2)\nlines(xs, dpois(xs, m), col = \"red\", lwd = 2)\n\n\n\n\nComments?\nLet’s plot some bivariate distributions:\n\n# caregiving ~ timebonding\nr &lt;- cor(dat$timebonding, dat$caregiving)\nplot(dat$timebonding, dat$caregiving, pch = 19)\nabline(lm(dat$caregiving ~ dat$timebonding), col = \"red\", lwd = 2)\ntext(30, 2, label = paste(\"r =\", round(r, 2)))\n\n\n\n\n\npar(mfrow = c(1,2))\nboxplot(caregiving ~ ses, data = dat)\nboxplot(timebonding ~ ses, data = dat)\n\n\n\n\n\nmosaicplot(table(dat$ses, dat$babysitter))\n\n\n\n# or\nbarplot(table(dat$babysitter, dat$ses), col = c(\"red\", \"green\"), beside = TRUE)\nlegend(7,45, legend = c(\"no\", \"yes\"), fill = c(\"red\", \"green\"))\n\n\n\n\n\npar(mfrow = c(1,2))\nboxplot(caregiving ~ babysitter, data = dat)\nboxplot(timebonding ~ babysitter, data = dat)\n\n\n\n\nThen the relationships with the nwords variable:\n\npar(mfrow = c(2,2))\nplot(dat$timebonding, dat$nwords, pch = 19)\nplot(dat$caregiving, dat$nwords, pch = 19)\nboxplot(dat$nwords ~ dat$ses)\nboxplot(dat$nwords ~ dat$babysitter)\n\n\n\n\nComments?\nFinally some interactions plot (with lm):\n\npar(mfrow = c(2,1))\ncolors &lt;- c(low = \"red\", middle = \"blue\", high = \"green\")\nplot(dat$timebonding, dat$nwords, col = colors[dat$ses], pch = 19)\nlms &lt;- lapply(split(dat, dat$ses), function(x) lm(nwords ~ timebonding, data = x))\nlapply(1:length(lms), function(i) abline(lms[[i]], col = colors[i], lwd = 2))\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\nplot(dat$caregiving, dat$nwords, col = colors[dat$ses], pch = 19)\nlms &lt;- lapply(split(dat, dat$ses), function(x) lm(nwords ~ caregiving, data = x))\nlapply(1:length(lms), function(i) abline(lms[[i]], col = colors[i], lwd = 2))\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n\n\npar(mfrow = c(2,1))\ncolors &lt;- c(no = \"orange\", yes = \"purple\")\n\nplot(dat$timebonding, dat$nwords, col = colors[dat$babysitter], pch = 19)\nlms &lt;- lapply(split(dat, dat$babysitter), function(x) lm(nwords ~ timebonding, data = x))\nlapply(1:length(lms), function(i) abline(lms[[i]], col = colors[i], lwd = 2))\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\nplot(dat$caregiving, dat$nwords, col = colors[dat$babysitter], pch = 19)\nlms &lt;- lapply(split(dat, dat$babysitter), function(x) lm(nwords ~ caregiving, data = x))\nlapply(1:length(lms), function(i) abline(lms[[i]], col = colors[i], lwd = 2))\n\n\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n\n\n\n3. Model fitting with glm() and poisson\nLet’s start by using an additive model:\n\nfit &lt;- glm(nwords ~ timebonding + caregiving + babysitter + ses, family = poisson(link = \"log\"), data = dat)\nsummary(fit)\n\n\nCall:\nglm(formula = nwords ~ timebonding + caregiving + babysitter + \n    ses, family = poisson(link = \"log\"), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.5942  -1.8861  -0.1598   1.1563   5.2008  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    2.38233    0.08162  29.187  &lt; 2e-16 ***\ntimebonding    0.02468    0.00298   8.283  &lt; 2e-16 ***\ncaregiving     0.04572    0.01139   4.014 5.97e-05 ***\nbabysitteryes -0.05342    0.04051  -1.319    0.187    \nsesmiddle     -0.03297    0.04910  -0.672    0.502    \nseshigh       -0.11921    0.08057  -1.480    0.139    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1074.54  on 149  degrees of freedom\nResidual deviance:  703.21  on 144  degrees of freedom\nAIC: 1411.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nAnd always plotting before anything else:\n\nplot(allEffects(fit))\n\n\n\n\nComments? How could you describe the results? Something different from the descriptive statistics?\n\ncar::residualPlots(fit)\n\n\n\n\n            Test stat Pr(&gt;|Test stat|)   \ntimebonding    8.4288         0.003693 **\ncaregiving     6.5550         0.010459 * \nbabysitter                               \nses                                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComments? Are we missing something?\n\ndat |&gt; \n    ggplot(aes(x = timebonding, y = nwords, color = ses)) +\n    geom_point() +\n    stat_smooth(method = \"glm\", method.args = list(family = poisson()), se = FALSE)\n\n\n\n\n\ndat |&gt; \n    ggplot(aes(x = caregiving, y = nwords, color = ses)) +\n    geom_point() +\n    stat_smooth(method = \"glm\", method.args = list(family = poisson()), se = FALSE)\n\n\n\n\n\ndat |&gt; \n    ggplot(aes(x = timebonding, y = nwords, color = babysitter)) +\n    geom_point() +\n    stat_smooth(method = \"glm\", method.args = list(family = poisson()), se = FALSE)\n\n\n\n\n\ndat |&gt; \n    ggplot(aes(x = caregiving, y = nwords, color = babysitter)) +\n    geom_point() +\n    stat_smooth(method = \"glm\", method.args = list(family = poisson()), se = FALSE)\n\n\n\n\nLet’s add some interactions:\n\nfit2 &lt;- glm(nwords ~ timebonding*ses + caregiving*ses + timebonding*babysitter + caregiving*babysitter, family = poisson(link = \"log\"), data = dat)\nsummary(fit2)\n\n\nCall:\nglm(formula = nwords ~ timebonding * ses + caregiving * ses + \n    timebonding * babysitter + caregiving * babysitter, family = poisson(link = \"log\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.0744  -1.7914  -0.2072   1.2771   5.1336  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                2.449904   0.130572  18.763  &lt; 2e-16 ***\ntimebonding                0.025282   0.005037   5.020 5.18e-07 ***\nsesmiddle                 -0.118244   0.147663  -0.801   0.4233    \nseshigh                   -0.731529   0.172366  -4.244 2.20e-05 ***\ncaregiving                 0.036904   0.019566   1.886   0.0593 .  \nbabysitteryes              0.208781   0.106645   1.958   0.0503 .  \ntimebonding:sesmiddle      0.012661   0.006201   2.042   0.0412 *  \ntimebonding:seshigh        0.057886   0.012124   4.775 1.80e-06 ***\nsesmiddle:caregiving      -0.026875   0.024638  -1.091   0.2754    \nseshigh:caregiving        -0.027161   0.039418  -0.689   0.4908    \ntimebonding:babysitteryes -0.029552   0.006247  -4.731 2.24e-06 ***\ncaregiving:babysitteryes   0.039355   0.019464   2.022   0.0432 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1074.54  on 149  degrees of freedom\nResidual deviance:  654.95  on 138  degrees of freedom\nAIC: 1374.8\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nplot(allEffects(fit2))\n\n\n\n\n\n\n4. Model fitting with MASS::glm.nb()\nThere is still evidence for overdispersion, even after including all predictors and a series of interactions. Let’s assume that this is our most complex model, we need to take into account the overdispersion:\n\nperformance::check_overdispersion(fit2)\n\n# Overdispersion test\n\n       dispersion ratio =   4.710\n  Pearson's Chi-Squared = 649.938\n                p-value = &lt; 0.001\n\nfit3 &lt;- MASS::glm.nb(nwords ~ timebonding*ses + caregiving*ses + timebonding*babysitter + caregiving*babysitter, data = dat)\n\nsummary(fit3)\n\n\nCall:\nMASS::glm.nb(formula = nwords ~ timebonding * ses + caregiving * \n    ses + timebonding * babysitter + caregiving * babysitter, \n    data = dat, init.theta = 5.511283984, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3222  -0.8568  -0.1203   0.5893   2.0347  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                2.42447    0.30858   7.857 3.94e-15 ***\ntimebonding                0.02188    0.01220   1.793   0.0729 .  \nsesmiddle                 -0.12795    0.33725  -0.379   0.7044    \nseshigh                   -0.65153    0.35611  -1.830   0.0673 .  \ncaregiving                 0.04869    0.04756   1.024   0.3060    \nbabysitteryes              0.20747    0.20566   1.009   0.3131    \ntimebonding:sesmiddle      0.01417    0.01450   0.978   0.3282    \ntimebonding:seshigh        0.05378    0.02394   2.246   0.0247 *  \nsesmiddle:caregiving      -0.02627    0.05691  -0.462   0.6444    \nseshigh:caregiving        -0.03298    0.08127  -0.406   0.6849    \ntimebonding:babysitteryes -0.02536    0.01392  -1.822   0.0684 .  \ncaregiving:babysitteryes   0.02732    0.04305   0.635   0.5257    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(5.5113) family taken to be 1)\n\n    Null deviance: 249.03  on 149  degrees of freedom\nResidual deviance: 154.97  on 138  degrees of freedom\nAIC: 1093\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  5.511 \n          Std. Err.:  0.835 \n\n 2 x log-likelihood:  -1066.951 \n\n\nNow overdispersion is taken into account and standard errors are larger:\n\ncar::compareCoefs(fit2, fit3)\n\nCalls:\n1: glm(formula = nwords ~ timebonding * ses + caregiving * ses + \n  timebonding * babysitter + caregiving * babysitter, family = poisson(link = \n  \"log\"), data = dat)\n2: MASS::glm.nb(formula = nwords ~ timebonding * ses + caregiving * ses + \n  timebonding * babysitter + caregiving * babysitter, data = dat, init.theta =\n   5.511283984, link = log)\n\n                           Model 1  Model 2\n(Intercept)                  2.450    2.424\nSE                           0.131    0.309\n                                           \ntimebonding                0.02528  0.02188\nSE                         0.00504  0.01220\n                                           \nsesmiddle                   -0.118   -0.128\nSE                           0.148    0.337\n                                           \nseshigh                     -0.732   -0.652\nSE                           0.172    0.356\n                                           \ncaregiving                  0.0369   0.0487\nSE                          0.0196   0.0476\n                                           \nbabysitteryes                0.209    0.207\nSE                           0.107    0.206\n                                           \ntimebonding:sesmiddle       0.0127   0.0142\nSE                          0.0062   0.0145\n                                           \ntimebonding:seshigh         0.0579   0.0538\nSE                          0.0121   0.0239\n                                           \nsesmiddle:caregiving       -0.0269  -0.0263\nSE                          0.0246   0.0569\n                                           \nseshigh:caregiving         -0.0272  -0.0330\nSE                          0.0394   0.0813\n                                           \ntimebonding:babysitteryes -0.02955 -0.02536\nSE                         0.00625  0.01392\n                                           \ncaregiving:babysitteryes    0.0394   0.0273\nSE                          0.0195   0.0431\n                                           \n\n# test statistics for the poisson model\n\ndata.frame(\n    poisson = fit2$coefficients/sqrt(diag(vcov(fit2))),\n    negative_binomial = fit3$coefficients/sqrt(diag(vcov(fit3)))\n)\n\n                             poisson negative_binomial\n(Intercept)               18.7628976         7.8567464\ntimebonding                5.0195056         1.7932210\nsesmiddle                 -0.8007732        -0.3794009\nseshigh                   -4.2440539        -1.8295760\ncaregiving                 1.8861966         1.0237380\nbabysitteryes              1.9577133         1.0087898\ntimebonding:sesmiddle      2.0417940         0.9777883\ntimebonding:seshigh        4.7746326         2.2463830\nsesmiddle:caregiving      -1.0907766        -0.4615336\nseshigh:caregiving        -0.6890598        -0.4057725\ntimebonding:babysitteryes -4.7305937        -1.8221897\ncaregiving:babysitteryes   2.0219802         0.6345467"
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "devtools::load_all() # if using the rproject dowloaded from the slides\n# source(\"utils-glm.R\") # if using a standard setup\nlibrary(here)\nlibrary(tidyr) # for data manipulation\nlibrary(dplyr) # for data manipulation\nlibrary(ggplot2) # plotting\nlibrary(car) # general utilities\nlibrary(effects) # for extracting and plotting effects \nlibrary(emmeans) # for marginal means\n\n\ndata(\"tantrums\")\ndat &lt;- tantrums\n\n\nOverview\nThe dataset tantrums.csv is about the number of tantrums of nrow(child) toddlers during two days at the nursery. The columns are:\n\nid: identifier for the child\ntemperament: the temperament of the child as “easy” or “difficult”\nattachment: the attachment of the child as “secure” or “insecure”\nparent_se: an average self-esteem value of the parents (self report)\nparent_skills: a score representing the teacher judgment about parenting skills\ntantrums: the number of tantrums\n\nWe want to predict the number of tantrums as a function of these predictors.\n\nImporting data and check\n\nin the presence of NA, remove the children\nconvert to factors the categorical variable with “difficult” and “insecure” as reference values\n\nExploratory data analysis\nModel fitting with glm()\nDiagnostic\nInterpreting parameters\nModel selection\nWhat about interactions?\n\n\n\n1. Importing data and check\nCheck the structure:\n\nstr(dat)\n\n'data.frame':   122 obs. of  6 variables:\n $ id           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ temperament  : chr  \"difficult\" \"easy\" \"difficult\" \"difficult\" ...\n $ attachment   : chr  \"insecure\" \"secure\" \"secure\" \"secure\" ...\n $ parent_se    : int  3 4 9 8 4 6 10 6 10 4 ...\n $ parent_skills: int  5 8 5 6 7 2 9 7 1 7 ...\n $ tantrum      : int  1 0 1 0 0 10 0 2 7 0 ...\n\n\nCheck for NA:\n\nsapply(dat, function(x) sum(is.na(x)))\n\n           id   temperament    attachment     parent_se parent_skills \n            0             1             1             0             0 \n      tantrum \n            2 \n\n\nSo we have some NA values. We managed them according to the instructions:\n\ndat &lt;- dat[complete.cases(dat), ]\ndat$id &lt;- 1:nrow(dat) # restore the id\nrownames(dat) &lt;- NULL\n\nLet’s convert the categorical variables into factor with the appropriate reference level:\n\ndat$temperament &lt;- factor(dat$temperament, levels = c(\"difficult\", \"easy\"))\ndat$temperament[1:5]\n\n[1] difficult easy      difficult difficult difficult\nLevels: difficult easy\n\ndat$attachment &lt;- factor(dat$attachment, levels = c(\"insecure\", \"secure\"))\ndat$attachment[1:5]\n\n[1] insecure secure   secure   secure   insecure\nLevels: insecure secure\n\n\n\n\n2. Exploratory data analysis\nLet’s compute some summary statistics and plots.\n\nsummary(dat)\n\n       id            temperament    attachment   parent_se     \n Min.   :  1.00   difficult:32   insecure:39   Min.   : 1.000  \n 1st Qu.: 30.25   easy     :86   secure  :79   1st Qu.: 5.000  \n Median : 59.50                                Median : 7.000  \n Mean   : 59.50                                Mean   : 6.364  \n 3rd Qu.: 88.75                                3rd Qu.: 8.000  \n Max.   :118.00                                Max.   :10.000  \n parent_skills       tantrum     \n Min.   : 1.000   Min.   : 0.00  \n 1st Qu.: 5.000   1st Qu.: 0.00  \n Median : 6.000   Median : 1.00  \n Mean   : 6.237   Mean   : 1.72  \n 3rd Qu.: 8.000   3rd Qu.: 2.00  \n Max.   :10.000   Max.   :20.00  \n\n\n\ntable(dat$temperament)\n\n\ndifficult      easy \n       32        86 \n\ntable(dat$attachment)\n\n\ninsecure   secure \n      39       79 \n\ntable(dat$attachment, dat$temperament)\n\n          \n           difficult easy\n  insecure        12   27\n  secure          20   59\n\n\n\npar(mfrow = c(1,3))\nhist(dat$parent_se)\nhist(dat$parent_skills)\nhist(dat$tantrum)\n\n\n\n\nLet’s compute some bivariate relationships:\n\nplot(dat$parent_se, dat$tantrum, pch = 19)\n\n\n\nplot(dat$parent_skills, dat$tantrum, pch = 19)\n\n\n\n\n\nboxplot(tantrum ~ temperament, data = dat)\n\n\n\nboxplot(tantrum ~ attachment, data = dat)\n\n\n\n\n\n\n3. Model fitting with glm()\nWe can start by fitting our null model with the poisson() family:\n\nfit0 &lt;- glm(tantrum ~ 1, family = poisson(link = \"log\"), data = dat)\n\nWhat is the intercept here?\nThen we can fit a model with the attachment effect:\n\nfit1 &lt;- glm(tantrum ~ parent_se, family = poisson(link = \"log\"), data = dat)\nsummary(fit1)\n\n\nCall:\nglm(formula = tantrum ~ parent_se, family = poisson(link = \"log\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1239  -1.8145  -0.8368   0.2666   7.3901  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  0.02631    0.22966   0.115   0.9088  \nparent_se    0.07870    0.03240   2.429   0.0151 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 421.11  on 117  degrees of freedom\nResidual deviance: 415.05  on 116  degrees of freedom\nAIC: 590.21\n\nNumber of Fisher Scoring iterations: 6\n\n\nWhat about the overdispersion? What could be the reason?\nAssuming that the attachment is the only variable that we have, we could estimate the degree of overdispersion:\n\nsum(residuals(fit1, type = \"pearson\")^2)/fit1$df.residual\n\n[1] 5.065601\n\nperformance::check_overdispersion(fit1)\n\n# Overdispersion test\n\n       dispersion ratio =   5.066\n  Pearson's Chi-Squared = 587.610\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.\n\n\nLet’s have a look also at the residual plot:\n\nresidualPlots(fit1)\n\n\n\n\n          Test stat Pr(&gt;|Test stat|)  \nparent_se    3.1933          0.07394 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is clear evidence of overdispersion. But we have several other variables so before using another model let’s fit everything:\n\nfit_s &lt;- glm(tantrum ~ attachment + temperament + parent_se + parent_skills, family = poisson(link = \"log\"), data = dat)\nsummary(fit_s)\n\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9635  -1.0742  -0.6412   0.2012   7.7786  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.19125    0.34888   9.147  &lt; 2e-16 ***\nattachmentsecure -0.05147    0.15964  -0.322    0.747    \ntemperamenteasy  -0.82435    0.14127  -5.835 5.36e-09 ***\nparent_se        -0.01881    0.03605  -0.522    0.602    \nparent_skills    -0.38883    0.03454 -11.257  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 421.11  on 117  degrees of freedom\nResidual deviance: 218.91  on 113  degrees of freedom\nAIC: 400.07\n\nNumber of Fisher Scoring iterations: 6\n\n\nLet’s check again overdispersion and pearson residuals:\n\nresidualPlots(fit_s)\n\n\n\n\n              Test stat Pr(&gt;|Test stat|)    \nattachment                                  \ntemperament                                 \nparent_se        9.5838         0.001963 ** \nparent_skills   29.1366        6.745e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe majority of the distribution seems ok, but there are some values with very high residuals and the overdispersion is still present:\n\nsum(residuals(fit_s, type = \"pearson\")^2)/fit_s$df.residual\n\n[1] 8.14082\n\nperformance::check_overdispersion(fit_s)\n\n# Overdispersion test\n\n       dispersion ratio =   8.141\n  Pearson's Chi-Squared = 919.913\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.\n\n\n\n\n4. Diagnostic\nAnother reason for overdispersion could be the presence of outliers and influential points. Let’s have a look at the Cook distances:\n\ncar::influenceIndexPlot(fit_s, vars = c(\"cook\", \"hat\", \"Studentized\"))\n\n\n\n\nThere are two values (117 and 118) with a very high cook distance and very high studentized residual. We can try to fit a model without these values and check what happens to the model:\n\ndat_no_out &lt;- dat[-c(117, 118), ]\nfit_no_out &lt;- glm(tantrum ~ attachment + temperament + parent_se + parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\nsummary(fit_no_out)\n\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6340  -0.7705  -0.4001   0.2804   2.1595  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.72884    0.39403   9.463  &lt; 2e-16 ***\nattachmentsecure -0.41074    0.16838  -2.439   0.0147 *  \ntemperamenteasy  -1.08473    0.15090  -7.188 6.56e-13 ***\nparent_se         0.01940    0.04055   0.478   0.6324    \nparent_skills    -0.53074    0.04109 -12.915  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  84.882  on 111  degrees of freedom\nAIC: 257.73\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe model seems to be clearly improved, especially in terms of overdispersion:\n\nsum(residuals(fit_no_out, type = \"pearson\")^2)/fit_no_out$df.residual\n\n[1] 0.81442\n\nperformance::check_overdispersion(fit_no_out)\n\n# Overdispersion test\n\n       dispersion ratio =  0.814\n  Pearson's Chi-Squared = 90.401\n                p-value =  0.924\n\n\nNo overdispersion detected.\n\n\nWe can also compare the two models in terms of coefficients:\n\ncar::compareCoefs(fit_s, fit_no_out)\n\nCalls:\n1: glm(formula = tantrum ~ attachment + temperament + parent_se + \n  parent_skills, family = poisson(link = \"log\"), data = dat)\n2: glm(formula = tantrum ~ attachment + temperament + parent_se + \n  parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\n                 Model 1 Model 2\n(Intercept)        3.191   3.729\nSE                 0.349   0.394\n                                \nattachmentsecure -0.0515 -0.4107\nSE                0.1596  0.1684\n                                \ntemperamenteasy   -0.824  -1.085\nSE                 0.141   0.151\n                                \nparent_se        -0.0188  0.0194\nSE                0.0360  0.0405\n                                \nparent_skills    -0.3888 -0.5307\nSE                0.0345  0.0411\n                                \n\n\nIn fact, there are some coefficients with different values. We can check also the dfbeta plots:\n\ndfbeta_plot(fit_s)\n\n\n\n\nThe previous observations seems to do not affect the estimated parameters but they impact the overall model fit, deviance and residuals.\nLet’s have a look at residuals now:\n\ncar::residualPlot(fit_no_out)\n\n\n\n\nThere is still some strange pattern but the majority of the distribution seems to be between -1 and 1.\n\n\n5. Interpreting parameters\nBefore anything else, just plot the effects:\n\nplot(allEffects(fit_no_out))\n\n\n\n\nNow we can interpret model parameters:\n\nsummary(fit_no_out)\n\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6340  -0.7705  -0.4001   0.2804   2.1595  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.72884    0.39403   9.463  &lt; 2e-16 ***\nattachmentsecure -0.41074    0.16838  -2.439   0.0147 *  \ntemperamenteasy  -1.08473    0.15090  -7.188 6.56e-13 ***\nparent_se         0.01940    0.04055   0.478   0.6324    \nparent_skills    -0.53074    0.04109 -12.915  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  84.882  on 111  degrees of freedom\nAIC: 257.73\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe (Intercept) is the expected number of tantrums for “insecure”, “difficult” children where parent_skills are rated as 0 and parent self esteem is 0, thus 41.6307958. Similarly to the binomial lab, we could center the two numerical variables to have a more meaningful interpretation or we can use the predict function to obtain the values that we want.\n\npredict(fit_no_out, newdata = data.frame(attachment = \"insecure\", \n                                         temperament = \"difficult\",\n                                         parent_se = mean(dat$parent_se), \n                                         parent_skills = mean(dat$parent_skills)),\n        type = \"response\") # same as exp(prediction)\n\n       1 \n1.719356 \n\n\nThe attachmentsecure is the expected difference in log number of tantrums between secure - insecure attachment, controlling for other variables:\n\nemmeans(fit_no_out, pairwise~attachment)\n\n$emmeans\n attachment  emmean    SE  df asymp.LCL asymp.UCL\n insecure    0.0302 0.152 Inf    -0.268    0.3289\n secure     -0.3805 0.153 Inf    -0.679   -0.0816\n\nResults are averaged over the levels of: temperament \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast          estimate    SE  df z.ratio p.value\n insecure - secure    0.411 0.168 Inf   2.439  0.0147\n\nResults are averaged over the levels of: temperament \nResults are given on the log (not the response) scale. \n\n\nIn terms of the response scale, we can intepret it as the multiplicative increase of the number of tantrums from secure to insecure attachment:\n\nexp(coef(fit_no_out)[\"attachmentsecure\"])\n\nattachmentsecure \n       0.6631612 \n\n\nMoving from insecure from secure attachment, there is a decrease in the expected number of tantrums of 33.6838801 %.\nThe temperamenteasy can be interpreted in the same way:\n\nemmeans(fit_no_out, pairwise~temperament)\n\n$emmeans\n temperament emmean    SE  df asymp.LCL asymp.UCL\n difficult    0.367 0.143 Inf    0.0868     0.648\n easy        -0.717 0.152 Inf   -1.0161    -0.419\n\nResults are averaged over the levels of: attachment \nResults are given on the log (not the response) scale. \nConfidence level used: 0.95 \n\n$contrasts\n contrast         estimate    SE  df z.ratio p.value\n difficult - easy     1.08 0.151 Inf   7.188  &lt;.0001\n\nResults are averaged over the levels of: attachment \nResults are given on the log (not the response) scale. \n\n\n\nexp(coef(fit_no_out)[\"temperamenteasy\"])\n\ntemperamenteasy \n      0.3379941 \n\n\nSo there is a reduction of the 66.2005908 % by moving from difficult to easy temperament.\nparent_se and parent_skills are interpreted similarly. The coefficient represent the increase/decrease in the log number of tantrums for a unit increase in the predictors.\n\nexp(coef(fit_no_out)[4:5])\n\n    parent_se parent_skills \n    1.0195878     0.5881722 \n\n\nSo the number of tantrums seems to be unaffected by the parents self-esteem but as the parent skills increases there is a reduction in the number of tantrums.\n\n\n6. Model selection\nLet’s compare the model with and without the parent_se terms that appear to be not very useful:\n\nfit_no_parent_se &lt;- update(fit_no_out, . ~ . -parent_se)\nsummary(fit_no_parent_se)\n\n\nCall:\nglm(formula = tantrum ~ attachment + temperament + parent_skills, \n    family = poisson(link = \"log\"), data = dat_no_out)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6052  -0.7389  -0.4018   0.2766   2.1726  \n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       3.88096    0.23280  16.671  &lt; 2e-16 ***\nattachmentsecure -0.40422    0.16787  -2.408    0.016 *  \ntemperamenteasy  -1.08057    0.15080  -7.166 7.75e-13 ***\nparent_skills    -0.53723    0.03905 -13.757  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 382.413  on 115  degrees of freedom\nResidual deviance:  85.111  on 112  degrees of freedom\nAIC: 255.96\n\nNumber of Fisher Scoring iterations: 5\n\nanova(fit_no_parent_se, fit_no_out, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: tantrum ~ attachment + temperament + parent_skills\nModel 2: tantrum ~ attachment + temperament + parent_se + parent_skills\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       112     85.111                     \n2       111     84.882  1  0.22948   0.6319\n\n\n\ndrop1(fit_no_out, test = \"LRT\")\n\nSingle term deletions\n\nModel:\ntantrum ~ attachment + temperament + parent_se + parent_skills\n              Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;             84.882 257.73                      \nattachment     1   90.605 261.45   5.724   0.01674 *  \ntemperament    1  136.028 306.87  51.146 8.573e-13 ***\nparent_se      1   85.111 255.96   0.229   0.63191    \nparent_skills  1  304.711 475.56 219.829 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOr using the MuMIn::dredge() function:\n\nfit_no_out &lt;- update(fit_no_out, na.action = na.fail)\nMuMIn::dredge(fit_no_out, rank = \"AIC\")\n\nFixed term is \"(Intercept)\"\n\n\nGlobal model call: glm(formula = tantrum ~ attachment + temperament + parent_se + \n    parent_skills, family = poisson(link = \"log\"), data = dat_no_out, \n    na.action = na.fail)\n---\nModel selection table \n      (Int) att     prn_se prn_skl tmp df   logLik   AIC  delta weight\n14  3.88100   +            -0.5372   +  4 -123.978 256.0   0.00  0.608\n16  3.72900   +  0.0194000 -0.5307   +  5 -123.863 257.7   1.77  0.251\n13  3.48300                -0.5133   +  3 -126.768 259.5   3.58  0.102\n15  3.38600      0.0118300 -0.5090   +  4 -126.725 261.5   5.49  0.039\n5   2.89700                -0.5160      2 -150.397 304.8  48.84  0.000\n6   3.08300   +            -0.5235      3 -149.436 304.9  48.92  0.000\n7   2.96600     -0.0082560 -0.5189      3 -150.376 306.8  50.80  0.000\n8   3.08500   + -0.0002435 -0.5236      4 -149.436 306.9  50.92  0.000\n11  0.16220      0.1467000           +  3 -234.197 474.4 218.44  0.000\n12  0.06833   +  0.1469000           +  4 -233.778 475.6 219.60  0.000\n9   1.13900                          +  2 -243.108 490.2 234.26  0.000\n10  1.04200   +                      +  3 -242.642 491.3 235.33  0.000\n3  -0.48580      0.1398000              2 -264.575 533.1 277.19  0.000\n4  -0.52520   +  0.1395000              3 -264.498 535.0 279.04  0.000\n1   0.45590                             1 -272.629 547.3 291.30  0.000\n2   0.39690   +                         2 -272.475 549.0 292.99  0.000\nModels ranked by AIC(x) \n\n\n\n\n7. What about interactions?\nWe can also have a look at interactions, try by yourself to explore interactions between numerical (parent_skills and parent_se) and categorical (attachment and temperament) variables. I’m only interested in 1 continuous variable interacting with 1 categorical variable.\n\nfit a separate model for each interaction\ninterpret the model parameters and the analysis of deviance table (car:: something :)) or using a model comparison (Likelihood Ratio Test) for testing the interaction\nplot the model effects\ncomment the results"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "On this page are collected the slides presented during the course.\n\nCourse introduction\nIntroduction to GLM\nBinomial GLM\nPoisson GLM\nGamma GLM\nSimulating GLM"
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "On this page are collected the laboratories with real and simulated data.\n\nlab1\nlab2\nlab3\nlab4\nlab5\nlab6"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "This repository contains the materials for the course Generalized Linear Models from the PhD program in Psychological Sciences (University of Padova). The repository is organized as an R Package.\n\n\nTo start with the course you can download or clone the entire Git repository. Simply run git clone link or follow this link.\nThen you can open the GLMphd.Rproj file and R Studio will be opened on the correct folder. From now, everything is organized as a package. To load all the custom function of the R folder and datasets (data folder) you can use devtools::load_all(). This is the same as doing source(\"file.R\") without cluttering the global environment."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "To start with the course you can download or clone the entire Git repository. Simply run git clone link or follow this link.\nThen you can open the GLMphd.Rproj file and R Studio will be opened on the correct folder. From now, everything is organized as a package. To load all the custom function of the R folder and datasets (data folder) you can use devtools::load_all(). This is the same as doing source(\"file.R\") without cluttering the global environment."
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-1",
    "title": "Gamma GLM",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nThe Gamma distribution has several :parametrizations. One of the most common is the shape-scale parametrization:\n\\[\nf(x;k,\\theta )={\\frac {x^{k-1}e^{-x/\\theta }}{\\theta ^{k}\\Gamma (k)}}\n\\] Where \\(\\theta\\) is the scale parameter and \\(k\\) is the shape parameter."
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-2",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-2",
    "title": "Gamma GLM",
    "section": "Gamma distribution",
    "text": "Gamma distribution\n\nCodeggamma(mean = c(10, 20, 30), sd = c(10, 10, 10), show = \"ss\")"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-mu-and-sigma2",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-mu-and-sigma2",
    "title": "Gamma GLM",
    "section": "Gamma \\(\\mu\\) and \\(\\sigma^2\\)\n",
    "text": "Gamma \\(\\mu\\) and \\(\\sigma^2\\)\n\nThe mean and variance are defined as:\n\n\n\\(\\mu = k \\theta\\) and \\(\\sigma^2 = k \\theta^2\\) with the shape-scale parametrization\n\n\\(\\mu = \\frac{\\alpha}{\\beta}\\) and \\(\\frac{\\alpha}{\\beta^2}\\) with the shape-rate parametrization\n\n\nAnother important quantity is the coefficient of variation defined as \\(\\frac{\\sigma}{\\mu}\\) or \\(\\frac{1}{\\sqrt{k}}\\) (or \\(\\frac{1}{\\sqrt{\\alpha}}\\))."
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-3",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-distribution-3",
    "title": "Gamma GLM",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nAgain, we can see the mean-variance relationship:\n\nCodeggamma(shape = c(5, 5), scale = c(10, 20), show = \"ss\")"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization",
    "title": "Gamma GLM",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization\nTo convert between different parametrizations, you can use the gamma_params() function:\n\ngamma_params &lt;- function(shape = NULL, scale = 1/rate, rate = 1,\n                         mean = NULL, sd = NULL,\n                         eqs = FALSE){\n  if(eqs){\n    cat(rep(\"=\", 25), \"\\n\")\n    cat(eqs()$gamma, \"\\n\")\n    cat(rep(\"=\", 25), \"\\n\")\n  }else{\n      if(is.null(shape)){\n      var &lt;- sd^2\n      shape &lt;- mean^2 / var\n      scale &lt;- mean / shape\n      rate &lt;- 1/scale\n    } else if(is.null(mean) & is.null(sd)){\n      if(is.null(rate)){\n        scale &lt;- 1/rate\n      } else{\n        rate &lt;- 1/scale\n      }\n      mean &lt;- shape * scale\n      var &lt;- shape * scale^2\n      sd &lt;- sqrt(var)\n    }else{\n      stop(\"when shape and scale are provided, mean and sd need to be NULL (and viceversa)\")\n    }\n    out &lt;- list(shape = shape, scale = scale, rate = rate, mean = mean, var = var, sd = sd)\n    # coefficient of variation\n    out$cv &lt;- 1/sqrt(shape)\n    return(out)\n  }\n}"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization-1",
    "title": "Gamma GLM",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization\n\ngm &lt;- gamma_params(mean = 30, sd = 10)\nunlist(gm)\n\n#&gt;       shape       scale        rate        mean         var          sd \n#&gt;   9.0000000   3.3333333   0.3000000  30.0000000 100.0000000  10.0000000 \n#&gt;          cv \n#&gt;   0.3333333\n\ny &lt;- rgamma(1000, shape = gm$shape, scale = gm$scale)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization-2",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#gamma-parametrization-2",
    "title": "Gamma GLM",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\n\nUsing the gamma_params() function we can think in terms of \\(\\mu\\) and \\(\\sigma\\) and generate the right parameters (e.g., shape and rate).\nLet’s simulate observations from a Gamma distribution with \\(\\mu = 500\\) and \\(\\sigma = 200\\)\n\n\n\nCodegm &lt;- gamma_params(mean = 500, sd = 200)\ny &lt;- rgamma(1e4, shape = gm$shape, scale = gm$scale)\nhist(y, breaks = 100, col = \"dodgerblue\")"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-1",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nThen we can fit an intercept-only model with the Gamma family and a log link function. You have to specify the link because the default is inverse.\n\nCodefam &lt;- Gamma(link = \"log\")\ndat &lt;- data.frame(y)\nfit0 &lt;- glm(y ~ 1, family = fam, data = dat)\nsummary(fit0)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ 1, family = fam, data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -1.57209  -0.32203  -0.05245   0.21919   1.61157  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  6.21446    0.00399    1558   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Gamma family taken to be 0.1591893)\n#&gt; \n#&gt;     Null deviance: 1640  on 9999  degrees of freedom\n#&gt; Residual deviance: 1640  on 9999  degrees of freedom\n#&gt; AIC: 133226\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-2",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-2",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\n\n\n\\(\\beta_0\\) is the \\(\\mu\\) of the Gamma distribution. We need to apply the inverse (exp) to get the original scale:\n\n\nCodec(mean = mean(dat$y), mean_true = gm$mean, b0 = exp(coef(fit0)[1]))\n\n#&gt;           mean      mean_true b0.(Intercept) \n#&gt;       499.9283       500.0000       499.9283\n\n\n\nas always you can use the fam() object if you are not sure about the link functions as fam$linkinv(coef(fit0)[1])"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-3",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-3",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nNow let’s simulate the difference between two groups. Again fixing the \\(\\mu_0 = 500\\), \\(\\mu_1 = 600\\) and a common \\(\\sigma = 200\\). Let’s plot the empirical densities:\n\nCodeggamma(mean = c(500, 600), sd = c(200, 200))"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-4",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-4",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nUsing the group variable as dummy-coded, \\(\\beta_0 = \\mu_0\\) and \\(\\beta_1 = \\mu_1 - \\mu_0\\). Note that we are in the log scale.\n\nns &lt;- 1e4\nm0 &lt;- 500\nm1 &lt;- 600\ns &lt;- 200\n# parameters, log link\nb0 &lt;- log(m0)\nb1 &lt;- log(m1) - log(m0) # equivalent to log(m1 / m0)\nx &lt;- rep(c(0, 1), each = ns/2)\nlp &lt;- b0 + b1 * x # linear predictor\nmu &lt;- exp(lp) # inverse exp link\ngm &lt;- gamma_params(mean = mu, sd = s)\ny &lt;- rgamma(ns, shape = gm$shape, scale = gm$scale)\ndat &lt;- data.frame(y, x)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-5",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-5",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nLet’s see the simulated data:\n\nCodepar(mfrow = c(1,2))\nhist(dat$y, col = \"dodgerblue\", breaks = 100)\nboxplot(y ~ x, data = dat)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-6",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-6",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nNow we can fit the model and extract the parameters:\n\nCodefit &lt;- glm(y ~ x, data = dat, family = fam)\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = fam, data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -1.68207  -0.28735  -0.04518   0.19373   1.33198  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 6.218306   0.005208 1194.09   &lt;2e-16 ***\n#&gt; x           0.171225   0.007365   23.25   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Gamma family taken to be 0.135594)\n#&gt; \n#&gt;     Null deviance: 1444.1  on 9999  degrees of freedom\n#&gt; Residual deviance: 1370.9  on 9998  degrees of freedom\n#&gt; AIC: 133449\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-7",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-7",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\n\n\n\\(\\beta_0\\) is the mean of the first group and \\(\\beta_1\\) is the \\(\\log(\\mu_1/\\mu_0)\\) or the difference \\(\\log(\\mu_1) - log(\\mu_0)\\)\n\n\n\nmm &lt;- tapply(dat$y, dat$x, mean)\ncoefs &lt;- coef(fit)\n# manually\nc(mm[\"0\"], mm[\"1\"], diff = log(mm[\"1\"]) - log(mm[\"0\"]))\n\n#&gt;           0           1      diff.1 \n#&gt; 501.8524463 595.5770897   0.1712247\n\n# model\nc(exp(coefs[1]), exp(coefs[1] + coefs[2]), coefs[2])\n\n#&gt; (Intercept) (Intercept)           x \n#&gt; 501.8524463 595.5770897   0.1712247"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-8",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-parametrization-8",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) parametrization",
    "text": "\\(\\mu\\) and \\(\\sigma\\) parametrization\nThe other estimated parameter is the dispersion that is defined as the inverse of the shape. We have not a single shape but the average is roughly similar to the true value.\n\nfits &lt;- summary(fit)\nfits$dispersion\n\n#&gt; [1] 0.135594\n\n1/mean(unique(gm$shape))\n\n#&gt; [1] 0.1311475"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-shape-parametrization",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-shape-parametrization",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and shape parametrization",
    "text": "\\(\\mu\\) and shape parametrization\n\nThis is common in brms and other packages1. The \\(\\mu\\) is the same as before and the shape (\\(\\alpha\\)) determine the skewness of the distribution. For the Gamma, the skewness is calculated as \\(\\frac{2}{\\sqrt{\\alpha}}\\).\nTo generate data, we calculate the scale (\\(\\theta\\)) as \\(\\frac{\\mu}{\\alpha}\\) (remember that \\(\\mu = \\alpha\\theta\\))\n\n\nCodemu &lt;- 50\nshape &lt;- 10\ny &lt;- rgamma(1e4, shape = shape, scale = mu/shape)\nhist(y, col = \"dodgerblue\", breaks = 100)\n\n\n\n\n\n\n\nSee an example https://rpubs.com/jwesner/gamma_glm"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-shape-parametrization-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-shape-parametrization-1",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and shape parametrization",
    "text": "\\(\\mu\\) and shape parametrization\n\nthe expected skewness is 2/sqrt(shape) 0.632 and is similar to the value computed on the simulated data\n\n\n2/sqrt(shape)\n\n#&gt; [1] 0.6324555\n\npsych::skew(y)\n\n#&gt; [1] 0.6279813\n\n\n\nas \\(\\alpha\\) increase, the Gamma distribution is less skewed and approaches a Gaussian distribution. When \\(\\mu = \\alpha\\) the distribution already start to be pretty Gaussian"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#skewness---alpha-relationship",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#skewness---alpha-relationship",
    "title": "Gamma GLM",
    "section": "Skewness - \\(\\alpha\\) relationship",
    "text": "Skewness - \\(\\alpha\\) relationship\nWe can plot the function that determine the skewness of the Gamma fixing \\(\\mu\\) and varying \\(\\alpha\\):\n\nCodecurve(2/sqrt(x), 0, 50, ylab = \"Skewness\", xlab = latex(\"\\\\alpha (shape)\"))"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#skewness---alpha-relationship-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#skewness---alpha-relationship-1",
    "title": "Gamma GLM",
    "section": "Skewness - \\(\\alpha\\) relationship",
    "text": "Skewness - \\(\\alpha\\) relationship\nCompared to the \\(\\mu\\)-\\(\\sigma\\) method, here we fix the skewness and \\(\\mu\\), thus the \\(\\hat \\sigma\\) will differ when \\(\\mu\\) change but the skewness is the same. The opposite is also true.\n\nCodemu &lt;- c(50, 80)\n\n# mu-shape parametrization\ny1 &lt;- rgamma(1e6, shape = 10, scale = mu[1]/10)\ny2 &lt;- rgamma(1e6, shape = 10, scale = mu[2]/10)\n\n# mu-sigma parametrization\ngm &lt;- gamma_params(mean = mu, sd = c(20, 20))\nx1 &lt;- rgamma(1e6, shape = gm$shape[1], scale = gm$scale[1])\nx2 &lt;- rgamma(1e6, shape = gm$shape[2], scale = gm$scale[2])\n\npar(mfrow = c(1,2))\n\nplot(density(y1), lwd = 2, main = latex(\"\\\\mu and \\\\alpha parametrization\"), xlab = \"x\", xlim = c(0, 250))\nlines(density(y2), col = \"firebrick\", lwd = 2)\nlegend(\"topright\", \n       legend = c(latex(\"\\\\mu = %s, \\\\alpha = %s, \\\\hat{\\\\sigma} = %.0f, sk = %.2f\", mu[1], 10, sd(y1), psych::skew(y1)),\n                  latex(\"\\\\mu = %s, \\\\alpha = %s, \\\\hat{\\\\sigma} = %.0f, sk = %.2f\", mu[2], 10, sd(y2), psych::skew(y1))),\n       fill = c(\"black\", \"firebrick\"))\n\nhatshape &lt;- c(gamma_shape(x1, \"invskew\"), gamma_shape(x2, \"invskew\"))\n\nplot(density(x1), lwd = 2, main = latex(\"\\\\mu and \\\\sigma parametrization\"), xlab = \"x\", xlim = c(0, 250))\nlines(density(x2), col = \"firebrick\", lwd = 2)\nlegend(\"topright\", \n       legend = c(latex(\"\\\\mu = %s, \\\\sigma = %s, \\\\hat{\\\\alpha} = %.0f, sk = %.2f\", mu[1], 20, hatshape[1], psych::skew(x1)),\n                  latex(\"\\\\mu = %s, \\\\sigma = %s, \\\\hat{\\\\alpha} = %.0f, sk = %.2f\", mu[2], 20, hatshape[2], psych::skew(x2))),\n       fill = c(\"black\", \"firebrick\"))"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#coefficient-of-variation",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#coefficient-of-variation",
    "title": "Gamma GLM",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\n\nThe coefficient of variation \\(\\frac{\\sigma}{\\mu} = \\frac{1}{\\sqrt{\\alpha}}\\) is constant under the \\(\\mu\\)-\\(\\alpha\\) parametrization while can be different under the \\(\\mu\\)-\\(\\sigma\\) one when \\(\\alpha\\) or \\(\\sigma\\) is fixed across conditions.\n\nCode# mu-shape\nc(cv(y1), cv(y2))\n\n#&gt; [1] 0.3164741 0.3161030\n\nCode# mu-sigma\nc(cv(x1), cv(x2))\n\n#&gt; [1] 0.3996642 0.2503175\n\n\nThe \\(\\alpha\\) parameter allow to control the coefficient of variation."
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-relationship",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-relationship",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) relationship",
    "text": "\\(\\mu\\) and \\(\\sigma\\) relationship\nSee https://civil.colorado.edu/~balajir/CVEN6833/lectures/GammaGLM-01.pdf. The \\(\\sigma = \\frac{\\mu}{\\sqrt{\\alpha}}\\).\n\nCodemu &lt;- 50\ncurve(mu / sqrt(x), 0, 100, xlab = latex(\"\\\\alpha\"), ylab = latex(\"\\\\sigma = \\\\mu/\\\\sqrt{\\\\alpha}\"))"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-relationship-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#mu-and-sigma-relationship-1",
    "title": "Gamma GLM",
    "section": "\n\\(\\mu\\) and \\(\\sigma\\) relationship",
    "text": "\\(\\mu\\) and \\(\\sigma\\) relationship\nAs noted by Agresti (2015), fixing \\(\\alpha\\) and varying \\(\\mu\\), the coefficient of variation will be constant and the standard deviation \\(\\sigma\\) increase proportionally with \\(\\mu\\). Given that \\(\\sigma = \\frac{\\mu}{\\sqrt{\\alpha}}\\):\n\nmu1 &lt;- 20\nmu2 &lt;- 40\nshape &lt;- 10 # alpha\ny1 &lt;- rgamma(1e5, shape = shape, scale = mu1/shape)\ny2 &lt;- rgamma(1e5, shape = shape, scale = mu2/shape)\n\nc(mean(y1), mean(y2))\n#&gt; [1] 20.01384 40.03475\nc(sd(y1), sd(y2)) # sd increase\n#&gt; [1]  6.294855 12.654074\nc(cv(y1), cv(y2)) # cv is constant\n#&gt; [1] 0.3145251 0.3160773\nc(psych::skew(y1), psych::skew(y2)) # skewness is similar\n#&gt; [1] 0.6206598 0.6351052"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\n\nThe Simon effect is the difference in accuracy or reaction time between trials in which stimulus and response are on the same side and trials in which they are on opposite sides, with responses being generally slower and less accurate when the stimulus and response are on opposite sides.\n\n\nSource: Wildenberg et al. (2010)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-1",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-1",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nLet’s import the data/simon.rda file1. You can use the load() function or the read_rda().\n\nCodesimon &lt;- read_rda(here(\"data\", \"simon.rda\"))\nhead(simon)\n\n#&gt; # A tibble: 6 × 15\n#&gt;   submission_id    RT condition   correctness class    experiment_id key_pressed\n#&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n#&gt; 1          7432  1239 incongruent correct     Intro C…            52 q          \n#&gt; 2          7432   938 incongruent correct     Intro C…            52 q          \n#&gt; 3          7432   744 incongruent correct     Intro C…            52 q          \n#&gt; 4          7432   528 incongruent correct     Intro C…            52 q          \n#&gt; 5          7432   706 incongruent correct     Intro C…            52 p          \n#&gt; 6          7432   547 congruent   correct     Intro C…            52 p          \n#&gt; # ℹ 8 more variables: p &lt;chr&gt;, pause &lt;dbl&gt;, q &lt;chr&gt;, target_object &lt;chr&gt;,\n#&gt; #   target_position &lt;chr&gt;, timeSpent &lt;dbl&gt;, trial_number &lt;dbl&gt;,\n#&gt; #   trial_type &lt;chr&gt;\n\n\nSource: https://github.com/michael-franke/aida-package"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-2",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-2",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nFor simplicity, let’s consider only a single subject (submission_id: 7432), otherwise the model require including random effects. We also exclude strange trials with RT &gt; 2500 ms.\n\nsimon &lt;- filter(simon, \n                submission_id == 7432,\n                RT &lt; 2500)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-3",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-3",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nLet’s plot the reaction times. Clearly the two distributions are right-skewed with a difference in location (\\(\\mu\\)). The shape also differs between thus also the skewness is probably different:\n\nCodeggplot(simon, aes(x = RT, fill = condition)) +\n  geom_density(alpha = 0.7)"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-4",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-4",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nLet’s see some summary statistics. We see the difference between the two conditions.\n\nfuns &lt;- list(mean = mean, sd = sd, skew = psych::skew, cv = cv)\nsumm &lt;- tapply(simon$RT, simon$condition, function(x) sapply(funs, function(f) f(x)))\nsumm\n\n#&gt; $congruent\n#&gt;        mean          sd        skew          cv \n#&gt; 504.1818182  81.7038670   0.5328521   0.1620524 \n#&gt; \n#&gt; $incongruent\n#&gt;        mean          sd        skew          cv \n#&gt; 564.0312500 123.4188852   2.8702013   0.2188157"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-5",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-5",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nGiven that we modelling the difference in \\(\\mu\\), this is the expected difference. We are working on the log scale, thus the model is estimating the log difference or the log ratio.\n\nsumm$incongruent[\"mean\"] - summ$congruent[\"mean\"]\n#&gt;     mean \n#&gt; 59.84943\nlog(summ$incongruent[\"mean\"]) - log(summ$congruent[\"mean\"])\n#&gt;      mean \n#&gt; 0.1121727\nlog(summ$incongruent[\"mean\"] / summ$congruent[\"mean\"])\n#&gt;      mean \n#&gt; 0.1121727"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-6",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-6",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nLet’s fit the model:\n\nfit &lt;- glm(RT ~ condition, data = simon, family = Gamma(link = \"log\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = RT ~ condition, family = Gamma(link = \"log\"), data = simon)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -0.44206  -0.12874  -0.04505   0.08526   0.90525  \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           6.22294    0.02625 237.053  &lt; 2e-16 ***\n#&gt; conditionincongruent  0.11217    0.03580   3.134  0.00218 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Gamma family taken to be 0.03790215)\n#&gt; \n#&gt;     Null deviance: 4.1148  on 118  degrees of freedom\n#&gt; Residual deviance: 3.7439  on 117  degrees of freedom\n#&gt; AIC: 1424.4\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-7",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-7",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nPlotting the results:\n\nplot(ggeffect(fit))\n\n#&gt; $condition"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-8",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#example-the-simon-effect-8",
    "title": "Gamma GLM",
    "section": "Example: the Simon effect",
    "text": "Example: the Simon effect\nThe main parameter of interest here is the \\(\\beta_1\\) representing the difference in \\(\\mu\\). We can interpret \\(\\exp(\\beta_1) = 1.119\\) as the multiplicative increase in RT when moving from congruent to incongruent condition. In the RT scale, we have a difference of 59.8494318. Remember that the statistical test is performed on the link-function scale.\n\nemmeans(fit, pairwise ~ condition)$contrast\n#&gt;  contrast                estimate     SE  df t.ratio p.value\n#&gt;  congruent - incongruent   -0.112 0.0358 117  -3.134  0.0022\n#&gt; \n#&gt; Results are given on the log (not the response) scale.\nemmeans(fit, pairwise ~ condition, type = \"response\")$contrast\n#&gt;  contrast                ratio    SE  df null t.ratio p.value\n#&gt;  congruent / incongruent 0.894 0.032 117    1  -3.134  0.0022\n#&gt; \n#&gt; Tests are performed on the log scale"
  },
  {
    "objectID": "slides/04-gamma-glm/04-gamma-glm.html#references",
    "href": "slides/04-gamma-glm/04-gamma-glm.html#references",
    "title": "Gamma GLM",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. https://play.google.com/store/books/details?id=jlIqBgAAQBAJ\n\n\nWildenberg, W. P. M. van den, Wylie, S. A., Forstmann, B. U., Burle, B., Hasbroucq, T., & Ridderinkhof, K. R. (2010). To head or to heed? Beyond the surface of selective action inhibition: A review. Frontiers in Human Neuroscience, 4, 222. https://doi.org/10.3389/fnhum.2010.00222"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution",
    "href": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution",
    "title": "Generalized Linear Models",
    "section": "Quick recap about Gaussian distribution",
    "text": "Quick recap about Gaussian distribution\n\nThe Gaussian distribution is part of the Exponential family\n\nIt is defined with mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\))\nIt is symmetric (mean, mode and median are the same)\nThe support is \\([- \\infty, + \\infty]\\)\n\n\n\nThe Probability Density Function (PDF) is:\n\\[\nf(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\n\\]"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution-1",
    "title": "Generalized Linear Models",
    "section": "Quick recap about Gaussian distribution",
    "text": "Quick recap about Gaussian distribution"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution-2",
    "href": "slides/01-intro-glm/01-intro-glm.html#quick-recap-about-gaussian-distribution-2",
    "title": "Generalized Linear Models",
    "section": "Quick recap about Gaussian distribution",
    "text": "Quick recap about Gaussian distribution"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#but",
    "href": "slides/01-intro-glm/01-intro-glm.html#but",
    "title": "Generalized Linear Models",
    "section": "But…",
    "text": "But…\nIn Psychology, variables do not always satisfy the properties of the Gaussian distribution. For example:\n\nReaction times\nPercentages or proportions (e.g., task accuracy)\nCounts\nLikert scales\n…"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#reaction-times",
    "href": "slides/01-intro-glm/01-intro-glm.html#reaction-times",
    "title": "Generalized Linear Models",
    "section": "Reaction times",
    "text": "Reaction times\nNon-negative and probably skewed data:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binary-outcomes",
    "href": "slides/01-intro-glm/01-intro-glm.html#binary-outcomes",
    "title": "Generalized Linear Models",
    "section": "Binary outcomes",
    "text": "Binary outcomes\nA series of binary (i.e., bernoulli) trials with two possible outcomes:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#counts",
    "href": "slides/01-intro-glm/01-intro-glm.html#counts",
    "title": "Generalized Linear Models",
    "section": "Counts",
    "text": "Counts\nNumber of symptoms for a group of patients during the last month:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-1",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nExample: probability of passing the exam as a function of hours of study:\n\n\n\n\n\n\n\n\n\nid\n      studyh\n      passed\n    \n\n\n1\n59\n1\n\n\n2\n41\n0\n\n\n3\n51\n0\n\n\n4\n26\n0\n\n\n...\n...\n...\n\n\n47\n57\n0\n\n\n48\n89\n1\n\n\n49\n50\n0\n\n\n50\n41\n0\n\n\n\n\n\n\n\n\ndat |&gt; \n  summarise(n = n(),\n            npass = sum(passed),\n            nfail = n - npass,\n            ppass = npass / n)\n\n#&gt;    n npass nfail ppass\n#&gt; 1 50    19    31  0.38"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-2",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-2",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nLet’s plot the data:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-3",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-3",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nLet’s fit a linear model passing ~ study_hours using lm:\n\n\nDo you see something strange?"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-4",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-4",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nA little spoiler, the relationship should be probably like this:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-5",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-5",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nAnother example, the number of solved exercises in a semester as a function of the number of attended lectures (\\(N = 100\\)):\n\n\n\n\n\n\n\nid\n      nattended\n      nsolved\n    \n\n\n1\n5\n0\n\n\n2\n20\n0\n\n\n3\n53\n15\n\n\n4\n19\n3\n\n\n...\n...\n...\n\n\n97\n43\n5\n\n\n98\n60\n33\n\n\n99\n33\n8\n\n\n100\n49\n10"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-6",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-6",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-7",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-7",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-8",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-8",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nAgain, fitting the linear model seems partially appropriate but there are some problems:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-9",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-9",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nAgain, fitting the linear model seems partially appropriate but there are some problems:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-10",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-10",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nAlso the residuals are quite problematic:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-11",
    "href": "slides/01-intro-glm/01-intro-glm.html#should-we-use-a-linear-model-for-these-variables-11",
    "title": "Generalized Linear Models",
    "section": "Should we use a linear model for these variables?",
    "text": "Should we use a linear model for these variables?\nAnother little spoiler, the model should consider both the support of the y variable and the non-linear pattern. Probably something like this:"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#so-what",
    "href": "slides/01-intro-glm/01-intro-glm.html#so-what",
    "title": "Generalized Linear Models",
    "section": "So what?",
    "text": "So what?\nBoth linear models somehow capture the expected relationship but there are serious fitting problems:\n\nimpossible predictions\npoor fitting for non-linear patterns\nlinear regression assumptions not respected\n\n\nAs a general rule in life statistics:\n\nAll models are wrong, some are useful"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#we-need-a-new-class-of-models",
    "href": "slides/01-intro-glm/01-intro-glm.html#we-need-a-new-class-of-models",
    "title": "Generalized Linear Models",
    "section": "We need a new class of models…",
    "text": "We need a new class of models…\n\nTaking into account the specific features of our response variable\n\nWorking on a linear scale when fitting the model and for inference\nWe need a model that is closer to the true data generation process"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nFor a detailed introduction about GLMs\n\nChapters: 1 (intro), 4 (GLM fitting), 5 (GLM for binary data)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references-1",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nFor a basic and well written introduction about GLM, especially the Binomial GLM\n\nChapters: 3 (intro GLMs), 4-5 (Binomial Logistic Regression)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references-2",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references-2",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nGreat resource for interpreting Binomial GLM parameters:\n\nChapters: 13-14 (Binomial Logistic GLM), 15 (Poisson and others GLMs)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references-3",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references-3",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nDetailed GLMs book. Very useful especially for the diagnostic part:\n\nChapters: 8 (intro), 9 (Binomial GLM), 10 (Poisson GLM and overdispersion)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references-4",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references-4",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nThe holy book :)\n\nChapters: 14 and 15"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#main-references-5",
    "href": "slides/01-intro-glm/01-intro-glm.html#main-references-5",
    "title": "Generalized Linear Models",
    "section": "Main references",
    "text": "Main references\nAnother good reference…\n\nChapters: 8"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#general-idea",
    "href": "slides/01-intro-glm/01-intro-glm.html#general-idea",
    "title": "Generalized Linear Models",
    "section": "General idea",
    "text": "General idea\n\nusing distributions beyond the Gaussian\n\nmodeling non linear functions on the response scale\ntaking into account mean-variance relationships"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#recipe-for-a-glm",
    "href": "slides/01-intro-glm/01-intro-glm.html#recipe-for-a-glm",
    "title": "Generalized Linear Models",
    "section": "Recipe for a GLM",
    "text": "Recipe for a GLM\n\nRandom Component\nSystematic Component\nLink Function"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#random-component",
    "href": "slides/01-intro-glm/01-intro-glm.html#random-component",
    "title": "Generalized Linear Models",
    "section": "Random Component",
    "text": "Random Component\nThe random component of a GLM identify the response variable \\(y\\) coming from a certain probability distribution."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#systematic-component",
    "href": "slides/01-intro-glm/01-intro-glm.html#systematic-component",
    "title": "Generalized Linear Models",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component or linear predictor (\\(\\eta\\)) of a GLM is \\(\\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\\).\n\\[\\begin{align*}\n\\eta = \\beta_0 + \\beta_1x_1 + ... + \\beta_px_p\n\\end{align*}\\]\nThis part is invariant to the type of model and is the combination of explanatory variables to predict the expected value \\(\\mu\\) (i.e. the mean) of the distribution."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#link-function",
    "href": "slides/01-intro-glm/01-intro-glm.html#link-function",
    "title": "Generalized Linear Models",
    "section": "Link Function",
    "text": "Link Function\nThe link function \\(g(\\mu)\\) is an invertible function that connects the mean \\(\\mu\\) of the random component with the linear combination of predictors \\(g(\\mu) = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\\). The inverse of the link function \\(g^{-1}\\) map the linear predictor (\\(\\eta\\)) into the original scale.\n\\[\ng(\\mu) = \\eta = \\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi}\n\\]\n\\[\n\\mu = g^{-1}(\\eta) = g^{-1}(\\beta_0 + \\beta_1x_{1i} + ... + \\beta_px_{pi})\n\\]\nThus, the relationship between \\(\\mu\\) and \\(\\eta\\) is linear only when the link function  is applied i.e. \\(g(\\mu) = \\eta\\)."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#link-function-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#link-function-1",
    "title": "Generalized Linear Models",
    "section": "Link function",
    "text": "Link function\nThe simplest link function is the identity link where \\(g(\\mu) = \\mu\\) and correspond to the standard linear model. In fact, the linear regression is just a GLM with a Gaussian random component and the identity link function.\n\n\n\n\n\n\nMain distributions and link functions\n  \nFamily\n      Link\n      Range\n    \n\n\n`gaussian`\nidentity\n$$(-\\infty,+\\infty)$$\n\n\n`gamma`\nlog\n$$(0,+\\infty)$$\n\n\n`binomial`\nlogit\n$$\\frac{0, 1, ..., n_{i}}{n_{i}}$$\n\n\n`binomial`\nprobit\n$$\\frac{0, 1, ..., n_{i}}{n_{i}}$$\n\n\n`poisson`\nlog\n$$0, 1, 2, ...$$"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#bernoulli-distribution",
    "href": "slides/01-intro-glm/01-intro-glm.html#bernoulli-distribution",
    "title": "Generalized Linear Models",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nA single Bernoulli trial is defined as:\n\\[\nf(x, p) = p^x (1 - p)^{1 - x}\n\\]\nWhere \\(p\\) is the probability of success and \\(k\\) the two possible results \\(0\\) and \\(1\\). The mean is \\(p\\) and the variance is \\(p(1 - p)\\)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binomial-distribution",
    "href": "slides/01-intro-glm/01-intro-glm.html#binomial-distribution",
    "title": "Generalized Linear Models",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe probability of having \\(k\\) success (e.g., 0, 1, 2, etc.), out of \\(n\\) trials with a probability of success \\(p\\) (and failing \\(q = 1 - p\\)) is:\n\\[\nf(n, k, p)= \\binom{n}{k} p^k(1 - p)^{n - k}\n\\]\nThe \\(np\\) is the mean of the binomial distribution is \\(np\\) is the variance \\(npq = np(1-p)\\). The binomial distribution is just the repetition of \\(n\\) independent Bernoulli trials."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#bernoulli-and-binomial",
    "href": "slides/01-intro-glm/01-intro-glm.html#bernoulli-and-binomial",
    "title": "Generalized Linear Models",
    "section": "Bernoulli and Binomial",
    "text": "Bernoulli and Binomial\nA classical example for a Bernoulli trial is the coin flip. In R:\n\nn &lt;- 1\np &lt;- 0.7\nrbinom(1, n, p) # a single bernoulli trial\n\n#&gt; [1] 1\n\nn &lt;- 10\nrbinom(10, 1, p) # n bernoulli trials\n\n#&gt;  [1] 1 1 1 1 1 0 1 1 1 1\n\nrbinom(1, n, p) # binomial version\n\n#&gt; [1] 8"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binomial",
    "href": "slides/01-intro-glm/01-intro-glm.html#binomial",
    "title": "Generalized Linear Models",
    "section": "Binomial",
    "text": "Binomial\n\nCoden &lt;- 30\np &lt;- 0.7\ndat &lt;- data.frame(k = 0:n)\ndat$y &lt;- dbinom(dat$k, n, p)\n\ndat |&gt; \n    ggplot(aes(x = k, y = y)) +\n    geom_point() +\n    geom_segment(aes(x = k, xend = k, y = 0, yend = y)) +\n    mytheme() +\n    ylab(\"dbinom(x, n, p)\") +\n    ggtitle(latex2exp::TeX(\"$n = 30$, $p = 0.7$\"))"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binomial-in-r",
    "href": "slides/01-intro-glm/01-intro-glm.html#binomial-in-r",
    "title": "Generalized Linear Models",
    "section": "Binomial in R",
    "text": "Binomial in R\n\n# generate k (success)\nn &lt;- 50 # number of trials\np &lt;- 0.7 # probability of success\nrbinom(1, n, p)\n#&gt; [1] 35\n\n# let's do several experiments (e.g., participants)\nrbinom(10, n, p)\n#&gt;  [1] 32 31 32 34 30 36 37 39 34 36\n\n# calculate the probability density given k successes\nn &lt;- 50\nk &lt;- 25\np &lt;- 0.5\ndbinom(k, n, p)\n#&gt; [1] 0.1122752\n\n# calculate the probability of doing 0, 1, 2, up to k successes\nn &lt;- 50\nk &lt;- 25\np &lt;- 0.5\npbinom(k, n, p)\n#&gt; [1] 0.5561376"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binomial-glm",
    "href": "slides/01-intro-glm/01-intro-glm.html#binomial-glm",
    "title": "Generalized Linear Models",
    "section": "Binomial GLM",
    "text": "Binomial GLM\nThe Bernoulli distributions is used as random component when we have a binary dependent variable or the number of successes over the total number of trials:\n\nAccuracy on a cognitive task\nPatients recovered or not after a treatment\nPeople passing or not the exam\n\n\nThe Bernoulli or the Binomial distributions can be used as random component when we have a binary dependent variable or the number of successes over the total number of trials.\n\n\nWhen fitting a GLM with the binomial distribution we are including linear predictors on the expected value \\(\\mu\\) i.e. the probability of success."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#binomial-glm-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#binomial-glm-1",
    "title": "Generalized Linear Models",
    "section": "Binomial GLM",
    "text": "Binomial GLM\nMost of the GLM models deal with a mean-variance relationship:\n\nCodep &lt;- seq(0, 1, 0.01)\npar(mfrow = c(1,2))\ncurve(plogis(x), \n      -4, 4, \n      ylab = \"Probability\", \n      xlab = \"x\", \n      lwd = 2,\n      main = \"Logistic Distribution\")\nplot(p, \n     p * (1 - p), \n     xlim = c(0, 1),\n     xlab = latex(\"\\\\mu = p\"), \n     ylab = latex(\"V = p(1 - p)\"),\n     type = \"l\",\n     lwd = 2,\n     main = \"Mean-Variance Relationship\")"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution",
    "href": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution",
    "title": "Generalized Linear Models",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe number of events \\(k\\) during a fixed time interval (e.g., number of new users on a website in 1 week) is:\n\\[\\begin{align*}\nf(k,\\lambda) = Pr(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\end{align*}\\]\nWhere \\(k\\) is the number of occurrences (\\(k = 0, 1, 2, ...\\)), \\(e\\) is Euler’s number (\\(e = 2.71828...\\)) and \\(!\\) is the factorial function. The mean and the variance of the Poisson distribution is \\(\\lambda\\)."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-1",
    "title": "Generalized Linear Models",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\nAs \\(\\lambda\\) increases, the distribution is well approximated by a Gaussian distribution, but the Poisson is discrete."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-in-r",
    "href": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-in-r",
    "title": "Generalized Linear Models",
    "section": "Poisson distribution in R",
    "text": "Poisson distribution in R\n\nn &lt;- 1000\nx &lt;- rpois(n, lambda = 30)\n\nmean(x)\n\n#&gt; [1] 29.993\n\nvar(x)\n\n#&gt; [1] 31.4484\n\n\nWe can also use all the other functions such as the dpois(), qpois() and ppois()"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-in-r-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#poisson-distribution-in-r-1",
    "title": "Generalized Linear Models",
    "section": "Poisson distribution in R",
    "text": "Poisson distribution in R\nThe mean-variance relationship can be easily seen with a continuous predictor:\n\nCodex &lt;- rnorm(1000)\ny &lt;- rpois(1000, exp(log(10) + log(1.8)*x))\n\nplot(x, y, pch = 19)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution",
    "title": "Generalized Linear Models",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nThe Gamma distribution has several :parametrizations. One of the most common is the shape-scale parametrization:\n\\[\nf(x;k,\\theta )={\\frac {x^{k-1}e^{-x/\\theta }}{\\theta ^{k}\\Gamma (k)}}\n\\] Where \\(\\theta\\) is the scale parameter and \\(k\\) is the shape parameter."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution-1",
    "title": "Generalized Linear Models",
    "section": "Gamma distribution",
    "text": "Gamma distribution\n\nCodeggamma(mean = c(10, 20, 30), sd = c(10, 10, 10), show = \"ss\")"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-mu-and-sigma2",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-mu-and-sigma2",
    "title": "Generalized Linear Models",
    "section": "Gamma \\(\\mu\\) and \\(\\sigma^2\\)\n",
    "text": "Gamma \\(\\mu\\) and \\(\\sigma^2\\)\n\nThe mean and variance are defined as:\n\n\n\\(\\mu = k \\theta\\) and \\(\\sigma^2 = k \\theta^2\\) with the shape-scale parametrization\n\n\\(\\mu = \\frac{\\alpha}{\\beta}\\) and \\(\\frac{\\alpha}{\\beta^2}\\) with the shape-rate parametrization\n\n\nAnother important quantity is the coefficient of variation defined as \\(\\frac{\\sigma}{\\mu}\\) or \\(\\frac{1}{\\sqrt{k}}\\) (or \\(\\frac{1}{\\sqrt{\\alpha}}\\))."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution-2",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-distribution-2",
    "title": "Generalized Linear Models",
    "section": "Gamma distribution",
    "text": "Gamma distribution\nAgain, we can see the mean-variance relationship:\n\nCodeggamma(shape = c(5, 5), scale = c(10, 20), show = \"ss\")"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization",
    "title": "Generalized Linear Models",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization\nTo convert between different parametrizations, you can use the gamma_params() function:\n\ngamma_params &lt;- function(shape = NULL, scale = 1/rate, rate = 1,\n                         mean = NULL, sd = NULL,\n                         eqs = FALSE){\n  if(eqs){\n    cat(rep(\"=\", 25), \"\\n\")\n    cat(eqs()$gamma, \"\\n\")\n    cat(rep(\"=\", 25), \"\\n\")\n  }else{\n      if(is.null(shape)){\n      var &lt;- sd^2\n      shape &lt;- mean^2 / var\n      scale &lt;- mean / shape\n      rate &lt;- 1/scale\n    } else if(is.null(mean) & is.null(sd)){\n      if(is.null(rate)){\n        scale &lt;- 1/rate\n      } else{\n        rate &lt;- 1/scale\n      }\n      mean &lt;- shape * scale\n      var &lt;- shape * scale^2\n      sd &lt;- sqrt(var)\n    }else{\n      stop(\"when shape and scale are provided, mean and sd need to be NULL (and viceversa)\")\n    }\n    out &lt;- list(shape = shape, scale = scale, rate = rate, mean = mean, var = var, sd = sd)\n    # coefficient of variation\n    out$cv &lt;- 1/sqrt(shape)\n    return(out)\n  }\n}"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization-1",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization-1",
    "title": "Generalized Linear Models",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization\n\ngm &lt;- gamma_params(mean = 30, sd = 10)\nunlist(gm)\n\n#&gt;       shape       scale        rate        mean         var          sd \n#&gt;   9.0000000   3.3333333   0.3000000  30.0000000 100.0000000  10.0000000 \n#&gt;          cv \n#&gt;   0.3333333\n\ny &lt;- rgamma(1000, shape = gm$shape, scale = gm$scale)"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization-2",
    "href": "slides/01-intro-glm/01-intro-glm.html#gamma-parametrization-2",
    "title": "Generalized Linear Models",
    "section": "Gamma parametrization",
    "text": "Gamma parametrization"
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#glm-in-r",
    "href": "slides/01-intro-glm/01-intro-glm.html#glm-in-r",
    "title": "Generalized Linear Models",
    "section": "GLM in R",
    "text": "GLM in R\nIs not always easy to work with link functions. In R we can use the distribution(link = ) function to have several useful information. For example:\n\nfam &lt;- binomial(link = \"logit\")\nfam$linkfun() # link function, from probability to eta\nfam$linkinv() # inverse of the link function, from eta to probability\n\nWe are going to see the specific arguments, but this tricks works for any family and links, even if you do not remember the specific function or formula."
  },
  {
    "objectID": "slides/01-intro-glm/01-intro-glm.html#references",
    "href": "slides/01-intro-glm/01-intro-glm.html#references",
    "title": "Generalized Linear Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. https://play.google.com/store/books/details?id=jlIqBgAAQBAJ\n\n\nAgresti, A. (2018). An introduction to categorical data analysis. John Wiley & Sons. https://play.google.com/store/books/details?id=pHZyDwAAQBAJ\n\n\nDunn, P. K., & Smyth, G. K. (2018). Generalized linear models with examples in r. Springer. https://play.google.com/store/books/details?id=tBh5DwAAQBAJ\n\n\nFaraway, J. J. (2016). Extending the linear model with r: Generalized linear, mixed effects and nonparametric regression models, second edition. Chapman; Hall/CRC. https://doi.org/10.1201/9781315382722\n\n\nFox, J. (2015). Applied regression analysis and generalized linear models. SAGE Publications. https://play.google.com/store/books/details?id=BlsKogEACAAJ\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam",
    "text": "Example: Passing the exam\nWe want to measure the impact of watching tv-shows on the probability of passing the statistics exam.\n\n\nexam: passing the exam (1 = “passed”, 0 = “failed”)\n\ntv_shows: watching tv-shows regularly (1 = “yes”, 0 = “no”)\n\n\n\n#&gt; # A tibble: 9 × 2\n#&gt;   tv_shows exam \n#&gt;   &lt;chr&gt;    &lt;chr&gt;\n#&gt; 1 1        1    \n#&gt; 2 1        0    \n#&gt; 3 1        1    \n#&gt; 4 1        1    \n#&gt; 5 ...      ...  \n#&gt; 6 0        0    \n#&gt; 7 0        0    \n#&gt; 8 0        0    \n#&gt; 9 0        0"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam-1",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam",
    "text": "Example: Passing the exam\nWe can create the contingency table\n\nxtabs(~exam + tv_shows, data = dat) |&gt; \n    addmargins()\n\n#&gt;      tv_shows\n#&gt; exam    0   1 Sum\n#&gt;   0    37  22  59\n#&gt;   1    13  28  41\n#&gt;   Sum  50  50 100"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam-2",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam",
    "text": "Example: Passing the exam\nEach cell probability \\(p_{ij}\\) is computed as \\(p_{ij}/n\\)\n\n(xtabs(~exam + tv_shows, data = dat)/n) |&gt; \n    addmargins()\n\n#&gt;      tv_shows\n#&gt; exam     0    1  Sum\n#&gt;   0   0.37 0.22 0.59\n#&gt;   1   0.13 0.28 0.41\n#&gt;   Sum 0.50 0.50 1.00"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam - Odds",
    "text": "Example: Passing the exam - Odds\nThe most common way to analyze a 2x2 contingency table is using the odds ratio (OR). Firsly let’s define the odds of success as:\n\\[\\begin{align*}\nodds = \\frac{p}{1 - p} \\\\\np = \\frac{odds}{odds + 1}\n\\end{align*}\\]\n\nthe odds are non-negative, ranging between 0 and \\(+\\infty\\)\n\nan odds of e.g. 3 means that we expect 3 success for each failure"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-1",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam - Odds",
    "text": "Example: Passing the exam - Odds\nFor the exam example:\n\nodds &lt;- function(p) p / (1 - p)\np11 &lt;- mean(with(dat, exam[tv_shows == 1])) # passing exam | tv_shows\np11\n\n#&gt; [1] 0.56\n\nodds(p11)\n\n#&gt; [1] 1.272727"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-ratio",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-ratio",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam - Odds Ratio",
    "text": "Example: Passing the exam - Odds Ratio\nThe OR is a ratio of odds:\n\\[\nOR = \\frac{\\frac{p_1}{1 - p_1}}{\\frac{p_2}{1 - p_2}}\n\\]\n\nOR ranges between 0 and \\(+\\infty\\). When \\(OR = 1\\) the odds for the two conditions are equal\nAn e.g. \\(OR = 3\\) means that being in the condition at the numerator increase 3 times the odds of success"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-ratio-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#example-passing-the-exam---odds-ratio-1",
    "title": "Binomial GLM",
    "section": "Example: Passing the exam - Odds Ratio",
    "text": "Example: Passing the exam - Odds Ratio\n\nodds_ratio &lt;- function(p1, p2) odds(p1) / odds(p2)\np11 &lt;- mean(with(dat, exam[tv_shows == 1])) # passing exam | tv_shows\np10 &lt;- mean(with(dat, exam[tv_shows == 0])) # passing exam | not tv_shows\np11\n\n#&gt; [1] 0.56\n\np10\n\n#&gt; [1] 0.26\n\nodds_ratio(p11, p10)\n\n#&gt; [1] 3.622378"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#why-using-these-measure",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#why-using-these-measure",
    "title": "Binomial GLM",
    "section": "Why using these measure?",
    "text": "Why using these measure?\nThe odds have an interesting property when taking the logarithm. We can express a probability \\(p\\) using a scale ranging \\([-\\infty, +\\infty]\\)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nWe considered a Study conducted by the University of Padua (TEDDY Child Study, 2020)1. Within the study, researchers asked the participants (mothers of a young child) about the presence of post-partum depression and measured the parental stress using the PSI-Parenting Stress Index.\n\n\n\n\n\n\n\nID\n      Parental_stress\n      Depression_pp\n    \n\n\n1\n75\nNo\n\n\n2\n51\nNo\n\n\n3\n76\nNo\n\n\n4\n88\nNo\n\n\n...\n...\n...\n\n\n376\n67\nNo\n\n\n377\n71\nNo\n\n\n378\n63\nNo\n\n\n379\n70\nNo\n\n\n\n\n\n\nThanks to Prof. Paolo Girardi for the example, see https://teddychild.dpss.psy.unipd.it/ for information"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-1",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nWe want to see if the parental stress increase the probability of having post-partum depression:"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-2",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-3",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nLet’s start by fitting a linear model Depression_pp ~ Parental_stress. We consider “Yes” as 1 and “No” as 0.\n\nfit_lm &lt;- lm(Depression_pp01 ~ Parental_stress, data = teddy)\nsummary(fit_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Depression_pp01 ~ Parental_stress, data = teddy)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.42473 -0.13768 -0.10003 -0.05768  0.94702 \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)     -0.172900   0.077561  -2.229 0.026389 *  \n#&gt; Parental_stress  0.004706   0.001201   3.919 0.000105 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.3239 on 377 degrees of freedom\n#&gt; Multiple R-squared:  0.03915,    Adjusted R-squared:  0.0366 \n#&gt; F-statistic: 15.36 on 1 and 377 DF,  p-value: 0.0001054"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-4",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nLet’s add the fitted line to our plot:"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-5",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-5",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\n… and check the residuals, pretty bad right?"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-6",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-6",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nAs for the exam example, we could compute a sort of contingency table despite the Parental_stress is a numerical variable by creating some discrete categories (just for exploratory analysis):\n\n\n\ntable(teddy$Depression_pp, teddy$Parental_stress_c) |&gt; \n    round(2)\n\n#&gt;      \n#&gt;       &lt; 40 40-60 60-80 80-100 &gt; 100\n#&gt;   No     0   164   136     26     6\n#&gt;   Yes    0    15    21      7     4\n\n\n\n\ntable(teddy$Depression_pp, teddy$Parental_stress_c) |&gt; \n    prop.table(margin = 2) |&gt; \n    round(2)\n\n#&gt;      \n#&gt;       &lt; 40 40-60 60-80 80-100 &gt; 100\n#&gt;   No        0.92  0.87   0.79  0.60\n#&gt;   Yes       0.08  0.13   0.21  0.40"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-7",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#another-example-teddy-child-7",
    "title": "Binomial GLM",
    "section": "Another example, Teddy Child\n",
    "text": "Another example, Teddy Child\n\nIdeally, we could compute the increase in the odds of having the post-partum depression as the parental stress increase. In fact, as we are going to see, the Binomial GLM is able to estimate the non-linear increase in the probability."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-glm",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-glm",
    "title": "Binomial GLM",
    "section": "Binomial GLM",
    "text": "Binomial GLM\n\nThe random component of a Binomial GLM the binomial distribution with parameter \\(p\\)\n\nThe systematic component is a linear combination of predictors and coefficients \\(\\boldsymbol{\\beta X}\\)\n\nThe link function is a function that map probabilities into the \\([-\\infty, +\\infty]\\) range."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#logit-link",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#logit-link",
    "title": "Binomial GLM",
    "section": "Logit Link",
    "text": "Logit Link\nThe logit link is the most common link function when using a binomial GLM:\n\\[\nlog \\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_{1}X_{1} + ...\\beta_pX_p  \n\\]\nThe inverse of the logit maps again the probability into the \\([0, 1]\\) range:\n\\[\np = \\frac{e^{\\beta_0 + \\beta_{1}X_{1} + ...\\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_{1}X_{1} + ...\\beta_pX_p}}  \n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#logit-link-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#logit-link-1",
    "title": "Binomial GLM",
    "section": "Logit Link",
    "text": "Logit Link\nThus with a single numerical predictor \\(x\\) the relationship between \\(x\\) and \\(p\\) in non-linear on the probability scale but linear on the logit scale."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#logit-link-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#logit-link-2",
    "title": "Binomial GLM",
    "section": "Logit Link",
    "text": "Logit Link\nThe problem is that effects are non-linear, thus is more difficult to interpret and report model results"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-fitting-in-r",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-fitting-in-r",
    "title": "Binomial GLM",
    "section": "Model fitting in R",
    "text": "Model fitting in R"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#the-big-picture",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#the-big-picture",
    "title": "Binomial GLM",
    "section": "The big picture…",
    "text": "The big picture…"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-fitting-in-r-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-fitting-in-r-1",
    "title": "Binomial GLM",
    "section": "Model fitting in R",
    "text": "Model fitting in R\nWe can model the contingency table presented before. We put data in binary form:\n\n\n\n\n#&gt;     tv_shows\n#&gt; exam  0  1\n#&gt;    0 35 22\n#&gt;    1 15 28\n\n\n\n\n\n#&gt; # A tibble: 9 × 2\n#&gt;   tv_shows exam \n#&gt;   &lt;chr&gt;    &lt;chr&gt;\n#&gt; 1 1        0    \n#&gt; 2 1        1    \n#&gt; 3 1        1    \n#&gt; 4 1        1    \n#&gt; 5 ...      ...  \n#&gt; 6 0        0    \n#&gt; 7 0        1    \n#&gt; 8 0        0    \n#&gt; 9 0        0"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model",
    "title": "Binomial GLM",
    "section": "Intercept only model",
    "text": "Intercept only model\nLet’s start from the simplest model (often called null model) where there are no predictors:\n\nfit0 &lt;- glm(exam ~ 1, data = dat, family = binomial(link = \"logit\"))\nsummary(fit0)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = exam ~ 1, family = binomial(link = \"logit\"), data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -1.060  -1.060  -1.060   1.299   1.299  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)  -0.2819     0.2020  -1.395    0.163\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 136.66  on 99  degrees of freedom\n#&gt; Residual deviance: 136.66  on 99  degrees of freedom\n#&gt; AIC: 138.66\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model-1",
    "title": "Binomial GLM",
    "section": "Intercept only model",
    "text": "Intercept only model\nWhen fitting an intercept-only model, the parameter is the average value of the y variable:\n\\[\\begin{align*}\n\\log(\\frac{p}{1 - p}) = \\beta_0 \\\\\np = \\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#intercept-only-model-2",
    "title": "Binomial GLM",
    "section": "Intercept only model",
    "text": "Intercept only model\nIn R, the \\(logit(p)\\) is computed using qlogis() that is the q + logis combination of functions to work with probability distributions. The \\(logit^{-1}\\) thus the inverse of the logit function is plogis():\n\n# average y on the respo nse scale\nmean(dat$exam)\n\n#&gt; [1] 0.43\n\nc(\"logit\" = coef(fit0)[1],\n  \"inv-logit\" = plogis(coef(fit0)[1])\n)\n\n#&gt;     logit.(Intercept) inv-logit.(Intercept) \n#&gt;            -0.2818512             0.4300000"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#link-function-tips",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#link-function-tips",
    "title": "Binomial GLM",
    "section": "Link function (TIPS)",
    "text": "Link function (TIPS)\nIf you are not sure about how to transform using the link function you can directly access the family() object in R that contains the appropriate link function and the corresponding inverse.\n\nbin &lt;- binomial(link = \"logit\")\nbin$linkfun() # the same as qlogis\nbin$linkinv() # the same as plogis"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x",
    "title": "Binomial GLM",
    "section": "Model with \\(X\\)\n",
    "text": "Model with \\(X\\)\n\nNow we can add the tv_shows predictor. Now the model has two coefficients. Given that the tv_shows is a binary variable, the intercept is the average y when tv_shows is 0, and the tv_shows coefficient is the increase in y for a unit increase in tv_shows:\n\nfit &lt;- glm(exam ~ tv_shows, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = exam ~ tv_shows, family = binomial(link = \"logit\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -1.2814  -0.8446  -0.8446   1.0769   1.5518  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept)  -0.8473     0.3086  -2.746  0.00604 **\n#&gt; tv_shows      1.0885     0.4200   2.592  0.00956 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 136.66  on 99  degrees of freedom\n#&gt; Residual deviance: 129.68  on 98  degrees of freedom\n#&gt; AIC: 133.68\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x-1",
    "title": "Binomial GLM",
    "section": "Model with \\(X\\)\n",
    "text": "Model with \\(X\\)\n\nThinking about our data, the (Intercept) is the probability of passing the exam without watching tv-shows. The tv_shows (should be) the difference in the probability of passing the exam between people who watched or did not watched tv-shows, BUT:\n\nwe are on the logit scale. Thus we are modelling log(odds) and not probabilities\na difference on the log scale is a ratio on the raw scale. Thus taking the exponential of tv_shows we obtain the ratio of odds of passing the exam watching vs non-watching tv-shows. Do you remember something?"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-with-x-2",
    "title": "Binomial GLM",
    "section": "Model with \\(X\\)\n",
    "text": "Model with \\(X\\)\n\nThe tv_shows is exactly the Odds Ratio that we calculated on the contingency table:\n\n# from model coefficients\nexp(coef(fit)[\"tv_shows\"])\n\n#&gt; tv_shows \n#&gt; 2.969697\n\n# from the contingency table\nodds_ratio(mean(dat$exam[dat$tv_shows == 1]),\n           mean(dat$exam[dat$tv_shows == 0]))\n\n#&gt; [1] 2.969697"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-intepretation",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-intepretation",
    "title": "Binomial GLM",
    "section": "Model Intepretation",
    "text": "Model Intepretation\nGiven the non-linearity and the link function, parameter intepretation is not easy for GLMs. In the case of the Binomial GLM we will see:\n\ninterpreting model coefficients on the linear and logit scale\nodds ratio (already introduced)\nthe divide by 4 rule (Gelman et al., 2020; Gelman & Hill, 2006)\n\nmarginal effects\npredicted probabilities"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#intepreting-model-coefficients",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#intepreting-model-coefficients",
    "title": "Binomial GLM",
    "section": "Intepreting model coefficients",
    "text": "Intepreting model coefficients\nModels coefficients are interpreted in the same way as standard regression models. The big difference concerns:\n\nnumerical predictors\ncategorical predictors\n\nUsing contrast coding and centering/standardizing we can make model coefficients easier to intepret."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors",
    "title": "Binomial GLM",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nWe use a categorical predictor with \\(p\\) levels, the model will estimate \\(p - 1\\) parameters. The interpretation of these parameters is controlled by the contrast coding. In R the default is the treatment coding (or dummy coding). Essentially R create \\(p - 1\\) dummy variables (0 and 1) where 0 is the reference level (usually the first category) and 1 is the current level. We can see the coding scheme using the model.matrix() function that return the \\(\\boldsymbol{X}\\) matrix:\n\n\n#&gt; # A tibble: 9 × 2\n#&gt;   X.Intercept. tv_shows\n#&gt;   &lt;chr&gt;        &lt;chr&gt;   \n#&gt; 1 1            1       \n#&gt; 2 1            1       \n#&gt; 3 1            1       \n#&gt; 4 1            1       \n#&gt; 5 ...          ...     \n#&gt; 6 1            0       \n#&gt; 7 1            0       \n#&gt; 8 1            0       \n#&gt; 9 1            0"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors-1",
    "title": "Binomial GLM",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nIn the simple case of the exam dataset, the intercept (\\(\\beta_0\\)) is the reference level (default to 0 because is the first) and \\(\\beta_0\\) is the difference between the actual level and the reference level. If we change the order of the levels we could change the intercept value while \\(\\beta_1\\) will be the same. As an example we could use the so-called sum to zero coding where instead of assigning 0 and 1 we use different values. For example assigning -0.5 and 0.5 will make the intercept the grand-mean:\n\ndat$tv_shows0 &lt;- ifelse(dat$tv_shows == 0, -0.5, 0.5)\nfit &lt;- glm(exam ~ tv_shows0, data = dat, family = binomial(link = \"logit\"))\n# grand mean\nmean(c(mean(dat$exam[dat$tv_shows == 1]), mean(dat$exam[dat$tv_shows == 0])))\n\n#&gt; [1] 0.43\n\n# intercept\nplogis(coef(fit)[1])\n\n#&gt; (Intercept) \n#&gt;   0.4248077"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictors-2",
    "title": "Binomial GLM",
    "section": "Categorical predictors",
    "text": "Categorical predictors"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nWith numerical predictors the idea is the same as categorical predictors. In fact, categorical predictors are converted into numbers (i.e., 0 and 1 or -0.5 and 0.5). The only caveat is that the effects are linear only the logit scale. Thus \\(\\beta_1\\) is interpreted in the same way as standard linear models only on the link-function scale. For the binomial\nGLM the \\(\\beta_1\\) is the increase in the \\(log(odds(p))\\) for a unit-increase in the \\(x\\). In the response (probability) scale, the \\(\\beta_1\\) is the multiplicative increase in the odds of \\(y = 1\\) for a unit increase in the predictor."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-1",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nWith numerical predictors we could mean-center and or standardize the predictors. With centering (similarly to the categorical example) we change the interpretation of the intercept. Standardizing is helpful to have more meaningful \\(\\beta\\) values. The \\(\\beta_i\\) of a centered predictor is the increase in \\(y\\) for a increase in one standard deviation of \\(x\\).\n\\[\nx_{cen} = x_i - \\hat x\n\\]\n\\[\nx_{z} = \\frac{x_i - \\hat x}{\\sigma_x}\n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-2",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-3",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nLet’s return to our teddy child example and fitting the proper model:\n\nfit_glm &lt;- glm(Depression_pp01 ~ Parental_stress, data = teddy, family = binomial(link = \"logit\"))\nsummary(fit_glm)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = Depression_pp01 ~ Parental_stress, family = binomial(link = \"logit\"), \n#&gt;     data = teddy)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -1.2852  -0.5165  -0.4509  -0.3861   2.3096  \n#&gt; \n#&gt; Coefficients:\n#&gt;                  Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)     -4.323906   0.690689  -6.260 3.84e-10 ***\n#&gt; Parental_stress  0.036015   0.009838   3.661 0.000251 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 284.13  on 378  degrees of freedom\n#&gt; Residual deviance: 271.23  on 377  degrees of freedom\n#&gt; AIC: 275.23\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-4",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nThe (Intercept) (\\(\\beta_0\\)) is the probability of having post-partum depression for a mother with parental stress zero (maybe better centering?)\n\\[\np(yes|x = 0) = g^{-1}(\\beta_0)\n\\]\n\nplogis(coef(fit_glm)[\"(Intercept)\"])\n\n#&gt; (Intercept) \n#&gt;  0.01307482"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-5",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-5",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nThe Parental_stress (\\(\\beta_1\\)) is the increase in the \\(log(odds)\\) of having the post-partum depression for a unit increase in the parental stress index. If we take the exponential of \\(\\beta_1\\) we obtain the increase in the \\(odds\\) of having post-partum depression for a unit increase in parental stress index.\n\nexp(coef(fit_glm)[\"Parental_stress\"])\n\n#&gt; Parental_stress \n#&gt;        1.036671"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-6",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-6",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nThe problem is that, as shown before, the effects are non-linear on the probability scale while are linear on the logit scale. On the logit scale, all differences are constant:\n\npr &lt;- list(c(10, 11), c(50, 51), c(70, 71))\n\npredictions &lt;- lapply(pr, function(x) {\n    predict(fit_glm, newdata = data.frame(Parental_stress = x))\n})\n\n# notice that the difference is exactly the Parental_stress parameter\nsapply(predictions, diff)\n\n#&gt;         2         2         2 \n#&gt; 0.0360147 0.0360147 0.0360147"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-7",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictors-7",
    "title": "Binomial GLM",
    "section": "Numerical predictors",
    "text": "Numerical predictors\nWhile on the probability scale, the differences are not the same. This is problematic when interpreting the results of a Binomial GLM with a numerical predictor.\n\n(predictions &lt;- lapply(predictions, plogis))\n\n#&gt; [[1]]\n#&gt;          1          2 \n#&gt; 0.01863764 0.01930790 \n#&gt; \n#&gt; [[2]]\n#&gt;          1          2 \n#&gt; 0.07424969 0.07676350 \n#&gt; \n#&gt; [[3]]\n#&gt;         1         2 \n#&gt; 0.1415012 0.1459330\n\nsapply(predictions, diff)\n\n#&gt;            2            2            2 \n#&gt; 0.0006702661 0.0025138036 0.0044317558"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#divide-by-4-rule",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#divide-by-4-rule",
    "title": "Binomial GLM",
    "section": "Divide by 4 rule",
    "text": "Divide by 4 rule\nThe divide by 4 rule is a very easy way to evaluate the effect of a continuous predictor.\nGiven the non-linearity, the derivative of the logistic function (i.e., the slope) is maximal for probabilities around ~\\(0.5\\).\nIn fact, \\(\\beta_i p (1 - p)\\) is maximized when \\(p = 0.5\\) turning into \\(\\beta_i 0.25\\) (i.e., dividing by 4).\nDividing \\(\\beta/4\\) we obtain the maximal slope thus the maximal difference in probability."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#divide-by-4-rule-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#divide-by-4-rule-1",
    "title": "Binomial GLM",
    "section": "Divide by 4 rule",
    "text": "Divide by 4 rule"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities",
    "title": "Binomial GLM",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn a similar way we can use the inverse logit function to find the predicted probability specific values of \\(x\\). For example, the difference between \\(p(y = 1|x = 2.5) - p(y = 1|x = 5)\\) can be calculated using the model equation:\n\n\\(logit^{-1}p(y = 1|x = 2.5) = \\frac{e^{\\beta_0 + \\beta_1 2.5}}{1 + e^{\\beta_0 + \\beta_1 2.5}}\\)\n\\(logit^{-1}p(y = 1|x = 5) = \\frac{e^{\\beta_0 + \\beta_1 5}}{1 + e^{\\beta_0 + \\beta_1 5}}\\)\n\\(logit^{-1}p(y = 1|x = 5) - logit^{-1}p(y = 1|x = 2.5)\\)\n\n\ncoefs &lt;- coef(fit)\nplogis(coefs[1] + coefs[2]*5) - plogis(coefs[1] + coefs[2]*2.5)\n\n#&gt; (Intercept) \n#&gt;   0.2237369"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities-1",
    "title": "Binomial GLM",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nIn R we can use directly the predict() function with the argument type = \"response\" to return the predicted probabilities instead of the logits:\n\npreds &lt;- predict(fit, newdata = list(x = c(2.5, 5)), type = \"response\")\npreds\n\n#&gt;         1         2 \n#&gt; 0.0329886 0.2567255\n\npreds[2] - preds[1]\n\n#&gt;         2 \n#&gt; 0.2237369"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#predicted-probabilities-2",
    "title": "Binomial GLM",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\nI have written the epredict() function that extend the predict() function giving some useful messages when computing predictions. you can use it with every model and also with multiple predictors.\n\nepredict(fit, values = list(x = c(2.5, 5)), type = \"response\")\n\n#&gt; y ~ -5.693 + 0.926*c(2.5, 5)\n\n\n#&gt; [1] 0.0329886 0.2567255"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nA marginal effect measures the association between a change in a predictor (\\(x\\)), and a change in the response \\(y\\).\nThe slope can be evaluated for any level of \\(x\\). In fact, the divide-by-4 rule is the maximal slope evaluated at a specific \\(x\\)."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-1",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nThe divide-by-4 rule is the maximal marginal effect. The average marginal effects is the average of all slopes. This can be easily done with the marginaleffects package:\n\nlibrary(marginaleffects)\n\n# all marginal effects\nslopes(fit)\n#&gt; # A tibble: 100 × 14\n#&gt;    rowid term  estimate std.error statistic   p.value s.value conf.low conf.high\n#&gt;    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1     1 x      0.00403   0.00354      1.14   2.55e-1    1.97 -2.91e-3   0.0110 \n#&gt;  2     2 x      0.137     0.0277       4.95   7.47e-7   20.4   8.28e-2   0.191  \n#&gt;  3     3 x      0.0489    0.0213       2.30   2.15e-2    5.54  7.23e-3   0.0906 \n#&gt;  4     4 x      0.0383    0.0188       2.04   4.15e-2    4.59  1.48e-3   0.0750 \n#&gt;  5     5 x      0.222     0.0407       5.45   4.91e-8   24.3   1.42e-1   0.302  \n#&gt;  6     6 x      0.0526    0.0193       2.72   6.48e-3    7.27  1.47e-2   0.0905 \n#&gt;  7     7 x      0.0811    0.0229       3.54   3.98e-4   11.3   3.62e-2   0.126  \n#&gt;  8     8 x      0.199     0.0386       5.16   2.52e-7   21.9   1.23e-1   0.275  \n#&gt;  9     9 x      0.00353   0.00319      1.11   2.68e-1    1.90 -2.72e-3   0.00979\n#&gt; 10    10 x      0.0270    0.0136       1.99   4.68e-2    4.42  3.82e-4   0.0537 \n#&gt; # ℹ 90 more rows\n#&gt; # ℹ 5 more variables: predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;, predicted &lt;dbl&gt;,\n#&gt; #   y &lt;int&gt;, x &lt;dbl&gt;"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-2",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\n\n# marginal effects when x = 5\nslopes(fit, newdata = data.frame(x = 5))\n\n#&gt; # A tibble: 1 × 14\n#&gt;   rowid term  estimate std.error statistic    p.value s.value conf.low conf.high\n#&gt;   &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1 x        0.177    0.0338      5.22    1.76e-7    22.4    0.110     0.243\n#&gt; # ℹ 5 more variables: predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;, predicted &lt;dbl&gt;,\n#&gt; #   x &lt;dbl&gt;, y &lt;int&gt;\n\n# average marginal effect\navg_slopes(fit)\n\n#&gt; # A tibble: 1 × 8\n#&gt;   term  estimate std.error statistic   p.value s.value conf.low conf.high\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 x       0.0934   0.00312      29.9 5.94e-197    652.   0.0873    0.0995"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-3",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nWith multiple variables, marginal effects are also useful to see the effect of one predictor fixing the level of others. Let’s simulate a model with two predictors:\n\ndat &lt;- sim_design(100, nx = list(x1 = runif(100), x2 = runif(100)))\ndat &lt;- sim_data(dat, plogis(qlogis(0.001) + 5 * x1 + 2 * x2), model = \"binomial\")\nfit &lt;- glm(y ~ x1 + x2, data = dat, family = binomial(link = \"logit\"))\nplot(ggeffect(fit)$x1) | plot(ggeffect(fit)$x2)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-4",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nWe can see the marginal effects (each, average or marginal) for the \\(x1\\) while fixing \\(x2\\) to the mean:\n\nslopes(fit, newdata = data.frame(x1 = dat$x1, x2 = mean(dat$x2)))\n\n#&gt; # A tibble: 200 × 15\n#&gt;    rowid term  estimate std.error statistic p.value s.value conf.low conf.high\n#&gt;    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1     1 x1      0.0991    0.0624     1.59   0.112     3.16 -0.0232     0.221 \n#&gt;  2     2 x1      0.191     0.195      0.979  0.327     1.61 -0.191      0.573 \n#&gt;  3     3 x1      0.0805    0.0419     1.92   0.0546    4.20 -0.00157    0.163 \n#&gt;  4     4 x1      0.0555    0.0227     2.45   0.0145    6.11  0.0110     0.100 \n#&gt;  5     5 x1      0.0499    0.0204     2.45   0.0145    6.11  0.00990    0.0899\n#&gt;  6     6 x1      0.0998    0.0632     1.58   0.114     3.13 -0.0241     0.224 \n#&gt;  7     7 x1      0.135     0.110      1.23   0.219     2.19 -0.0803     0.350 \n#&gt;  8     8 x1      0.0605    0.0254     2.38   0.0174    5.84  0.0106     0.110 \n#&gt;  9     9 x1      0.220     0.241      0.913  0.361     1.47 -0.252      0.691 \n#&gt; 10    10 x1      0.152     0.134      1.13   0.258     1.95 -0.111      0.414 \n#&gt; # ℹ 190 more rows\n#&gt; # ℹ 6 more variables: predicted_lo &lt;dbl&gt;, predicted_hi &lt;dbl&gt;, predicted &lt;dbl&gt;,\n#&gt; #   x1 &lt;dbl&gt;, x2 &lt;dbl&gt;, y &lt;int&gt;"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation",
    "title": "Binomial GLM",
    "section": "Inverse estimation1\n",
    "text": "Inverse estimation1\n\nSometimes it is useful to do what is called inverse estimation, thus predicting the \\(x\\) level associated with a certain \\(y\\). In this case the \\(x\\) producing a certain \\(p\\) of response.\nSometimes this is called median effective dose when finding the \\(x\\) level producing \\(50\\%\\) of correct responses.\nWe can use the MASS::dose.p() function:\n\nMASS::dose.p(fit, p = c(0.25, 0.5, 0.8))\n\n#&gt;               Dose       SE\n#&gt; p = 0.25: 2.301454 1.318885\n#&gt; p = 0.50: 2.917503 1.732353\n#&gt; p = 0.80: 3.694871 2.271767\n\n\nsee also the investr package"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics",
    "title": "Binomial GLM",
    "section": "Inverse estimation and Psychophysics",
    "text": "Inverse estimation and Psychophysics\nIn Psychophysics it is common to do the inverse estimation to estimate the detection or performance threshold. For example, estimating the \\(x\\) level associated with a certain probability of success e.g. 50%.\n\nCodecurve(plogis(x, 0.7, 1/8), \n      ylim = c(0, 1),\n      xlab = \"Stimulus Contrast\",\n      ylab = \"Probability of Detection\",\n      lwd = 2)\nsegments(x0 = 0.7, y0 = 0, x1 = 0.7, 0.5, lty = 2)\nsegments(x0 = 0, y0 = 0.5, x1 = 0.7, 0.5, lty = 2)\npoints(x = 0.7, y = 0.5, pch = 19, col = \"firebrick\", cex = 2)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-1",
    "title": "Binomial GLM",
    "section": "Inverse estimation and Psychophysics",
    "text": "Inverse estimation and Psychophysics\nLet’s simulate and ideal observer that respond to stimuli with different contrast:\n\nCodeth &lt;- 0.7 # -(b0/b1)\nslope &lt;- 1/8\n\nb1 &lt;- 1/slope\nb0 &lt;- -th*b1\n\nx &lt;- rep(seq(0, 1, 0.1), each = 20)\n\np &lt;- plogis(b0 + b1 * x)\ny &lt;- rbinom(length(x), 1, p)\n\nyp &lt;- tapply(y, x, mean)\nxc &lt;- unique(x)\n\nplot(xc, yp, type = \"b\", ylim = c(0, 1), xlab = \"Contrast\", ylab = \"Probability of Detection\")\ncurve(plogis(x, th, slope), add = TRUE, col = \"firebrick\", lwd = 2)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-2",
    "title": "Binomial GLM",
    "section": "Inverse estimation and Psychophysics",
    "text": "Inverse estimation and Psychophysics\n\nfit &lt;- glm(y ~ x, family = binomial(link = \"logit\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = binomial(link = \"logit\"))\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.0471  -0.6306  -0.2320   0.5124   2.6938  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  -4.9927     0.6644  -7.515 5.69e-14 ***\n#&gt; x             6.9569     0.9410   7.393 1.44e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 273.67  on 219  degrees of freedom\n#&gt; Residual deviance: 163.71  on 218  degrees of freedom\n#&gt; AIC: 167.71\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#inverse-estimation-and-psychophysics-3",
    "title": "Binomial GLM",
    "section": "Inverse estimation and Psychophysics",
    "text": "Inverse estimation and Psychophysics\nWe can estimate the threshold and slope of the psychometric function using the equations from Knoblauch & Maloney (2012).\n\n-(coef(fit)[1]/coef(fit)[2]) # threshold\n\n#&gt; (Intercept) \n#&gt;   0.7176681\n\nMASS::dose.p(fit, p = 0.5) # with inverse estimation\n\n#&gt;               Dose        SE\n#&gt; p = 0.5: 0.7176681 0.0287526"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#odds-ratio-or-to-cohens-d",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#odds-ratio-or-to-cohens-d",
    "title": "Binomial GLM",
    "section": "Odds Ratio (OR) to Cohen’s \\(d\\)\n",
    "text": "Odds Ratio (OR) to Cohen’s \\(d\\)\n\nThe Odds Ratio can be considered an effect size measure. We can transform the OR into other effect size measure (Borenstein et al., 2009; Sánchez-Meca et al., 2003).\n\\[\nd = \\log(OR) \\frac{\\sqrt{3}}{\\pi}\n\\]\n\n# in R with the effectsize package\nor &lt;- 1.5\neffectsize::logoddsratio_to_d(log(or))\n\n#&gt; [1] 0.2235446\n\n# or \neffectsize::oddsratio_to_d(or)\n\n#&gt; [1] 0.2235446"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#odds-ratio-or-to-cohens-d-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#odds-ratio-or-to-cohens-d-1",
    "title": "Binomial GLM",
    "section": "Odds Ratio (OR) to Cohen’s \\(d\\)\n",
    "text": "Odds Ratio (OR) to Cohen’s \\(d\\)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#wald-tests",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#wald-tests",
    "title": "Binomial GLM",
    "section": "Wald tests",
    "text": "Wald tests\nThe basic approach when doing inference with GLM is interpreting the Wald test of each model coefficients. The Wald test is calculated as follows:\n\\[\nz = \\frac{\\beta_i - \\beta_0}{\\sigma_{\\beta_i}}\n\\]\nWhere \\(\\beta\\) is the regression coefficients, \\(\\beta_0\\) is the value under the null hypothesis (generally 0) and \\(\\sigma_{\\beta_i}\\) is the standard error. P-values and confidence intervals can be calculated based on a standard normal distribution."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#wald-type-confidence-intervals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#wald-type-confidence-intervals",
    "title": "Binomial GLM",
    "section": "Wald-type confidence intervals",
    "text": "Wald-type confidence intervals\nWald-type confidence interval (directly from model summary), where \\(\\Phi\\) is the cumulative Gaussian function qnorm():\n\\[\n95\\%CI = \\hat \\beta \\pm \\Phi(\\alpha/2) SE_{\\beta}  \n\\]\n\n(summ &lt;- data.frame(summary(fit)$coefficients))\n\n#&gt; # A tibble: 2 × 4\n#&gt;   Estimate Std..Error z.value Pr...z..\n#&gt;      &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1   -0.575      0.295   -1.95 0.0508  \n#&gt; 2    1.42       0.427    3.33 0.000855\n\n# 95% confidence interval\nsumm$Estimate + qnorm(c(0.025, 0.95))*summ$Std..Error\n\n#&gt; [1] -1.152824  2.124464"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#profile-likelihood-confidence-intervals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#profile-likelihood-confidence-intervals",
    "title": "Binomial GLM",
    "section": "Profile likelihood confidence intervals",
    "text": "Profile likelihood confidence intervals\nThe computation is a little bit different and they are not always symmetric:\n\n# profile likelihood, different from wald type\nconfint(fit)\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) -1.1726621 -0.00947434\n#&gt; tv_shows     0.6034638  2.28247024"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#profile-likelihood-confidence-intervals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#profile-likelihood-confidence-intervals-1",
    "title": "Binomial GLM",
    "section": "Profile likelihood confidence intervals",
    "text": "Profile likelihood confidence intervals\nYou can use the plot_param() function (quite overkilled):\n\nplot_param(fit, \"tv_shows\")"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals",
    "title": "Binomial GLM",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWhen calculating confidence intervals it is important to consider the link function. In the same way as we compute the inverse logit function on the parameter value, we could revert also the confidence intervals. IMPORTANT, do not apply the inverse logit on the standard error and then compute the confidence interval.\n\nfits &lt;- broom::tidy(fit) # extract parameters as dataframe\nfits\n\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    -1.15     0.331     -3.48 0.000500 \n#&gt; 2 tv_shows        1.73     0.443      3.90 0.0000967"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals-1",
    "title": "Binomial GLM",
    "section": "Confidence intervals1\n",
    "text": "Confidence intervals1\n\n\nb &lt;- fits$estimate[2]\nse &lt;- fits$std.error[2]\n\n# correct, wald-type confidence intervals\nc(b = exp(b), lower = exp(b - 2*se), upper = exp(b + 2*se))\n\n#&gt;        b    lower    upper \n#&gt;  5.62963  2.32003 13.66049\n\n# correct, likelihood based confidence intervals\nexp(confint(fit, \"tv_shows\"))\n\n#&gt;     2.5 %    97.5 % \n#&gt;  2.417513 13.850465\n\n# wrong wald type\nc(b = exp(b), lower = exp(b) - 2*exp(se), upper = exp(b) + 2*exp(se))\n\n#&gt;        b    lower    upper \n#&gt; 5.629630 2.514163 8.745097\n\n\nNotice that I use 2*se instead of the precise quantile of the Gaussian distribution that for \\(\\alpha = 0.05\\) is \\(\\sim 1.96\\)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#confidence-intervals-2",
    "title": "Binomial GLM",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nThe same principle holds for predicted probabilities. First compute the intervals on the logit scale and then transform-back on the probability scale:\n\nfits &lt;- dat |&gt; \n    select(tv_shows) |&gt; \n    distinct() |&gt; \n    add_predict(fit, se.fit = TRUE)\n\nfits$p &lt;- plogis(fits$fit)\nfits$lower &lt;- plogis(with(fits, fit - 2*se.fit))\nfits$upper &lt;- plogis(with(fits, fit + 2*se.fit))\n\nfits\n\n#&gt; # A tibble: 2 × 7\n#&gt;   tv_shows    fit se.fit residual.scale     p lower upper\n#&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1  0.575  0.295              1 0.640 0.497 0.762\n#&gt; 2        0 -1.15   0.331              1 0.240 0.140 0.380"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#anova",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#anova",
    "title": "Binomial GLM",
    "section": "Anova",
    "text": "Anova\nWith multiple predictors, especially with categorical variables with more than 2 levels, we can compute the an anova-like analysis individuating the effect of each predictor. In R we can do this using the car::Anova() function. Let’s simulate a model with a 2x2 interaction:\n\n\n#&gt; # A tibble: 6 × 4\n#&gt;      id x1    x2        y\n#&gt;   &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n#&gt; 1     1 a     c         1\n#&gt; 2     2 a     d         0\n#&gt; 3     3 b     c         1\n#&gt; 4     4 b     d         1\n#&gt; 5     5 a     c         1\n#&gt; 6     6 a     d         0"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#anova-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#anova-1",
    "title": "Binomial GLM",
    "section": "Anova",
    "text": "Anova\nWe can fit the most complex model containing the two main effects and the interaction. I set the contrasts for the two factors as contr.sum()/2 that are required for a proper analysis of factorial designs:\n\nfit_max &lt;- glm(y ~ x1 + x2 + x1:x2, data = dat, family = binomial(link = \"logit\")) # same as x1 * x2\nsummary(fit_max)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x1 + x2 + x1:x2, family = binomial(link = \"logit\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -1.1213  -0.9005  -0.5350   1.2346   2.0074  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  -0.8864     0.2138  -4.147 3.37e-05 ***\n#&gt; x11           0.7921     0.4275   1.853   0.0639 .  \n#&gt; x21           0.9462     0.4275   2.213   0.0269 *  \n#&gt; x11:x21      -0.4649     0.8551  -0.544   0.5867    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 148.26  on 119  degrees of freedom\n#&gt; Residual deviance: 139.86  on 116  degrees of freedom\n#&gt; AIC: 147.86\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#anova-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#anova-2",
    "title": "Binomial GLM",
    "section": "Anova",
    "text": "Anova\n\nplot(ggeffect(fit_max))\n\n#&gt; $x1\n\n\n\n\n\n\n\n\n#&gt; \n#&gt; $x2"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#anova-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#anova-3",
    "title": "Binomial GLM",
    "section": "Anova",
    "text": "Anova\nThen using car::Anova(). For each effect we have the \\(\\chi^2\\) statistics and the associated p-value. The null hypothesis is that the specific factor did not contribute in reducing the residual deviance.\n\ncar::Anova(fit_max)\n\n#&gt; # A tibble: 3 × 3\n#&gt;   `LR Chisq`    Df `Pr(&gt;Chisq)`\n#&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1      3.32      1       0.0683\n#&gt; 2      4.92      1       0.0266\n#&gt; 3      0.299     1       0.584"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison",
    "title": "Binomial GLM",
    "section": "Model comparison",
    "text": "Model comparison\nThe table obtained with car::Anova() is essentially a model comparison using the Likelihood Ratio test. This can be done using the anova(...) function.\n\\[\\begin{align*}\nD = 2 (log(\\mathcal{L}_{full}) - log(\\mathcal{L}_{reduced})) \\\\\nD \\sim \\chi^2_{df_{full} - df_{reduced}}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-1",
    "title": "Binomial GLM",
    "section": "Model comparison",
    "text": "Model comparison\nTo better understanding, the x2 effect reported in the car::Anova() table is a model comparison between a model with y ~ x1 + x2 and a model without x2. The difference between these two model is the unique contribution of x2 after controlling for x1:\n\nfit &lt;- glm(y ~ x1 + x2, data = dat, family = binomial(link = \"logit\"))\nfit0 &lt;- glm(y ~ x1, data = dat, family = binomial(link = \"logit\"))\n\nanova(fit0, fit, test = \"LRT\") # ~ same as car::Anova(fit_max)\n\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1         118         145.    NA    NA       NA     \n#&gt; 2         117         140.     1     4.92     0.0266"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-2",
    "title": "Binomial GLM",
    "section": "Model comparison",
    "text": "Model comparison\nThe model comparison using anova() (i.e., likelihood ratio tests) is limited to nested models thus models that differs only for one term. For example:\n\nfit1 &lt;- glm(y ~ x1, data = dat, family = binomial(link = \"logit\"))\nfit2 &lt;- glm(y ~ x2, data = dat, family = binomial(link = \"logit\"))\nfit3 &lt;- glm(y ~ x1 + x2, data = dat, family = binomial(link = \"logit\"))\n\nfit1 and fit2 are non-nested because we have the same number of predictors (thus degrees of freedom). fit3 and fit1/fit2 are nested because fit3 is more complex and removing one term we can obtain the less complex models."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#model-comparison-3",
    "title": "Binomial GLM",
    "section": "Model comparison",
    "text": "Model comparison\n\nanova(fit1, fit2, test = \"LRT\") # do not works properly\n\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1         118         145.    NA    NA            NA\n#&gt; 2         118         143.     0     1.59         NA\n\nanova(fit1, fit3, test = \"LRT\") # same anova(fit2, fit3)\n\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1         118         145.    NA    NA       NA     \n#&gt; 2         117         140.     1     4.92     0.0266"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#information-criteria",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#information-criteria",
    "title": "Binomial GLM",
    "section": "Information Criteria",
    "text": "Information Criteria\nAs for standard linear models I can use the Akaike Information Criteria (AIC) or the Bayesian Information Criteria (BIC) to compare non-nested models. The downside is not having a properly hypothesis testing setup.\n\ndata.frame(BIC(fit1, fit2, fit3)) |&gt; \n    arrange(BIC)\n\n#&gt; # A tibble: 3 × 2\n#&gt;      df   BIC\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     2  153.\n#&gt; 2     3  155.\n#&gt; 3     2  155.\n\ndata.frame(AIC(fit1, fit2, fit3)) |&gt; \n    arrange(AIC)\n\n#&gt; # A tibble: 3 × 2\n#&gt;      df   AIC\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     3  146.\n#&gt; 2     2  147.\n#&gt; 3     2  149."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\nFor post-hoc contrasts you can use the emmeans package both for numerical but especially categorical predictors. Let’s simulate a 2x2 interaction:\n\ndat &lt;- sim_design(100, cx = list(x1 = c(\"a\", \"b\"), x2 = c(\"c\", \"d\")), contrasts = contr.sum2)\ndat &lt;- sim_data(dat, plogis(qlogis(0.25) + 0 * x1_c + 2 * x2_c + 2 * x1_c * x2_c))\nfit &lt;- glm(y ~ x1 * x2, data = dat, family = binomial(link = \"logit\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x1 * x2, family = binomial(link = \"logit\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -1.5096  -0.6866  -0.2468   0.8782   2.6482  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  -1.1449     0.1755  -6.524 6.86e-11 ***\n#&gt; x11          -0.4326     0.3510  -1.232    0.218    \n#&gt; x21           2.5113     0.3510   7.155 8.37e-13 ***\n#&gt; x11:x21       3.4372     0.7020   4.896 9.76e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 502.99  on 399  degrees of freedom\n#&gt; Residual deviance: 386.90  on 396  degrees of freedom\n#&gt; AIC: 394.9\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-1",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\n\nplot(ggeffects::ggeffect(fit, terms = c(\"x1\", \"x2\")))"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-2",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\nemmeans is a very powerful and complicate package (documentation). Firstly we can estimate the marginal means of the conditions:\n\nlibrary(emmeans)\nemmeans(fit, ~ x1 + x2)\n\n#&gt;  x1 x2 emmean    SE  df asymp.LCL asymp.UCL\n#&gt;  a  c   0.754 0.214 Inf     0.334     1.174\n#&gt;  b  c  -0.532 0.207 Inf    -0.938    -0.126\n#&gt;  a  d  -3.476 0.586 Inf    -4.625    -2.327\n#&gt;  b  d  -1.325 0.246 Inf    -1.806    -0.844\n#&gt; \n#&gt; Results are given on the logit (not the response) scale. \n#&gt; Confidence level used: 0.95"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-3",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\nWe can also calculate the marginal means on the response scale:\n\nlibrary(emmeans)\nemmeans(fit, ~ x1 + x2, type = \"response\")\n\n#&gt;  x1 x2 prob     SE  df asymp.LCL asymp.UCL\n#&gt;  a  c  0.68 0.0466 Inf   0.58264    0.7639\n#&gt;  b  c  0.37 0.0483 Inf   0.28127    0.4685\n#&gt;  a  d  0.03 0.0171 Inf   0.00971    0.0889\n#&gt;  b  d  0.21 0.0407 Inf   0.14111    0.3008\n#&gt; \n#&gt; Confidence level used: 0.95 \n#&gt; Intervals are back-transformed from the logit scale"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-4",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\nCrucially we can compute the contrasts (i.e., the post-hoc tests). Notice that they are computed on the link-function scale:\n\nemmeans(fit, pairwise ~ x1 | x2)$contrasts\n#&gt; x2 = c:\n#&gt;  contrast estimate    SE  df z.ratio p.value\n#&gt;  a - b        1.29 0.298 Inf   4.314  &lt;.0001\n#&gt; \n#&gt; x2 = d:\n#&gt;  contrast estimate    SE  df z.ratio p.value\n#&gt;  a - b       -2.15 0.636 Inf  -3.385  0.0007\n#&gt; \n#&gt; Results are given on the log odds ratio (not the response) scale.\nemmeans(fit, pairwise ~ x1 | x2, type = \"response\")$contrasts\n#&gt; x2 = c:\n#&gt;  contrast odds.ratio     SE  df null z.ratio p.value\n#&gt;  a / b         3.618 1.0786 Inf    1   4.314  &lt;.0001\n#&gt; \n#&gt; x2 = d:\n#&gt;  contrast odds.ratio     SE  df null z.ratio p.value\n#&gt;  a / b         0.116 0.0739 Inf    1  -3.385  0.0007\n#&gt; \n#&gt; Tests are performed on the log odds ratio scale"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-5",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-means-5",
    "title": "Binomial GLM",
    "section": "Marginal Means",
    "text": "Marginal Means\nNotice the order of the terms to change the actual type of contrasts:\n\nemmeans(fit, pairwise ~ x2 | x1)$contrasts\n#&gt; x1 = a:\n#&gt;  contrast estimate    SE  df z.ratio p.value\n#&gt;  c - d       4.230 0.624 Inf   6.777  &lt;.0001\n#&gt; \n#&gt; x1 = b:\n#&gt;  contrast estimate    SE  df z.ratio p.value\n#&gt;  c - d       0.793 0.321 Inf   2.468  0.0136\n#&gt; \n#&gt; Results are given on the log odds ratio (not the response) scale.\nemmeans(fit, pairwise ~ x2 | x1, type = \"response\")$contrasts\n#&gt; x1 = a:\n#&gt;  contrast odds.ratio    SE  df null z.ratio p.value\n#&gt;  c / d         68.71 42.89 Inf    1   6.777  &lt;.0001\n#&gt; \n#&gt; x1 = b:\n#&gt;  contrast odds.ratio    SE  df null z.ratio p.value\n#&gt;  c / d          2.21  0.71 Inf    1   2.468  0.0136\n#&gt; \n#&gt; Tests are performed on the log odds ratio scale"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-5",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-5",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nWhen plotting a binomial GLM the most useful way is plotting the marginal probabilities and standard errors/confidence intervals for a given combination of predictors. Let’s make an example for:\n\nsimple GLM with 1 categorical/numerical predictor\nGLM with 2 numerical/categorical predictors\nGLM with interaction between numerical and categorical predictors"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-6",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#marginal-effects-6",
    "title": "Binomial GLM",
    "section": "Marginal effects",
    "text": "Marginal effects\nA general workflow could be:\n\nfit the model\nuse the predict() function giving the grid of values on which computing predictions\ncalculating the confidence intervals\nplotting the results\n\nEverything can be simplified using some packages to perform each step and returning a plot:\n\n\nallEffects() from the effects() package (return a base R plot)\n\nggeffect() from the ggeffect() package (return a ggplot2 object)\n\nplot_model from the sjPlot package (similar to ggeffect())"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictor",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#categorical-predictor",
    "title": "Binomial GLM",
    "section": "1 categorical predictor",
    "text": "1 categorical predictor\nIn this situation we can just plot the marginal probabilities for each level of the categorical predictor. Plotting our exam dataset:"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictor",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#numerical-predictor",
    "title": "Binomial GLM",
    "section": "1 numerical predictor",
    "text": "1 numerical predictor"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#alleffects",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#alleffects",
    "title": "Binomial GLM",
    "section": "allEffects()",
    "text": "allEffects()"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#ggeffectplot_model",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#ggeffectplot_model",
    "title": "Binomial GLM",
    "section": "\nggeffect()/plot_model()\n",
    "text": "ggeffect()/plot_model()"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#plotting-coefficients",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#plotting-coefficients",
    "title": "Binomial GLM",
    "section": "Plotting coefficients",
    "text": "Plotting coefficients\n\n\nSometimes could be useful to plot the estimated sampling distribution of a coefficient. For example, we can plot the tv_shows effect on our example. I’ve written the plot_param() function that directly create a basic-plot given the model and the coefficient name. The plot highlight the null value and the 95% Wald confidence interval."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#glm---diagnostic-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#glm---diagnostic-1",
    "title": "Binomial GLM",
    "section": "GLM - Diagnostic",
    "text": "GLM - Diagnostic\nThe diagnostic for GLM is similar to standard linear models. Some areas are more complicated for example residual analysis and goodness of fit. We will see:\n\nDeviance\n\\(R^2\\)\nResiduals\n\nTypes of residuals\nResidual deviance\n\n\nClassification accuracy\nOutliers and influential observations"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#likelihood",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#likelihood",
    "title": "Binomial GLM",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood is the joint probability of the observed data viewed as a function of the parameters.\n\\[\n\\log \\mathcal{L}(\\mu|x) = \\sum^n_{i = 1} \\log(\\mu|x_i)\n\\]\n\nx &lt;- rnorm(10) # data from normal distribution\n\n# data\nx\n#&gt;  [1] -1.8694218  0.1218114 -1.3832446  0.9884026 -0.4416857 -0.3099413\n#&gt;  [7] -0.8407464 -1.8613016  0.2581889 -0.1618937\n\n# the model is a normal distribution with mu = 0 and sd = 1\ndnorm(x, 0, 1)\n#&gt;  [1] 0.06950842 0.39599347 0.15325971 0.24477683 0.36186586 0.38023327\n#&gt;  [7] 0.28016802 0.07056927 0.38586439 0.39374833\n\n# log likelihood\nsum(log(dnorm(x, 0, 1)))\n#&gt; [1] -14.66699"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#likelihood-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#likelihood-1",
    "title": "Binomial GLM",
    "section": "Likelihood",
    "text": "Likelihood\nBy summing the logarithm all the red segments we obtain the log likelihood of the model given the observed data."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#likelihood-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#likelihood-2",
    "title": "Binomial GLM",
    "section": "Likelihood",
    "text": "Likelihood\nIn the previous slide we tried only 3 values for the mean. Let’s image to calculate the log likelihood for several different means. The parameter with that is associated with highest likelihood is called the maximum likelihood estimator. In fact, the \\(\\mu = 0\\) is associated with the highest likelihood."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance",
    "title": "Binomial GLM",
    "section": "Deviance",
    "text": "Deviance\nThe (residual) deviance in the context of GLM can be considered as the distance between the current model and a perfect model (often called saturated model).\n\\[\nD_{res} = -2[\\log(\\mathcal{L}_{current}) - (\\log\\mathcal{L}_{sat})]\n\\]\nWhere \\(\\mathcal{L}\\) is the likelihood of the considered model (see the previous slides). Clearly, the lower the deviance, the closer the current model to the perfect model suggesting a good fit."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---saturated-model",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---saturated-model",
    "title": "Binomial GLM",
    "section": "Deviance - Saturated model",
    "text": "Deviance - Saturated model\nThe saturated model is a model where each observation \\(x_i\\) has a parameter."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---null-model",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---null-model",
    "title": "Binomial GLM",
    "section": "Deviance - Null model",
    "text": "Deviance - Null model\nAnother important quantity is the null deviance that is expressed as the distance between the null model and the saturated model.\n\\[\nD_{null} = -2[\\log(\\mathcal{L}_{null}) - (\\log\\mathcal{L}_{sat})]\n\\]\nThe null deviance can be interpreted as the maximal deviance because is estimated using a model without predictors. A good model will have a residual deviance lower than the null model."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---null-model-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---null-model-1",
    "title": "Binomial GLM",
    "section": "Deviance - Null model",
    "text": "Deviance - Null model"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test",
    "title": "Binomial GLM",
    "section": "Deviance - Likelihood Ratio Test",
    "text": "Deviance - Likelihood Ratio Test\nWhen we perform a likelihood ratio test (see previous slides) we are essentially comparing the residual deviance (or the likelihood) of two models.\n\nfit_null &lt;- glm(y ~ 1, data = dat, family = binomial(link = \"logit\"))\nfit_current &lt;- glm(y ~ x, data = dat, family = binomial(link = \"logit\"))"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-1",
    "title": "Binomial GLM",
    "section": "Deviance - Likelihood Ratio Test",
    "text": "Deviance - Likelihood Ratio Test\nWith the null model clearly the residual deviance is the same as the null deviance because we are not using predictors to reduce the deviance.\n\nsummary(fit_null)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ 1, family = binomial(link = \"logit\"), data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -1.044  -1.044  -1.044   1.317   1.317  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept)  -0.3228     0.2865  -1.126     0.26\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 68.029  on 49  degrees of freedom\n#&gt; Residual deviance: 68.029  on 49  degrees of freedom\n#&gt; AIC: 70.029\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-2",
    "title": "Binomial GLM",
    "section": "Deviance - Likelihood Ratio Test",
    "text": "Deviance - Likelihood Ratio Test\nIf the predictor x is useful in explaining y the residual deviance will be reduced compared to the null (overall deviance).\n\nsummary(fit_current)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = binomial(link = \"logit\"), data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.9121  -0.4071  -0.2063   0.6259   1.7336  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -5.045      1.470  -3.432 0.000599 ***\n#&gt; x              9.535      2.659   3.586 0.000336 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 68.029  on 49  degrees of freedom\n#&gt; Residual deviance: 36.745  on 48  degrees of freedom\n#&gt; AIC: 40.745\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-3",
    "title": "Binomial GLM",
    "section": "Deviance - Likelihood Ratio Test",
    "text": "Deviance - Likelihood Ratio Test\nComparing the two models we can understand if the deviance reduction can be considered statistically significant.\n\nanova(fit_null, fit_current, test = \"LRT\")\n\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance    `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1          49         68.0    NA     NA   NA           \n#&gt; 2          48         36.7     1     31.3  0.0000000223\n\n\nNotice that the difference between the two residual deviances is the test statistics that is distributed as a \\(\\chi^2\\) with \\(df = 1\\)."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---likelihood-ratio-test-4",
    "title": "Binomial GLM",
    "section": "Deviance - Likelihood Ratio Test",
    "text": "Deviance - Likelihood Ratio Test"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance-1",
    "title": "Binomial GLM",
    "section": "Deviance1\n",
    "text": "Deviance1\n\n\n\n\n\n\n\n\n\nAdapted from https://bookdown.org/egarpor/SSS2-UC3M/logreg-deviance.html"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---example",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---example",
    "title": "Binomial GLM",
    "section": "Deviance - Example",
    "text": "Deviance - Example\nLet’s fit the three models:\n\n# null\nfit0 &lt;- glm(exam ~ 1, data = dat, family = binomial(link = \"logit\"))\n# current\nfit &lt;- glm(exam ~ tv_shows, data = dat, family = binomial(link = \"logit\"))\n# saturated\nfits &lt;- glm(exam ~ 0 + id, data = dat, family = binomial(link = \"logit\"))"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance---example-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance---example-1",
    "title": "Binomial GLM",
    "section": "Deviance - Example",
    "text": "Deviance - Example\nWe can calculate the residual deviance:\n\n-2*(logLik(fit) - logLik(fits))\n\n#&gt; 'log Lik.' 126.3001 (df=2)\n\ndeviance(fit)\n\n#&gt; [1] 126.3001\n\n\nWe can calculate the null deviance:\n\n-2*(logLik(fit0) - logLik(fits))\n\n#&gt; 'log Lik.' 130.6836 (df=1)\n\ndeviance(fit0)\n\n#&gt; [1] 130.6836"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#r2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#r2",
    "title": "Binomial GLM",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nCompared to the standard linear regression, there are multiple ways to calculate an \\(R^2\\) like measure for GLMs and there is no consensus about the most appropriate method. There are some useful resources:\n\nhttps://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/\n\nTo note, some measures are specific for the binomial GLM while other measures can be applied also to other GLMs (e.g., the poisson)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#r2-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#r2-1",
    "title": "Binomial GLM",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nWe will se:\n\nMcFadden’s pseudo-\\(R^2\\) (for GLMs in general)\nTjur’s \\(R^2\\) (only for binomial/binary models)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#mcfaddens-pseudo-r2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#mcfaddens-pseudo-r2",
    "title": "Binomial GLM",
    "section": "McFadden’s pseudo-\\(R^2\\)\n",
    "text": "McFadden’s pseudo-\\(R^2\\)\n\nThe McFadden’s pseudo-\\(R^2\\) compute the ratio between the log-likelihood of the intercept-only (i.e., null) model and the current model(McFadden, 1987):\n\\[\nR^2 = 1 - \\frac{\\log(\\mathcal{L_{current}})}{\\log(\\mathcal{L_{null}})}\n\\]\nThere is also the adjusted version that take into account the number of parameters of the model. In R can be computed manually or using the performance::r2_mcfadden():\n\nperformance::r2_mcfadden(fit)\n\n#&gt; # R2 for Generalized Linear Regression\n#&gt;        R2: 0.034\n#&gt;   adj. R2: 0.018"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#tjurs-r2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#tjurs-r2",
    "title": "Binomial GLM",
    "section": "Tjur’s \\(R^2\\)\n",
    "text": "Tjur’s \\(R^2\\)\n\nThis measure is the easiest to interpret and calculate but can only be applied for binomial binary models (Tjur, 2009). Is the absolute value of the difference between the proportions of correctly classifying \\(y = 1\\) and \\(y = 0\\) from the model:\n\\[\\begin{align*}\np_1 = p(y_i = 1 |\\hat y_i = 1) \\\\\np_2 = p(y_i = 0 |\\hat y_i = 0) = 1 - p_1 \\\\\nR^2 = |p_1 - p_2|\n\\end{align*}\\]\n\nperformance::r2_tjur(fit2)\n\n#&gt; Tjur's R2 \n#&gt; 0.5654064"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#residuals",
    "title": "Binomial GLM",
    "section": "Residuals",
    "text": "Residuals\nThe main problem of GLM is that the mean and the variance are linked. For example, the mean of a binomial distribution is \\(np\\) and the variance is \\(np(1-p)\\). In the standard linear model we have two parameters, \\(\\mu\\) and the residual \\(\\sigma\\) that are independent.\n\nCodex &lt;- rnorm(1000)\ny &lt;- 0.1 + 0.6 * x + rnorm(1000)\nfit &lt;- lm(y ~ x)\nri_lm &lt;- residuals(fit) / sigma(fit)\nplot(fitted(fit), ri_lm, \n     ylab = \"Fitted Values\",\n     xlab = \"Standardized Residuals\")"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#residuals-1",
    "title": "Binomial GLM",
    "section": "Residuals",
    "text": "Residuals\nWhen using a linear model (thus assuming a constant \\(\\sigma\\) and independence between mean and variance) the residuals pattern is very different:\n\nCodex &lt;- rnorm(1000)\ny &lt;- rpois(1000, exp(log(10) + log(1.5) * x))\nfit_poi &lt;- lm(y ~ x)\n\nplot(fitted(fit_poi), rstandard(fit_poi),\n     xlab = \"Standardized Residuals\",\n     ylab = \"Fitted Values\")"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#residuals-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#residuals-2",
    "title": "Binomial GLM",
    "section": "Residuals",
    "text": "Residuals\nAs for standard linear models there are different types of residuals:\n\nraw (response) residuals\npearson residuals\ndeviance residuals\nstandardized residuals"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#raw-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#raw-residuals",
    "title": "Binomial GLM",
    "section": "Raw Residuals",
    "text": "Raw Residuals\nRaw residuals, also called response residuals are the simplest type of residuals. They are calculated as in standard regression as:\n\\[\\begin{align*}\nr_i = y_i - \\hat y_i\n\\end{align*}\\]\nWhere \\(\\hat y_i\\) are the fitted values where the inverse of the link function has been applied.\nIn R:\n\n# equivalent to residuals(fit, type = \"response\") \nri &lt;- fit$y - fitted(fit)\nri[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; 0.56826521 0.18039223 0.07097155 0.56848124 0.04399144\n\n\nThe problem is that in GLMs the mean and the variance of the distribution are not independent, creating problems in residual analysis."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#raw-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#raw-residuals-1",
    "title": "Binomial GLM",
    "section": "Raw Residuals",
    "text": "Raw Residuals\nWe can use the function car::residualPlot() to plot different type of residuals against fitted values:\n\ncar::residualPlot(fit, type = \"response\")"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#why-raw-residuals-are-problematic",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#why-raw-residuals-are-problematic",
    "title": "Binomial GLM",
    "section": "Why raw residuals are problematic?",
    "text": "Why raw residuals are problematic?\nThis plot1 shows an example with the same residual for two different \\(x\\) values on a Poisson GLM. Beyond the model itself, the same residual can be considered as extreme for low \\(x\\) values and plausible for high \\(x\\) values:\n\n\n\n\n\n\n\n\nAdapted from Dunn (2018), Fig. 8.1"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binned-raw-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binned-raw-residuals",
    "title": "Binomial GLM",
    "section": "Binned (raw) Residuals",
    "text": "Binned (raw) Residuals\nGelman and colleagues Gelman & Hill (2006) proposed a type of residuals called binned residuals to solve the problem of the previous plot for Binomial GLMs:\n\ndivide the fitted values into \\(n\\) bins. The number is arbitrary but we need each bin to have enough observation to compute a reliable average\ncalculate the average fitted value and residual for each bin\nfor each bin we can compute the standard error as \\(SE = \\frac{\\hat p_j (1 - p_j)}{n_j}\\) where \\(p_j\\) is the average fitted probability and \\(n_j\\) is the number of observation in the bin \\(j\\)\n\nThen we can plot each bin and the confidence intervals (e.g., as \\(\\pm 2*SE\\)) where ~95% of binned residuals should be within the CI if the model is true"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binned-raw-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binned-raw-residuals-1",
    "title": "Binomial GLM",
    "section": "Binned (raw) Residuals",
    "text": "Binned (raw) Residuals\nWe can use the arm::binnedplot() function to automatically create and plot the binned residuals:\n\nCodepar(mfrow = c(1,2))\nplot(x, y, xlab = \"Expected Values\", ylab = \"Raw Residual\", main = \"Raw Residual Plot\")\nbinnedplot(x,y)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#pearson-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#pearson-residuals",
    "title": "Binomial GLM",
    "section": "Pearson residuals",
    "text": "Pearson residuals\nPearson residuals are raw residuals divided by the standard deviation of each residual. Given that the mean-variance relationship of GLMs, dividing by the standard deviation partially solve the mean-variance relationship. The denominator can be calculated just using the appropriate variance formula for that specific GLM.\n\\[\nr_i = \\frac{y_i - \\hat y_i}{\\sqrt{V(\\hat y_i)}}\n\\]"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#pearson-vs-raw-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#pearson-vs-raw-residuals",
    "title": "Binomial GLM",
    "section": "Pearson vs Raw residuals",
    "text": "Pearson vs Raw residuals\nWe can see the difference for a Poisson model1 when using raw vs pearson residuals. The non-constant variance is controlled on the right.\n\n\n\n\n\n\n\n\nWe are using a Poisson model only because the residual pattern is more clear compared to a binomial model"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#pearson-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#pearson-residuals-1",
    "title": "Binomial GLM",
    "section": "Pearson residuals",
    "text": "Pearson residuals\nFor the Binomial (Bernoulli) GLM, the variance is calculated as \\(\\hat p(1 - \\hat p)\\) where \\(\\hat p\\) is the residual value.\n\\[\nr_i = \\frac{y_i - \\hat y_i}{\\sqrt{\\hat y_i(1 - \\hat y_i)}}\n\\]\n\n# equivalent to residuals(fit, type = \"pearson\")\nyi &lt;- fitted(fit)\nri_pearson &lt;- ri / sqrt(yi * (1 - yi))\nri_pearson[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; 1.17341398 0.42948479 0.14213499 1.21450070 0.09047435"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals",
    "title": "Binomial GLM",
    "section": "Deviance residuals",
    "text": "Deviance residuals\nDeviance residuals are based on the residual deviance that we defined before. In fact, the residual deviance was just the sum of squared deviance residuals.\n\\[\nr_i = sign(y_i - \\hat y_i) \\sqrt{-2[\\log \\mathcal{L}y_{i_{current}} - \\log \\mathcal{L}y_{i_{saturated}}]}\n\\] Where \\(sign\\) is the sign of the raw residual \\(i\\). For the Bernoulli model the \\(\\log \\mathcal{L}y_{i_{saturated}}\\) is always 0."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-1",
    "title": "Binomial GLM",
    "section": "Deviance Residuals",
    "text": "Deviance Residuals"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-2",
    "title": "Binomial GLM",
    "section": "Deviance Residuals",
    "text": "Deviance Residuals\nTo calculate and demonstrate manually the deviance residuals we can compute them manually in R:\n\n# residual(fit, type = \"deviance\")\nyhat &lt;- fitted(fit) # fitted\ny &lt;- fit$y # actual values\n\n# the likelihood is dbinom\nri_dev &lt;- sign(y - yhat) * sqrt(-2*(log(dbinom(y, 1, yhat)) - log(dbinom(y, 1, y))))"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#deviance-residuals-3",
    "title": "Binomial GLM",
    "section": "Deviance Residuals",
    "text": "Deviance Residuals\n\n# notice that the log lik of the saturated model log(dbinom(y, 1, y)) is 0\nlog(dbinom(y, 1, y))\n\n#&gt;  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n#&gt;  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 \n#&gt; 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n#&gt;  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n\nri_dev[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.2370588  0.3033530 -1.0310244  0.2331303 -0.8429485\n\nresiduals(fit, type = \"deviance\")[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.2370588  0.3033530 -1.0310244  0.2331303 -0.8429485"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#quick-recap-about-hat-values-in-linear-regression",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#quick-recap-about-hat-values-in-linear-regression",
    "title": "Binomial GLM",
    "section": "Quick recap about hat values in linear regression",
    "text": "Quick recap about hat values in linear regression\n\nThe hat matrix \\(H\\) is calculated as \\(H = X \\left(X^{\\top} X \\right)^{-1} X^{\\top}\\) is a \\(n \\times n\\) matrix where \\(n\\) is the number of observations. The diagonal of the \\(H\\) matrix contained the hat values or leverages.\nThe \\(i^{th}\\) leverage score (\\(h_{ii}\\)) is interpreted as the weighted distance between \\(x_i\\) and the mean of \\(x_i\\)’s. In practical terms is the \\(i^{th}\\) observed value influence the \\(i^{th}\\) fitted value. An high leverage suggest that the observation is far from the mean of predictors and have an high influence on the fitted values.\n\n\n\\(h_{ii}\\) ranges between 0 and 1\nThe sum of all \\(h_{ii}\\) values is the number of parameters \\(p\\)\n\nAs a rule of thumb, an observation have an high leverage if \\(h_{ii} &gt; 2 \\bar h\\) where \\(\\bar h\\) is the average hat value"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#quick-recap-about-hatvalues-in-linear-regression",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#quick-recap-about-hatvalues-in-linear-regression",
    "title": "Binomial GLM",
    "section": "Quick recap about hatvalues in linear regression",
    "text": "Quick recap about hatvalues in linear regression\n\nFor a simple linear regression (\\(y \\sim x\\)) the hat values are calculated as:\n\\[\nh_i = \\frac{1}{n} + \\frac{(X_i - \\bar X)^2}{\\sum^n_{j = 1}(X_i - \\bar X)^2}\n\\]\nIn R the function hatvalues() return the diagonal of the \\(H\\) matrix for glm and lm:\n\nhatvalues(fit)[1:10]\n\n#&gt;          1          2          3          4          5          6          7 \n#&gt; 0.02874015 0.03492581 0.03968557 0.02782301 0.04357230 0.04652508 0.04435804 \n#&gt;          8          9         10 \n#&gt; 0.04012613 0.03948651 0.04743780\n\n\nTo note, for GLM and multiple regression in general, the equation is different and more complicated but the intepretation and the R implementation is the same."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals",
    "title": "Binomial GLM",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nBoth the response, pearson and deviance residuals can be considered as raw residuals. We can standardize residuals by dividing for the standard error computed with the hat values. In this way, the distribution will be approximately normal with \\(\\mu = 0\\) and \\(\\sigma = 1\\).\n\\[\nr_{s_i} = \\frac{r_i}{\\sqrt{(1 - \\hat h_{ii})}}\n\\]\nWhere \\(r_i\\) can be raw, pearson or deviance residuals."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-1",
    "title": "Binomial GLM",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nIn R they can be extracted using rstandard():\n\nrstandard(fit, type = \"pearson\")[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.1712897  0.2208858 -0.8546823  0.1683326 -0.6678440\n\nrstandard(fit, type = \"deviance\")[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.2405406  0.3087934 -1.0521126  0.2364427 -0.8619359"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-2",
    "title": "Binomial GLM",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\nWe can try to manually calculate the residuals to better understand.\n\nyhat &lt;- fitted(fit) # fitted\nyi &lt;- fit$y # observed\nhi &lt;- hatvalues(fit) # diagonal of the hat matrix\n\n# pearson residuals\npi &lt;- (yi - yhat) / sqrt(yhat * (1 - yhat))\n\n# standardized\npis &lt;- pi / sqrt(1 - hi)\npis[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.1712897  0.2208858 -0.8546823  0.1683326 -0.6678440\n\nrstandard(fit, type = \"pearson\")[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.1712897  0.2208858 -0.8546823  0.1683326 -0.6678440"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#standardized-residuals-3",
    "title": "Binomial GLM",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n# deviance (for binomial GLM the loglik of the saturated model is 0)\ndi &lt;- sign(yi - yhat) * sqrt(-2*log(dbinom(y, 1, yhat)))\n\n# standardized\ndis &lt;- di / sqrt(1 - hi)\n\ndis[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.2405406  0.3087934 -1.0521126  0.2364427 -0.8619359\n\nrstandard(fit, type = \"deviance\")[1:5]\n\n#&gt;          1          2          3          4          5 \n#&gt; -0.2405406  0.3087934 -1.0521126  0.2364427 -0.8619359"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals",
    "title": "Binomial GLM",
    "section": "Quantile residuals",
    "text": "Quantile residuals\nThe quantile residuals is another proposal for residual analysis. The idea is to map the quantile of the cumulative density function (CDF) of the random component into the CDF of the normal distribution."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals-1",
    "title": "Binomial GLM",
    "section": "Quantile residuals",
    "text": "Quantile residuals\nQuantile residuals are very useful especially for Discrete GLMs (binomial and poisson) and are exactly normally distributed (under respected model assumptions) compared to deviance and pearson residuals (Dunn & Smyth, 1996). They can be calculated using the statmod::qresid(fit) function. Authors suggest to run the function 4 times to disentagle between the randomization and the systematic component.\n\nstatmod::qresid(fit)[1:5]\n\n#&gt; [1] -0.33677932  1.04616589 -0.43878900  0.01511622 -1.07265867\n\nstatmod::qresid(fit)[1:5] # different every time\n\n#&gt; [1]  1.2349067 -0.6689038  0.2171433 -0.9988869 -0.4189008"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#quantile-residuals-2",
    "title": "Binomial GLM",
    "section": "Quantile residuals",
    "text": "Quantile residuals"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#more-on-residuals",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#more-on-residuals",
    "title": "Binomial GLM",
    "section": "More on residuals",
    "text": "More on residuals\nResiduals for GLMs are complicated. To have a more detailed and clear overview see:\n\n\nAgresti (2015), Sections 4.4.5, 4.4.6\n\nDunn & Smyth (2018), Chapter 8"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate",
    "title": "Binomial GLM",
    "section": "Classification accuracy/Error rate",
    "text": "Classification accuracy/Error rate\nThe error rate (ER) is defined as the proportion of cases for which the deterministic prediction i.e. guessing \\(y_i = 1\\) if \\(logit^{-1}(\\hat y_i) &gt; 0.5\\) and guessing \\(y_i = 0\\) if \\(logit^{-1}(\\hat y_i) &gt; 0.5\\) is wrong. Clearly, \\(1 - ER\\) is the classification accuracy.\nI wrote the error_rate function that simply compute the error rate of a given model:\n\nerror_rate &lt;- function(fit){\n    pi &lt;- predict(fit, type = \"response\")\n    yi &lt;- fit$y\n    cr &lt;- mean((pi &gt; 0.5 & yi == 1) | (pi &lt; 0.5 & yi == 0))\n    1 - cr # error rate\n}"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate-1",
    "title": "Binomial GLM",
    "section": "Classification accuracy/Error rate",
    "text": "Classification accuracy/Error rate\n\nerror_rate(fit)\n\n#&gt; [1] 0.14\n\n\nWe could compare the error rate of a given model with the error rate of the null model or another similar model (with a model comparison approach):\n\nfit0 &lt;- update(fit, . ~ -x) # removing the x predictor, now intercept only model\nerror_rate(fit0)\n\n#&gt; [1] 0.48\n\n# the error rate of the null model is ... greater/less than the actual model\ner_ratio = error_rate(fit0)/error_rate(fit)\n\nThe error rate of the null model is 3.429 times greater than the actual model."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#classification-accuracyerror-rate-2",
    "title": "Binomial GLM",
    "section": "Classification accuracy/Error rate",
    "text": "Classification accuracy/Error rate\n\nFor a given model, the error rate should be less than \\(0.5\\), otherwise setting all \\(\\beta\\)’s to 0 (i.e., null model) would be considered a better model (Gelman et al., 2020).\nFor example if the average \\(p\\) in a dataset is \\(\\overline p = 0.3\\) means that a null model would have an error rate of \\(0.3\\) and a classification accuracy of \\(1 - \\overline p = 0.7\\).\nIncluding predictors we aim to reduce the error rate (compared to the null model) because the ability to correctly classify an observation increase.\nThe error rate can be misleading, see Gelman et al. (2020) (pp. 255-256)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#outliers-and-influential-observations",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#outliers-and-influential-observations",
    "title": "Binomial GLM",
    "section": "Outliers and influential observations",
    "text": "Outliers and influential observations\nIdentification of influential observation and outliers of GLMs is very similar to standard regression models. We will briefly see:\n\nCook Distances\nDFBETAs"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#cook-distances",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#cook-distances",
    "title": "Binomial GLM",
    "section": "Cook Distances",
    "text": "Cook Distances\nThe Cook Distance of an observation \\(i\\) measured the impact of that observation on the overall model fit. If removing the observation \\(i\\) has an high impact, the observation \\(i\\) is likely an influential observation. For GLMs they are defined as:\n\\[\\begin{align*}\nD_i = \\frac{r_i^2}{\\phi p} \\frac{h_{ii}}{1 - h_{ii}}\n\\end{align*}\\]\nWhere \\(p\\) is the number of model parameters, \\(r_i\\) are the standardized pearson residuals (rstandard(fit, type = \"pearson\")) and \\(h_{ii}\\) are the hatvalues (leverages). \\(\\phi\\) is the dispersion parameter of the GLM that for binomial and poisson models is fixed to 1 (see Dunn (2018, Table 5.1)) Usually an observation is considered influential if \\(D_i &gt; \\frac{4}{n}\\) where \\(n\\) is the sample size."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#dfbetas",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#dfbetas",
    "title": "Binomial GLM",
    "section": "DFBETAs",
    "text": "DFBETAs\nDFBETAs measure the impact of the observation \\(i\\) on the estimated parameter \\(\\beta_j\\):\n\\[\\begin{align*}\nDFBETAS_i = \\frac{\\beta_j - \\beta_{j(i))}}{\\sigma_{\\beta_{j(i)}}}\n\\end{align*}\\]\nWhere \\(i\\) denote the parameters and standard error on a model fitted without the \\(i\\) observation1. Usually an observation is considered influential if \\(|DFBETAs_{i}| &gt; \\frac{2}{\\sqrt{n}}\\) where \\(n\\) is the sample size.\nNote that you are not required to re-fit the model to estimate the DFBETAs, you can use the \\(H\\) matrix, see https://en.wikipedia.org/wiki/Influential_observation"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#extracting-influence-measures",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#extracting-influence-measures",
    "title": "Binomial GLM",
    "section": "Extracting influence measures",
    "text": "Extracting influence measures\nIn R we can use the influence.measures() function to calculate each influence measure explained before1.\n\nfit &lt;- update(fit, data = fit$model[1:50, ])\n\n\ninfl &lt;- influence.measures(fit)$infmat\nhead(infl)\n\n#&gt;        dfb.1_      dfb.x       dffit    cov.r       cook.d        hat\n#&gt; 1 -0.04419414 0.04109800 -0.04442749 1.070827 0.0004340963 0.02874015\n#&gt; 2 -0.04672053 0.05681844  0.06310261 1.075710 0.0008828590 0.03492581\n#&gt; 3 -0.11865551 0.05713533 -0.23265873 1.028011 0.0150937972 0.03968557\n#&gt; 4 -0.03348170 0.03972704  0.04294688 1.069920 0.0004054759 0.02782301\n#&gt; 5 -0.14588644 0.10150547 -0.19921442 1.051194 0.0101596403 0.04357230\n#&gt; 6 -0.07575414 0.10700698  0.14303484 1.074641 0.0048052405 0.04652508\n\n\nThe first two columns are the DFBETAs for each parameter, the cook.d columns contains the Cook Distances and hat is the diagonal of the \\(H\\) matrix.\nThe function actually computes also other influence measures, see Dunn (2018, section 8.8.3) for other details"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#plotting-influence-measures",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#plotting-influence-measures",
    "title": "Binomial GLM",
    "section": "Plotting influence measures",
    "text": "Plotting influence measures\nI wrote the cook_plot() function to easily plot the cook distances along with the identification of influential observations:\n\ncook_plot(fit)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#plotting-influence-measures-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#plotting-influence-measures-1",
    "title": "Binomial GLM",
    "section": "Plotting influence measures",
    "text": "Plotting influence measures\nI wrote the dfbeta_plot() function to easily plot the cook distances along with the identification of influential observations:\n\ndfbeta_plot(fit)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary",
    "text": "Binomial vs Binary\nThere are several practical differences between binomial and binary models:\n\ndata structure\nfitting function in R\nresiduals and residual deviance\ntype of predictors"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary data structure",
    "text": "Binomial vs Binary data structure\nThe most basic Binomial regression is a vector of binary \\(y\\) values and a continuous or categorical predictor. Let’s see a common data structure in this case:\n\nCoden &lt;- 30\nx &lt;- runif(n, 0, 1)\ndat &lt;- sim_design(ns = n, nx = list(x = x))\nb0 &lt;- qlogis(0.01)\nb1 &lt;- 8\n\ndat |&gt; \n    sim_data(plogis(b0 + b1*x), \"binomial\") |&gt; \n    select(-lp) |&gt; \n    round(2) |&gt; \n    filor::trim_df(4)\n\n#&gt; # A tibble: 9 × 3\n#&gt;   id    x     y    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     0.85  1    \n#&gt; 2 2     0.16  0    \n#&gt; 3 3     0.08  0    \n#&gt; 4 4     0.17  0    \n#&gt; 5 ...   ...   ...  \n#&gt; 6 27    0.52  1    \n#&gt; 7 28    0.64  1    \n#&gt; 8 29    0.11  0    \n#&gt; 9 30    0.54  1"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-1",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary data structure",
    "text": "Binomial vs Binary data structure\nOr equivalently with a categorical variable:\n\nn &lt;- 15\nx &lt;- c(\"a\", \"b\")\ndat &lt;- sim_design(n, cx = list(x = x))\nb0 &lt;- qlogis(0.4)\nb1 &lt;- log(odds_ratio(0.7, 0.4))\n\ndat |&gt; \n    sim_data(plogis(b0 + b1*x_c), \"binomial\") |&gt; \n    dplyr::select(-x_c, -lp) |&gt; \n    filor::trim_df(4)\n\n#&gt; # A tibble: 9 × 3\n#&gt;   id    x     y    \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     a     1    \n#&gt; 2 2     b     0    \n#&gt; 3 3     a     0    \n#&gt; 4 4     b     0    \n#&gt; 5 ...   ...   ...  \n#&gt; 6 27    a     0    \n#&gt; 7 28    b     1    \n#&gt; 8 29    a     0    \n#&gt; 9 30    b     0"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-2",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary data structure",
    "text": "Binomial vs Binary data structure\nWhen using a Binomial data structure we count the number of success for each level of \\(x\\). nc is the number of 1 responses, nf is the number of 0 response out of nt trials:\n\nCodex &lt;- seq(0, 1, 0.05)\nnt &lt;- 10\nnc &lt;- rbinom(length(x), nt, plogis(qlogis(0.01) + 8*x))\ndat &lt;- data.frame(x, nc, nf = nt - nc,  nt)\n\ndat |&gt; \n    filor::trim_df(4)\n\n#&gt; # A tibble: 9 × 4\n#&gt;   x     nc    nf    nt   \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 0     0     10    10   \n#&gt; 2 0.05  0     10    10   \n#&gt; 3 0.1   1     9     10   \n#&gt; 4 0.15  0     10    10   \n#&gt; 5 ...   ...   ...   ...  \n#&gt; 6 0.85  9     1     10   \n#&gt; 7 0.9   8     2     10   \n#&gt; 8 0.95  9     1     10   \n#&gt; 9 1     10    0     10"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-3",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary data structure",
    "text": "Binomial vs Binary data structure\nWith a categorical variable we have essentially a contingency table:\n\nCodex &lt;- c(\"a\", \"b\")\nnt &lt;- 10\nnc &lt;- rbinom(length(x), nt, plogis(qlogis(0.4) + log(odds_ratio(0.7, 0.4))*ifelse(x == \"a\", 1, 0)))\n\ndatc &lt;- data.frame(x, nc, nf = nt - nc,  nt)\ndatc\n\n#&gt; # A tibble: 2 × 4\n#&gt;   x        nc    nf    nt\n#&gt;   &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 a         5     5    10\n#&gt; 2 b         4     6    10"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-4",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: data structure",
    "text": "Binomial vs Binary: data structure\nClearly, expanding or aggregating data is the way to convert a binary into a binomial data structure and the opposite:\n\n# from binomial to binary\nbin_to_binary(datc, nc, nt) |&gt; \n    select(y, x) |&gt; \n    filor::trim_df()\n\n#&gt; # A tibble: 9 × 2\n#&gt;   y     x    \n#&gt;   &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     a    \n#&gt; 2 1     a    \n#&gt; 3 1     a    \n#&gt; 4 1     a    \n#&gt; 5 ...   ...  \n#&gt; 6 0     b    \n#&gt; 7 0     b    \n#&gt; 8 0     b    \n#&gt; 9 0     b"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-5",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-data-structure-5",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: data structure",
    "text": "Binomial vs Binary: data structure\n\n# from binary to binomial\nbin_to_binary(datc, nc, nt)  |&gt;\n    binary_to_bin(y, x)\n\n#&gt; # A tibble: 2 × 4\n#&gt;   x        nc    nf    nt\n#&gt;   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 a         5     5    10\n#&gt; 2 b         4     6    10"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-multilevel-disclaimer",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-multilevel-disclaimer",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: multilevel disclaimer",
    "text": "Binomial vs Binary: multilevel disclaimer\n\nClearly, in the previous examples, each row correspond to a single observation (in the binary model) or a condition (in the binomial model). Furthermore each row/observation/participants is assumed to be independent.\nIf participants performed multiple trials for each condition, we can still use a binomial or binary model BUT we need to take into account the multilevel structure.\nIn practice, we need to use a mixed-effects GLM, for example with the lme4::glmer() package specifying the appropriate random structure."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary---fitting-function-in-r",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary---fitting-function-in-r",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: - fitting function in R",
    "text": "Binomial vs Binary: - fitting function in R\nThere is a single way to implement a binary model in R and two ways for the binomial version:\n\n# binary regression\nglm(y ~ x, family = binomial(link = \"logit\"))\n\n# binomial with cbind syntax, nc = number of 1s, nf = number of 0s, nc + nf = nt\nglm(cbind(nc, nf) ~ x, family = binomial(link = \"logit\"))\n\n# binomial with proportions and weights, equivalent to the cbind approach, nt is the total trials\nglm(nc/nt ~ x, weights = nt, binomial(link = \"logit\"))"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: residuals and residual deviance",
    "text": "Binomial vs Binary: residuals and residual deviance\nA more relevant difference is about the residual analysis. The binary regression has different residuals compared to the binomial model fitted on the same dataset1.\n\nfit_binomial &lt;- glm(nc/nt ~ x, weights = nt, data = dat_binomial, family = binomial(link = \"logit\"))\nfit_binary &lt;- glm(y ~ x, data = dat_binary, family = binomial(link = \"logit\"))\n\nTo note, the binned residuals could be considered as an attempt to mitigate the binary residuals problem by creating bins of fitted/residual values similarly to fitting a binomial model."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-1",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: residuals and residual deviance",
    "text": "Binomial vs Binary: residuals and residual deviance"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-2",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: residuals and residual deviance",
    "text": "Binomial vs Binary: residuals and residual deviance\nThe residual deviance is also different. In fact, there is more residual deviance on the binary compared to the binomial model. However, comparing two binary and binomial models actually leads to the same conclusion. In other terms the deviance seems to be on a different scale1:\n\nfit0_binary &lt;- update(fit_binary, . ~ -x) # null binary model\nfit0_binomial &lt;- update(fit_binomial, . ~ -x) # null binary model\n\nThanks to Prof. Altoè for this suggestion"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-residuals-and-residual-deviance-3",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: residuals and residual deviance",
    "text": "Binomial vs Binary: residuals and residual deviance\nIn fact, if we compare the full model with the null model in both binary and binomial version, the LRT is the same but the deviance is on a different scale. Comparing a binary with a binomial model could be completely misleading.\n\nanova(fit_binomial, fit0_binomial, test = \"Chisq\")\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1          19         14.4    NA      NA   NA       \n#&gt; 2          20        148.     -1    -133.   8.27e-31\nanova(fit_binary, fit0_binary, test = \"Chisq\")\n#&gt; # A tibble: 2 × 5\n#&gt;   `Resid. Df` `Resid. Dev`    Df Deviance `Pr(&gt;Chi)`\n#&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1         208         156.    NA      NA   NA       \n#&gt; 2         209         289.    -1    -133.   8.27e-31"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: type of predictors",
    "text": "Binomial vs Binary: type of predictors\nThis point is less relevant in this course but important in general. Usually, binary regression is used when the predictor is at the trial level whereas binomial regression is used when the predictor is at the participant level. When both levels are of interests one should use a mixed-effects model where both levels can be modeled.\n\nthe probability of correct responses during an exam as a function of the question difficulty (each trial/row could have a different \\(x\\) level)\nthe probability of passing the exam as a function of the high-school background regardless of having multiple trials, each trial/row has the same \\(x_i\\) for the participant \\(i\\)"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-1",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: type of predictors",
    "text": "Binomial vs Binary: type of predictors\n\nThe probability of correct responses during an exam as a function of the question difficulty\n\n\neach question (i.e., trial) has a 0/1 response and a difficulty level\nwe are modelling a single participant, otherwise we need a multilevel (mixed-effects) model\n\n\nCodedat &lt;- expand_grid(id = 1, question = 1:30)\ndat$y &lt;- rbinom(nrow(dat), 1, 0.7)\ndat$difficulty &lt;- sample(1:5, nrow(dat), replace = TRUE)\n\ndat |&gt; \n    select(id, question, difficulty, y) |&gt; \n    filor::trim_df()\n\n#&gt; # A tibble: 9 × 4\n#&gt;   id    question difficulty y    \n#&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;\n#&gt; 1 1     1        1          1    \n#&gt; 2 1     2        3          1    \n#&gt; 3 1     3        3          1    \n#&gt; 4 1     4        1          0    \n#&gt; 5 ...   ...      ...        ...  \n#&gt; 6 1     27       1          1    \n#&gt; 7 1     28       5          1    \n#&gt; 8 1     29       1          0    \n#&gt; 9 1     30       1          1"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-2",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: type of predictors",
    "text": "Binomial vs Binary: type of predictors\n\nthe probability of passing the exam as a function of the high-school background\n\n\neach “background” has different students that passed or not the exam (0/1)\n\n\n\n#&gt; # A tibble: 4 × 4\n#&gt;   background nc    nf    nt   \n#&gt;   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 math       20    10    30   \n#&gt; 2 chemistry  16    4     20   \n#&gt; 3 art        4     6     10   \n#&gt; 4 sport      15    5     20\n\n\nOr the binary version:\n\n\n#&gt; # A tibble: 9 × 5\n#&gt;   y     background nc    nf    nt   \n#&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 1     math       20    10    30   \n#&gt; 2 1     math       20    10    30   \n#&gt; 3 1     math       20    10    30   \n#&gt; 4 1     math       20    10    30   \n#&gt; 5 ...   ...        ...   ...   ...  \n#&gt; 6 0     sport      15    5     20   \n#&gt; 7 0     sport      15    5     20   \n#&gt; 8 0     sport      15    5     20   \n#&gt; 9 0     sport      15    5     20"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#binomial-vs-binary-type-of-predictors-3",
    "title": "Binomial GLM",
    "section": "Binomial vs Binary: type of predictors",
    "text": "Binomial vs Binary: type of predictors\n\nTo note that despite we can convert between the binary/binomial, the two models are not always the same. The high-school background example can be easily modelled either with a binary or binomial model because the predictor is at the participant level that coincides with the trial level.\nOn the other side, the question difficulty example can only be modelled using a binary regression because each trial (0/1) has a different value for the predictor\nTo include both predictors or to model multiple participants on the question difficulty example we need a mixed-effects model where both levels together with the repeated-measures can be handled.\na very clear overview of this topic can be found here https://www.rensvandeschoot.com/tutorials/generalised-linear-models-with-glm-and-lme4/"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-1",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-1",
    "title": "Binomial GLM",
    "section": "Probit link",
    "text": "Probit link\n\nThe mostly used link function when using a binomial GLM is the logit link. The probit link is another link function that can be used. The overall approach is the same between logit and probit models. The only difference is the parameter interpretation (i.e., no odds ratios) and the specific link function (and the inverse) to use.\nThe probit model use the cumulative normal distribution but the actual difference with a logit functions is neglegible."
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-2",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-2",
    "title": "Binomial GLM",
    "section": "Probit link",
    "text": "Probit link"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-3",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-3",
    "title": "Binomial GLM",
    "section": "Probit link",
    "text": "Probit link\nWhen using the probit link the parameters are interpreted as difference in z-scores associated with a unit increase in the predictors. In fact probabilities are mapped into z-scores using the cumulative normal distribution.\n\np1 &lt;- 0.7\np2 &lt;- 0.5\n\nqlogis(c(p1, p2)) # log(odds(p1)), logit link\n\n#&gt; [1] 0.8472979 0.0000000\n\nqnorm(c(p1, p2)) # probit link\n\n#&gt; [1] 0.5244005 0.0000000\n\nlog(odds_ratio(p1, p2)) # ~ beta1, logit link\n\n#&gt; [1] 0.8472979\n\npnorm(p1) - pnorm(p2) # ~beta1, probit link\n\n#&gt; [1] 0.06657389"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-4",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#probit-link-4",
    "title": "Binomial GLM",
    "section": "Probit link",
    "text": "Probit link"
  },
  {
    "objectID": "slides/02-binomial-glm/02-binomial-glm.html#references",
    "href": "slides/02-binomial-glm/02-binomial-glm.html#references",
    "title": "Binomial GLM",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. https://play.google.com/store/books/details?id=jlIqBgAAQBAJ\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2009). Introduction to Meta-Analysis. https://doi.org/10.1002/9780470743386\n\n\nDunn, P. K., & Smyth, G. K. (1996). Randomized quantile residuals. Journal of Computational and Graphical Statistics: A Joint Publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America, 5, 236–244. https://doi.org/10.1080/10618600.1996.10474708\n\n\nDunn, P. K., & Smyth, G. K. (2018). Generalized linear models with examples in r. Springer. https://play.google.com/store/books/details?id=tBh5DwAAQBAJ\n\n\nGelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. https://doi.org/10.1017/CBO9780511790942\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nKnoblauch, K., & Maloney, L. T. (2012). Modeling psychophysical data in r. Springer New York. https://doi.org/10.1007/978-1-4614-4475-6\n\n\nMcFadden, D. (1987). Regression-based specification tests for the multinomial logit model. Journal of Econometrics, 34, 63–82. https://doi.org/10.1016/0304-4076(87)90067-4\n\n\nSánchez-Meca, J., Marín-Martínez, F., & Chacón-Moscoso, S. (2003). Effect-size indices for dichotomized outcomes in meta-analysis. Psychological Methods, 8, 448–467. https://doi.org/10.1037/1082-989X.8.4.448\n\n\nTjur, T. (2009). Coefficients of determination in logistic regression models—a new proposal: The coefficient of discrimination. The American Statistician, 63, 366–372. https://doi.org/10.1198/tast.2009.08210"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#poisson-glm",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#poisson-glm",
    "title": "Poisson GLM",
    "section": "Poisson GLM",
    "text": "Poisson GLM\nEverything that we discussed for the binomial GLM is also relevant for the Poisson GLM. We are gonna focus on specificity of the Poisson model in particular:\n\n\nPoisson distribution and link function\n\nParameters interpretation\n\nOverdispersion causes, consequences and remedies"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-1",
    "title": "Poisson GLM",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nThe Poisson distribution is defined as:\n\\[\np(y; \\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\n\\] Where the mean is \\(\\lambda\\) and the variance is \\(\\lambda\\)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-2",
    "title": "Poisson GLM",
    "section": "Poisson distribution",
    "text": "Poisson distribution"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-3",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-3",
    "title": "Poisson GLM",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nAs the mean increases also the variance increase and the distributions is approximately normal:"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-4",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#poisson-distribution-4",
    "title": "Poisson GLM",
    "section": "Poisson distribution",
    "text": "Poisson distribution"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#link-function",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#link-function",
    "title": "Poisson GLM",
    "section": "Link function",
    "text": "Link function\nThe common (and default in R) link function (\\(g(\\lambda)\\)) for the Poisson distribution is the log link function and the inverse of link function is the exponential.\n\\[\\begin{align*}\nlog(\\lambda) = \\beta_0 + \\beta_{1}X_{1} + ...\\beta_pX_p \\\\\n\\lambda = e^{\\beta_0 + \\beta_{1}X_{1} + ...\\beta_pX_p}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#an-example",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#an-example",
    "title": "Poisson GLM",
    "section": "An example…",
    "text": "An example…\nLet’s start by a simple example trying to explain the number of errors of math exercises by students (N = 100) as a function of the number of hours of study.\n\n\n\n\n\n\n\nid\n      studyh\n      solved\n    \n\n\n1\n88\n50\n\n\n2\n5\n11\n\n\n3\n65\n35\n\n\n4\n89\n57\n\n\n...\n...\n...\n\n\n97\n82\n58\n\n\n98\n18\n10\n\n\n99\n22\n14\n\n\n100\n1\n9"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#an-example-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#an-example-1",
    "title": "Poisson GLM",
    "section": "An example…",
    "text": "An example…\n\nThere is a clear non-linear and positive relationship\nThe both the mean and variance increase as a function of the predictor"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#model-fitting",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#model-fitting",
    "title": "Poisson GLM",
    "section": "Model fitting",
    "text": "Model fitting\nWe can fit the model using the glm function in R setting the appropriate random component (family) and the link function (link):\n\nfit &lt;- glm(solved ~ studyh, family = poisson(link = \"log\"), data = dat)\n\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = solved ~ studyh, family = poisson(link = \"log\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -2.27259  -0.66119  -0.07408   0.48584   2.20598  \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) 2.2664222  0.0487974   46.45   &lt;2e-16 ***\n#&gt; studyh      0.0204364  0.0006725   30.39   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1132.124  on 99  degrees of freedom\n#&gt; Residual deviance:   79.538  on 98  degrees of freedom\n#&gt; AIC: 597.89\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-1",
    "title": "Poisson GLM",
    "section": "Parameters intepretation",
    "text": "Parameters intepretation\n\nThe (Intercept) \\(2.266\\) is the log of the expected number of solved exercises for a student with 0 hours of studying. Taking the exponential we obtain the estimation on the response scale \\(9.645\\)\n\nthe studyh \\(0.020\\) is the increase in the expected increase of (log) solved exercises for a unit increase in hours of studying. Taking the exponential we obtain the ratio of increase of the number of solved exercises \\(1.021\\)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-2",
    "title": "Poisson GLM",
    "section": "Parameters intepretation",
    "text": "Parameters intepretation\nAgain, as in the binomial model, the effects are linear on the log scale but non-linear on the response scale."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-3",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation-3",
    "title": "Poisson GLM",
    "section": "Parameters intepretation",
    "text": "Parameters intepretation\nThe non-linearity can be easily seen using the predict() function:\n\nlinear &lt;- predict(fit, newdata = data.frame(studyh = c(10, 11)))\ndiff(linear) # same as the beta0\n\n#&gt;         2 \n#&gt; 0.0204364\n\nnonlinear &lt;- exp(linear) # or predict(..., type = \"response\")\ndiff(nonlinear)\n\n#&gt;        2 \n#&gt; 0.244286\n\n# ratio of increase when using the response scale\nnonlinear[2]/nonlinear[1]\n\n#&gt;        2 \n#&gt; 1.020647\n\n# equivalent to exp(beta1)\nexp(coef(fit)[2])\n\n#&gt;   studyh \n#&gt; 1.020647"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable",
    "title": "Poisson GLM",
    "section": "Parameters intepretation - Categorical variable",
    "text": "Parameters intepretation - Categorical variable\nLet’s make a similar example with the number of solved exercises comparing students who attended online classes and students attending in person. The class_c is the dummy version of class (0 = online, 1 = inperson).\n\n\n\n\n\n\n\nid\n      class\n      class_c\n      solved\n    \n\n\n1\nonline\n0\n10\n\n\n2\ninperson\n1\n18\n\n\n3\nonline\n0\n14\n\n\n4\ninperson\n1\n15\n\n\n...\n...\n...\n...\n\n\n97\nonline\n0\n10\n\n\n98\ninperson\n1\n15\n\n\n99\nonline\n0\n15\n\n\n100\ninperson\n1\n10"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-1",
    "title": "Poisson GLM",
    "section": "Parameters intepretation - Categorical variable",
    "text": "Parameters intepretation - Categorical variable"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-2",
    "title": "Poisson GLM",
    "section": "Parameters intepretation - Categorical variable",
    "text": "Parameters intepretation - Categorical variable\nR by default set the categorical variables using dummy-coding. In this case we set the reference category to online.\n\nfit &lt;- glm(solved ~ class, family = poisson(link = \"log\"), data = dat)\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = solved ~ class, family = poisson(link = \"log\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.6647  -0.5487  -0.1371   0.7294   2.8693  \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)    2.34564    0.04377  53.592  &lt; 2e-16 ***\n#&gt; classinperson  0.36772    0.05694   6.458 1.06e-10 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 148.45  on 99  degrees of freedom\n#&gt; Residual deviance: 106.04  on 98  degrees of freedom\n#&gt; AIC: 543.74\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-3",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#parameters-intepretation---categorical-variable-3",
    "title": "Poisson GLM",
    "section": "Parameters intepretation - Categorical variable",
    "text": "Parameters intepretation - Categorical variable\n\nSimilarly to the previous example, the intercept is the expected number of solved exercises when the class is 0. Thus the expected number of solved exercises for online students.\nthe classinperson is the difference in log solved exercises between online and in person classes. In the response scale is the expected increase in the ratio of solved exercises. People doing in person classes solve 144.44% of the exercises of people doing online classes\n\n\nc(coef(fit)[\"(Intercept)\"], exp(coef(fit)[\"(Intercept)\"]))\n\n#&gt; (Intercept) (Intercept) \n#&gt;    2.345645   10.440000\n\nc(coef(fit)[\"classinperson\"], exp(coef(fit)[\"classinperson\"]))\n\n#&gt; classinperson classinperson \n#&gt;     0.3677248     1.4444444"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-1",
    "title": "Poisson GLM",
    "section": "Overdispersion",
    "text": "Overdispersion\nOverdispersion concerns observing a greater variance compared to what would have been expected by the model.\nThe overdispersion \\(\\phi\\) can be estimated using Pearson Residuals:\n\\[\\begin{align*}\n\\hat \\phi = \\frac{\\sum_{i = 1}^n \\frac{(y_i - \\hat y_i)^2}{\\hat y_i}}{n - p - 1}\n\\end{align*}\\]\nWhere the numerator is the sum of squared Pearson residuals, \\(n\\) is the number of observations and \\(k\\) the number of predictors. For standard Binomial and Poisson models \\(\\phi = 1\\)."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-2",
    "title": "Poisson GLM",
    "section": "Overdispersion",
    "text": "Overdispersion\nIf the model is correctly specified for binomial and poisson models the ratio is equal to 1, of the ratio is \\(&gt; 1\\) there is evidence for overdispersion. In practical terms, if the residual deviance is higher than the residuals degrees of freedom, there is evidence for overdispersion.\n\nP &lt;- sum(residuals(fit, type = \"pearson\")^2)\nP / df.residual(fit) # nrow(fit$dat) - length(fit_p$coefficients)\n\n#&gt; [1] 1.089825"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#testing-overdispersion",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#testing-overdispersion",
    "title": "Poisson GLM",
    "section": "Testing overdispersion",
    "text": "Testing overdispersion\nTo formally test for overdispersion i.e. testing if the ratio is significantly different from 1 we can use the performance::check_overdispersion() function.\n\nperformance::check_overdispersion(fit)\n\n#&gt; # Overdispersion test\n#&gt; \n#&gt;        dispersion ratio =   1.090\n#&gt;   Pearson's Chi-Squared = 106.803\n#&gt;                 p-value =   0.255"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-plot",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-plot",
    "title": "Poisson GLM",
    "section": "Overdispersion plot",
    "text": "Overdispersion plot\nPearson residuals are defined as:\n\\[\\begin{align*}\nr_p = \\frac{y_i - \\hat y_i}{\\sqrt{V(\\hat y_i)}} \\\\\nV(\\hat y_i) = \\sqrt{\\hat y_i}\n\\end{align*}\\]\nRemember that the mean and the variance are the same in Poisson models. If the model is correct, the standardized residuals should be normally distributed with mean 0 and variance 1."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-plot-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#overdispersion-plot-1",
    "title": "Poisson GLM",
    "section": "Overdispersion plot",
    "text": "Overdispersion plot"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#variance-mean-relationship",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#variance-mean-relationship",
    "title": "Poisson GLM",
    "section": "Variance-mean relationship",
    "text": "Variance-mean relationship\nThe overdispersion can be expressed also in terms of variance-mean ratio. In fact, when the ratio is 1, there is no evidence of overdispersion."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#causes-of-overdispersion",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#causes-of-overdispersion",
    "title": "Poisson GLM",
    "section": "Causes of overdispersion",
    "text": "Causes of overdispersion\nThere could be multiple causes for overdispersion:\n\nthe phenomenon itself cannot be modelled with a Poisson distribution\n\noutliers or anomalous obervations that increases the observed variance\n\nmissing important variables in the model"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#outliers-or-anomalous-data",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#outliers-or-anomalous-data",
    "title": "Poisson GLM",
    "section": "Outliers or anomalous data",
    "text": "Outliers or anomalous data\nThis (simulated) dataset contains \\(n = 30\\) observations coming from a poisson model in the form \\(y = 1 + 2x\\) and \\(n = 7\\) observations coming from a model \\(y = 1 + 10x\\)."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#outliers-or-anomalous-data-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#outliers-or-anomalous-data-1",
    "title": "Poisson GLM",
    "section": "Outliers or anomalous data",
    "text": "Outliers or anomalous data\nClearly the sum of squared pearson residuals is inflated by these values producing more variance compared to what should be expected.\n\nc(mean = mean(datout$y), var = var(datout$y)) # mean and variance should be similar\n\n#&gt;     mean      var \n#&gt; 2.756757 6.689189\n\nperformance::check_overdispersion(fit)\n\n#&gt; # Overdispersion test\n#&gt; \n#&gt;        dispersion ratio =  1.515\n#&gt;   Pearson's Chi-Squared = 53.019\n#&gt;                 p-value =  0.026"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nLet’s imagine to analyze again the dataset with the number of solved exercises. We have the effect of the studyh variable. In addition we have the effect of the class variable, without interaction.\n\n\n\n\n\n\n\nid\n      class\n      studyh\n      class_c\n      lp\n      solved\n    \n\n\n1\nonline\n82\n0\n22.613\n24\n\n\n2\ninperson\n65\n1\n47.734\n50\n\n\n3\nonline\n12\n0\n11.268\n15\n\n\n4\ninperson\n54\n1\n42.785\n50\n\n\n...\n...\n...\n...\n...\n...\n\n\n37\nonline\n85\n0\n23.298\n35\n\n\n38\ninperson\n55\n1\n43.213\n43\n\n\n39\nonline\n80\n0\n22.167\n26\n\n\n40\ninperson\n77\n1\n53.788\n54"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-1",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nWe can also have a look at the data:"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-2",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nNow let’s fit the model considering only studyh and ignoring the group:\n\nfit &lt;- glm(solved ~ studyh, data = dat, family = poisson(link = \"log\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = solved ~ studyh, family = poisson(link = \"log\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -5.047  -2.247  -0.107   2.214   3.246  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) 2.692791   0.068831   39.12   &lt;2e-16 ***\n#&gt; studyh      0.013624   0.001063   12.82   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 417.99  on 39  degrees of freedom\n#&gt; Residual deviance: 243.99  on 38  degrees of freedom\n#&gt; AIC: 451.39\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-3",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-3",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nEssentially, we are fitting an average relationship across groups but the model ignore that the two groups differs, thus the observed variance is definitely higher because we need two separate means to explain the class effect."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-4",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-4",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nBy fitting the appropriate model, the overdispersion is no longer a problem:\n\nfit2 &lt;- glm(solved ~ studyh + class, data = dat, family = poisson(link = \"log\"))\nsummary(fit2)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = solved ~ studyh + class, family = poisson(link = \"log\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -1.93006  -0.48422   0.00417   0.43834   1.97814  \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   2.275159   0.078239   29.08   &lt;2e-16 ***\n#&gt; studyh        0.010895   0.001068   10.21   &lt;2e-16 ***\n#&gt; classinperson 0.907365   0.065373   13.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 417.993  on 39  degrees of freedom\n#&gt; Residual deviance:  29.022  on 37  degrees of freedom\n#&gt; AIC: 238.41\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-5",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-5",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\n\nperformance::check_overdispersion(fit)\n\n#&gt; # Overdispersion test\n#&gt; \n#&gt;        dispersion ratio =   6.115\n#&gt;   Pearson's Chi-Squared = 232.369\n#&gt;                 p-value = &lt; 0.001\n\n\n\nperformance::check_overdispersion(fit2)\n\n#&gt; # Overdispersion test\n#&gt; \n#&gt;        dispersion ratio =  0.777\n#&gt;   Pearson's Chi-Squared = 28.738\n#&gt;                 p-value =  0.832"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-6",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#missing-important-variables-in-the-model-6",
    "title": "Poisson GLM",
    "section": "Missing important variables in the model",
    "text": "Missing important variables in the model\nAlso the residuals plots clearly improved after including all relevant predictors:"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion",
    "title": "Poisson GLM",
    "section": "Why worring about overdispersion?",
    "text": "Why worring about overdispersion?\nBefore analyzing the two main strategies to deal with overdispersion, it is important to understand why it is very problematic.\n\nDespite the estimated parameters (\\(\\beta\\)) are not affected by overdispersion, standard errors are underestimated. The underestimation increase as the severity of the overdispersion increase.\nUnderestimated standard errors produce (biased) higher \\(z\\) scores inflating the type-1 error (i.e., lower p values)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion-1",
    "title": "Poisson GLM",
    "section": "Why worring about overdispersion?",
    "text": "Why worring about overdispersion?\nThe estimated overdispersion of fit is ~ 6.4208936. The summary() function in R has a dispersion argument to check how model parameters are affected.\n\nphi &lt;- fit$deviance / fit$df.residual\nsummary(fit, dispersion = 1)$coefficients # the default\n\n#&gt;               Estimate  Std. Error  z value     Pr(&gt;|z|)\n#&gt; (Intercept) 2.69279058 0.068830558 39.12202 0.000000e+00\n#&gt; studyh      0.01362422 0.001062724 12.82009 1.265575e-37\n\nsummary(fit, dispersion = 2)$coefficients # assuming phi = 2\n\n#&gt;               Estimate  Std. Error   z value      Pr(&gt;|z|)\n#&gt; (Intercept) 2.69279058 0.097341109 27.663447 1.923115e-168\n#&gt; studyh      0.01362422 0.001502919  9.065171  1.244091e-19\n\nsummary(fit, dispersion = phi)$coefficients # the appropriate\n\n#&gt;               Estimate  Std. Error   z value     Pr(&gt;|z|)\n#&gt; (Intercept) 2.69279058 0.174413070 15.439156 8.926080e-54\n#&gt; studyh      0.01362422 0.002692888  5.059333 4.207258e-07"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#why-worring-about-overdispersion-2",
    "title": "Poisson GLM",
    "section": "Why worring about overdispersion?",
    "text": "Why worring about overdispersion?\nBy using multiple values for \\(\\phi\\) we can see the impact on the standard error. \\(\\phi = 1\\) is what is assumed by the Poisson model."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#dealing-with-overdispersion-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#dealing-with-overdispersion-1",
    "title": "Poisson GLM",
    "section": "Dealing with overdispersion",
    "text": "Dealing with overdispersion\nIf all the variables are included and there are no outliers, the phenomenon itself contains more variability compared to what predicted by the Poisson. There are two main approaches to deal with the situation:\n\n\nquasi-poisson model\npoisson-gamma model AKA negative-binomial model"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model",
    "title": "Poisson GLM",
    "section": "Quasi-poisson model",
    "text": "Quasi-poisson model\nThe quasi-poisson model is essentially a poisson model that estimate the \\(\\phi\\) parameter and adjust the standard errors accordingly. Again, assuming to fit the studyh only model (with overdispersion):\n\nfit &lt;- glm(solved ~ studyh, data = dat, family = quasipoisson(link = \"log\"))\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = solved ~ studyh, family = quasipoisson(link = \"log\"), \n#&gt;     data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;    Min      1Q  Median      3Q     Max  \n#&gt; -5.047  -2.247  -0.107   2.214   3.246  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 2.692791   0.170209  15.821  &lt; 2e-16 ***\n#&gt; studyh      0.013624   0.002628   5.184 7.46e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for quasipoisson family taken to be 6.115055)\n#&gt; \n#&gt;     Null deviance: 417.99  on 39  degrees of freedom\n#&gt; Residual deviance: 243.99  on 38  degrees of freedom\n#&gt; AIC: NA\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model-1",
    "title": "Poisson GLM",
    "section": "Quasi-poisson model",
    "text": "Quasi-poisson model\nThe quasi-poisson model estimates the same parameters (\\(\\beta\\)) and adjust standard errors. Notice that using summary(dispersion = ) is the same as fitting a quasi-Poisson model and using the estimated \\(\\phi\\).\nThe quasi-poisson model is useful because it is a very simple way to deal with overdispersion.\nThe variance (\\(V(\\mu)\\)) of the Poisson model is no longer \\(\\mu\\) but \\(V(\\mu) = \\mu\\phi\\). When \\(\\phi\\) is close to 1, the quasi-Poisson model reduced to a standard Poisson model"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#quasi-poisson-model-2",
    "title": "Poisson GLM",
    "section": "Quasi-poisson model",
    "text": "Quasi-poisson model\nHere we compare the expected variance (\\(\\pm 2\\sqrt{V(y_i)}\\))1 of the Quasi-Poisson and Poisson models:\n\n\n\n\n\n\n\n\nThis is just an example that did not take into account the link function (notice that the yellow line goes a little bit under 0)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#problems-of-quasi--model",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#problems-of-quasi--model",
    "title": "Poisson GLM",
    "section": "Problems of Quasi-* model",
    "text": "Problems of Quasi-* model\nThe main problem of quasi-* models is that they are not a specific distribution family and there is not a likelihood function. For this reason, we cannot perform model comparison the standard AIC/BIC. See (Ver Hoef & Boveng, 2007) for an overview."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nA negative binomial model is different probability distribution with two parameters: the mean as in standard poisson model and the dispersion parameter. Similarly to the quasi-poisson model it estimates a dispersion parameter.\nPractically the negative-binomial model can be considered as a hierarchical model:\n\\[\\begin{align*}\ny_i \\sim Poisson(\\lambda_i) \\\\\n\\lambda_i \\sim Gamma(\\mu, \\frac{1}{\\theta})\n\\end{align*}\\]\nIn this way, the Gamma distribution regulate the \\(\\lambda\\) variability that otherwise is fixed to a single value in the Poisson model."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-1",
    "title": "Poisson GLM",
    "section": "Negative-binomial model1\n",
    "text": "Negative-binomial model1\n\nThe Poisson-Gamma mixture can be expressed as:\n\\[\np(y; \\mu, \\theta) = \\frac{\\Gamma(y + \\theta)}{\\Gamma(\\theta)\\Gamma(y + 1)}\\left(\\frac{\\mu}{\\mu + \\theta}\\right)^{y} \\left(\\frac{\\theta}{\\mu + \\theta}\\right)^\\theta\n\\]\nWhere \\(\\theta\\) is the overdispersion parameter, \\(\\Gamma()\\) is the gamma function. The mean is \\(\\mu\\) and the variance is \\(\\mu + \\frac{\\mu^2}{\\theta}\\). The \\(\\theta\\) is the inverse of the overdispersion in terms that as \\(1/\\theta\\) increase the data are more overdispersed. As \\(1/\\theta\\) approaches 0, the negative-binomial reduced to a Poisson model.\n\nThere are multiple parametrizations of the negative-binomial distribution. The one used by MASS is in terms of poisson-gamma mixture see https://mc-stan.org/docs/2_20/functions-reference/nbalt.html"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-2",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nCompared to the Poisson model, the negative binomial allows for overdispersion estimating the parameter \\(\\theta\\) and compared to the quasi-poisson model the variance is not a linear increase of the mean (\\(V(\\mu) = \\theta\\mu\\)) but have a quadratic relationship \\(V(\\mu) = \\mu + \\mu^2/\\theta\\)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-3",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-3",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nWe can use the MASS package to implement the negative binomial distribution using the MASS::rnegbin():"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-4",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-4",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nThe \\(\\theta\\) parameter is the estimated dispersion. To note, is not the same as \\(\\phi\\) in the quasi-poisson model. As \\(\\theta\\) increase, the overdispersion is reduced and the model is similar to a standard Poisson model."
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-5",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-5",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nFor fitting a negative-binomial model we cannot use the glm function but we need to use the MASS::glm.nb() function. The syntax is almost the same but we do not need to specify the family because this function only fit negative-binomial models. Let’s simulate some data coming from a negative binomial distribution with \\(\\theta = 10\\)\n\nCodetheta &lt;- 10\nn &lt;- 100\nb0 &lt;- 10\nb1 &lt;- 5\ndat &lt;- sim_design(n, nx = list(x = runif(n)))\ndat$lp &lt;- with(dat, exp(log(b0) + log(b1)*x))\ndat$y &lt;- with(dat, MASS::rnegbin(n, lp, theta))\n\n\n\n\n\n\n\n\n\nid\n      x\n      y\n    \n\n\n1\n0.427\n11\n\n\n2\n0.365\n13\n\n\n...\n...\n...\n\n\n99\n0.966\n41\n\n\n100\n0.154\n12"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-6",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-6",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-7",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-7",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nThen we can fit the model:\n\nfit_nb &lt;- MASS::glm.nb(y ~ x, data = dat)\nsummary(fit_nb)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = y ~ x, data = dat, init.theta = 11.32327809, \n#&gt;     link = log)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.7104  -0.7507  -0.1075   0.6324   2.4510  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.17693    0.08282   26.28   &lt;2e-16 ***\n#&gt; x            1.82119    0.13811   13.19   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(11.3233) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 273.823  on 99  degrees of freedom\n#&gt; Residual deviance:  98.035  on 98  degrees of freedom\n#&gt; AIC: 696.14\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  11.32 \n#&gt;           Std. Err.:  2.38 \n#&gt; \n#&gt;  2 x log-likelihood:  -690.143"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-8",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-8",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nHere we compare the expected variance (\\(\\pm 2\\sqrt{V(y_i)}\\)) of the Negative binomial and Poisson models:"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-9",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-9",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nHere we compare the expected variance (\\(\\pm 2\\sqrt{V(y_i)}\\)) of the three models:"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-10",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-10",
    "title": "Poisson GLM",
    "section": "Negative-binomial model",
    "text": "Negative-binomial model\nWe can compare the coefficients fitting the Poisson, the Quasi-Poisson and the Negative-binomial model:\n\nfit_p &lt;- glm(y ~ x, data = dat, family = poisson(link = \"log\"))\nfit_qp &lt;- glm(y ~ x, data = dat, family = quasipoisson(link = \"log\"))\ncar::compareCoefs(fit_nb, fit_p, fit_qp)\n\n#&gt; Calls:\n#&gt; 1: MASS::glm.nb(formula = y ~ x, data = dat, init.theta = 11.32327809, link \n#&gt;   = log)\n#&gt; 2: glm(formula = y ~ x, family = poisson(link = \"log\"), data = dat)\n#&gt; 3: glm(formula = y ~ x, family = quasipoisson(link = \"log\"), data = dat)\n#&gt; \n#&gt;             Model 1 Model 2 Model 3\n#&gt; (Intercept)  2.1769  2.2043  2.2043\n#&gt; SE           0.0828  0.0534  0.0970\n#&gt;                                    \n#&gt; x            1.8212  1.7723  1.7723\n#&gt; SE           0.1381  0.0802  0.1456\n#&gt;"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-nb-vs-quasi-poisson-qp",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#negative-binomial-model-nb-vs-quasi-poisson-qp",
    "title": "Poisson GLM",
    "section": "Negative-binomial model (NB) vs Quasi-poisson (QP)",
    "text": "Negative-binomial model (NB) vs Quasi-poisson (QP)\n\nThe NB has the likelihood function thus AIC, LRT and other likelihood-based metrics works compared to the QP\nThe NB assume a different mean-variance relationship thus estimated coefficients could be different to the P where QP produce the same estimates.\nBoth NB and QP estimate higher standard errors in the presence of overdispersion\nIf there is evidence of overdispersion, the important is to fit a model that take into account it"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#simulate-nb-data-extra",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#simulate-nb-data-extra",
    "title": "Poisson GLM",
    "section": "Simulate NB data #extra1\n",
    "text": "Simulate NB data #extra1\n\nIf you want to try to simulate NB data you can use the MASS::rnegbin(n, mu, theta) function or the rnb(n, mu, vmr) custom function that could be more intuitive because requires \\(\\mu\\) (i.e., the mean) and vmr that is the desired variance-mean ratio. Using message = TRUE it will tells the \\(\\theta\\) value:\n\ny &lt;- rnb(1e5, mu = 10, vmr = 7, message = TRUE)\n\n#&gt; y ~ NegBin(mu = 10, theta = 1.67), var = 70.00, vmr = 7.00\n\nnprint(mu = mean(y), v = var(y), vmr = var(y) / mean(y))\n\n#&gt;     mu      v    vmr \n#&gt;  9.958 69.569  6.986\n\n\nUsing vmr = 1 it will use the rpois() function"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#simulate-nb-data-extra-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#simulate-nb-data-extra-1",
    "title": "Poisson GLM",
    "section": "Simulate NB data #extra\n",
    "text": "Simulate NB data #extra\n\nYou can also use the theta parameter within the rnb function. As \\(\\theta\\) increase the overdispersion is reduced. Thus you can think the inverse of \\(1/\\theta\\) as an index of overdispersion.\n\ny &lt;- rnb(1e5, mu = 10, theta = 10, message = TRUE)\ny &lt;- rnb(1e5, mu = 10, theta = 5, message = TRUE)\ny &lt;- rnb(1e5, mu = 10, theta = 1, message = TRUE)"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#deviance-based-pseudo-r2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#deviance-based-pseudo-r2",
    "title": "Poisson GLM",
    "section": "Deviance based pseudo-\\(R^2\\)\n",
    "text": "Deviance based pseudo-\\(R^2\\)\n\nThe Deviance based pseudo-\\(R^2\\) is computed from the ratio between the residual deviance and the null deviance1:\n\\[\nR^2 = 1 - \\frac{D_{current}}{D_{null}}\n\\] \n\n1 - fit_p$deviance/fit_p$null.deviance\n\n#&gt; [1] 0.6229535\n\n\nhttps://en.wikipedia.org/wiki/Pseudo-R-squared"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#mcfaddens-pseudo-r2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#mcfaddens-pseudo-r2",
    "title": "Poisson GLM",
    "section": "McFadden’s pseudo-\\(R^2\\)\n",
    "text": "McFadden’s pseudo-\\(R^2\\)\n\nThe McFadden’s pseudo-\\(R^2\\) compute the ratio between the log-likelihood of the intercept-only (i.e., null) model and the current model (McFadden, 1987):\n\\[\nR^2 = 1 - \\frac{\\log(\\mathcal{L_{current}})}{\\log(\\mathcal{L_{null}})}\n\\]\nThere is also the adjusted version that take into account the number of parameters of the model. In R can be computed manually or using the performance::r2_mcfadden():\n\nperformance::r2_mcfadden(fit_p)\n\n#&gt; # R2 for Generalized Linear Regression\n#&gt;        R2: 0.396\n#&gt;   adj. R2: 0.394"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#nagelkerkes-pseudo-r2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#nagelkerkes-pseudo-r2",
    "title": "Poisson GLM",
    "section": "Nagelkerke’s pseudo-\\(R^2\\)\n",
    "text": "Nagelkerke’s pseudo-\\(R^2\\)\n\nThe Cox and Snell’s (Cox & Snell, 1989) \\(R^2\\) is defined as:\n\\[\nR^2 = 1 - \\left(\\frac{\\mathcal{L_{null}}}{\\mathcal{L_{current}}}\\right)^{\\frac{2}{n}}\n\\]\nWhere Nagelkerke (Nagelkerke, 1991) provide a correction to set the range of values between 0 and 1.\n\nperformance::r2_nagelkerke(fit_p)\n\n#&gt; Nagelkerke's R2 \n#&gt;       0.9950421"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear",
    "title": "Poisson GLM",
    "section": "Interaction, Poisson vs Linear1\n",
    "text": "Interaction, Poisson vs Linear1\n\nLet’s simulate a GLM where there is a main effect but no interaction:\n\nCodedat &lt;- sim_design(200, nx = list(x = rnorm(200)), cx = list(g = c(\"a\", \"b\")))\ndat &lt;- sim_data(dat, exp(log(10) + log(1.5) * x + log(2) * g_c + 0 * g_c * x), model = \"poisson\")\ndat |&gt; \n  ggplot(aes(x = x, y = y, color = g)) +\n  geom_point() +\n  stat_smooth(method = \"glm\",\n              method.args = list(family = poisson()))\n\n\n\n\n\n\n\nThank to Enrico Toffalini for this great example"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear-1",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear-1",
    "title": "Poisson GLM",
    "section": "Interaction, Poisson vs Linear",
    "text": "Interaction, Poisson vs Linear\nLet’s fit a linear model and the Poisson GLM with the interaction term:\n\n\nPoisson GLM\nGaussian GLM\n\n\n\n\nfit_lm &lt;- lm(y ~ x * g, data = dat)\nsummary(fit_lm)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x * g, data = dat)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9594  -2.7974  -0.3394   2.4466  14.8487 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  11.0610     0.2941  37.603   &lt;2e-16 ***\n#&gt; x             3.9829     0.2979  13.371   &lt;2e-16 ***\n#&gt; g2            9.9577     0.4163  23.921   &lt;2e-16 ***\n#&gt; x:g2          4.0287     0.4347   9.269   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.116 on 396 degrees of freedom\n#&gt; Multiple R-squared:  0.7717, Adjusted R-squared:   0.77 \n#&gt; F-statistic: 446.3 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nfit_glm &lt;- glm(y ~ x * g, data = dat, family = poisson(link = \"log\"))\nsummary(fit_glm)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x * g, family = poisson(link = \"log\"), data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;      Min        1Q    Median        3Q       Max  \n#&gt; -2.61194  -0.71125  -0.05532   0.55622   2.75160  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.33573    0.02238 104.350   &lt;2e-16 ***\n#&gt; x            0.38286    0.02254  16.986   &lt;2e-16 ***\n#&gt; g2           0.64219    0.02765  23.226   &lt;2e-16 ***\n#&gt; x:g2         0.02549    0.02850   0.894    0.371    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1816.48  on 399  degrees of freedom\n#&gt; Residual deviance:  385.92  on 396  degrees of freedom\n#&gt; AIC: 2157.8\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear-2",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#interaction-poisson-vs-linear-2",
    "title": "Poisson GLM",
    "section": "Interaction, Poisson vs Linear",
    "text": "Interaction, Poisson vs Linear\nThe linear model is estimating a completely misleading interaction due to the mean-variance relationship of the Poisson distribution.\n\nCodelp_plot &lt;- dat |&gt; \n  select(id, g, x) |&gt; \n  add_predict(fit_glm, se.fit = TRUE) |&gt; \n  ggplot(aes(x = x, \n             y = fit, \n             fill = g,\n             ymin = fit - se.fit * 2, \n             ymax = fit + se.fit * 2)) +\n  geom_line() +\n  geom_ribbon(alpha = 0.5) +\n  ylab(\"y\") +\n  ggtitle(\"Linear Predictor\")\nlp_plot | plot(ggeffect(fit_glm, terms = c(\"x\", \"g\"))) | plot(ggeffect(fit_lm, terms = c(\"x\", \"g\")))"
  },
  {
    "objectID": "slides/03-poisson-glm/03-poisson-glm.html#references",
    "href": "slides/03-poisson-glm/03-poisson-glm.html#references",
    "title": "Poisson GLM",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nCox, D. R., & Snell, E. J. (1989). Analysis of binary data, second edition. CRC Press. https://play.google.com/store/books/details?id=0R8J71LCLXsC\n\n\nMcFadden, D. (1987). Regression-based specification tests for the multinomial logit model. Journal of Econometrics, 34, 63–82. https://doi.org/10.1016/0304-4076(87)90067-4\n\n\nNagelkerke, N. J. D. (1991). A note on a general definition of the coefficient of determination. Biometrika, 78, 691–692. https://doi.org/10.1093/biomet/78.3.691\n\n\nVer Hoef, J. M., & Boveng, P. L. (2007). Quasi-poisson vs. Negative binomial regression: How should we model overdispersed count data? Ecology, 88, 2766–2772. https://doi.org/10.1890/07-0043.1"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#github",
    "href": "slides/00-intro-course/00-intro-course.html#github",
    "title": "Generalized Linear Models",
    "section": "Github",
    "text": "Github\nThe material is available on Github You can find the slides, code, datasets and other stuff.\n\nstat-teaching.github.io/GLMphd"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#getting-started",
    "href": "slides/00-intro-course/00-intro-course.html#getting-started",
    "title": "Generalized Linear Models",
    "section": "Getting started",
    "text": "Getting started\n\nDownload the repository from Github\nUnzip the folder\nOpen the GLMphd.Rproj file"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-style",
    "href": "slides/00-intro-course/00-intro-course.html#r-style",
    "title": "Generalized Linear Models",
    "section": "R style",
    "text": "R style\nI use sometimes a coding style that is not common. I try to stay as close as possible to base R. But here some general patterns that you will see:\nAccessing functions within a package, if I don’t want to load it:\n\nMASS::mvrnorm()"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-style-1",
    "href": "slides/00-intro-course/00-intro-course.html#r-style-1",
    "title": "Generalized Linear Models",
    "section": "R style",
    "text": "R style\nEspecially in slides or quick exploratory analysis, extensive use of pipes |&gt; or %&gt;%\n\nas.character(round(mean(iris$Sepal.Length)))\n\n# equivalent but more clear\niris$Sepal.Length |&gt; \n  mean() |&gt; \n  round() |&gt; \n  as.character()"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-style-2",
    "href": "slides/00-intro-course/00-intro-course.html#r-style-2",
    "title": "Generalized Linear Models",
    "section": "R style",
    "text": "R style\nUse of the tidyverse package for data-manipulation using dplyr, tidyr, etc. Sometimes you will se a tibble object. It is only a dataframe with some extra features.\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise(Sepal.Length = mean(Sepal.Length))\n\n# A tibble: 3 × 2\n  Species    Sepal.Length\n  &lt;fct&gt;             &lt;dbl&gt;\n1 setosa             5.01\n2 versicolor         5.94\n3 virginica          6.59\n\n# in base R\naggregate(Sepal.Length ~ Species, iris, mean)\n\n     Species Sepal.Length\n1     setosa        5.006\n2 versicolor        5.936\n3  virginica        6.588"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-style-3",
    "href": "slides/00-intro-course/00-intro-course.html#r-style-3",
    "title": "Generalized Linear Models",
    "section": "R style",
    "text": "R style\nExtensive use of *apply like function (functional programming) to make iterations. In the examples I use for loops because because is more transparent.\n\nmeans &lt;- vector(mode = \"numeric\", length = ncol(mtcars))\nfor(i in 1:length(means)){\n  means[i] &lt;- mean(mtcars[[i]])\n}\nmeans\n\n [1]  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250\n [7]  17.848750   0.437500   0.406250   3.687500   2.812500\n\n# equivalent to\nsapply(mtcars, mean)\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb \n  0.437500   0.406250   3.687500   2.812500"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-style-4",
    "href": "slides/00-intro-course/00-intro-course.html#r-style-4",
    "title": "Generalized Linear Models",
    "section": "R style",
    "text": "R style\nFor plotting I use ggplot2. Is not super easy at the beginning but it pays off.\n\niris |&gt; \n  ggplot(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#r-studio-projects",
    "href": "slides/00-intro-course/00-intro-course.html#r-studio-projects",
    "title": "Generalized Linear Models",
    "section": "R Studio projects",
    "text": "R Studio projects\nIf you have trouble understanding and using the working directory and setwd() I highly suggest you to use the R Studio projects.\nThe *.Rproj can be created in a folder and when you open the file R Studio will open an R session setting the working directory automatically.\nAll the paths will be relative to the *.Rproj file. You can move the folder or share it with other people without worrying about file location."
  },
  {
    "objectID": "slides/00-intro-course/00-intro-course.html#contents",
    "href": "slides/00-intro-course/00-intro-course.html#contents",
    "title": "Generalized Linear Models",
    "section": "Contents",
    "text": "Contents\n\nOverview about GLM and why they are useful\nBinomial, Poisson and Gamma GLM\n\nFitting the model\nParameters interpretation\nDiagnostic\n\n\nSimulating data\n\nUnderstanding the data generation process\nPower analysis\n(for fun 😁)\n…"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#monte-carlo-simulations",
    "href": "slides/05-extra-glm/binomial-glm-power.html#monte-carlo-simulations",
    "title": "Simulating GLM",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\n\nMonte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#general-workflow",
    "href": "slides/05-extra-glm/binomial-glm-power.html#general-workflow",
    "title": "Simulating GLM",
    "section": "General Workflow",
    "text": "General Workflow\nDespite the specific applications, Monte Carlo simulations follows a similar pattern:\n\nDefine the data generation process (DGP)\nUse random numbers sampling to generate data according to assumptions\n\nCalculate a statistics, fit a model or do some computations on the generated data\n\nRepeat 2-3 several times (e.g., 10000)\nGet a summary of the results"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r",
    "href": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r",
    "title": "Simulating GLM",
    "section": "Random numbers in R",
    "text": "Random numbers in R\nIn R there are several functions to generate random numbers and they are linked to specific probability distributions. You can type ?family() to see available distributions for glm.\n\n?family"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-1",
    "title": "Simulating GLM",
    "section": "Random numbers in R",
    "text": "Random numbers in R\nIn fact, there are other useful distributions not listed in ?family(), because they are not part of glm. For example the beta or the unif (uniform) distributions. Use ?Distributions for a complete list:\n\n?Distributions"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-2",
    "href": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-2",
    "title": "Simulating GLM",
    "section": "Random numbers in R",
    "text": "Random numbers in R\nHowever, it is always possible to include other distributions with packages. For example the MASS::mvrnorm() implement the multivariate normal distribution or the extraDistr::rhcauchy() for a series of truncated distributions."
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-3",
    "href": "slides/05-extra-glm/binomial-glm-power.html#random-numbers-in-r-3",
    "title": "Simulating GLM",
    "section": "Random numbers in R",
    "text": "Random numbers in R\nThe general pattern is always the same. There are 4 functions called r, p, q and d combined with a distribution e.g. norm creating several utilities. For example, rnorm() generate number from a normal distribution.\n\nx &lt;- rnorm(1e3)\nhist(x)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#why-monte-carlo-simulations-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#why-monte-carlo-simulations-1",
    "title": "Simulating GLM",
    "section": "Why Monte Carlo Simulations?",
    "text": "Why Monte Carlo Simulations?\nMonte Carlo simulations are used for several purposes:\n\nSolve computations impossible or hard to do analytically\nEstimate the statistical power, type-1 error, type-M error etc."
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error",
    "title": "Simulating GLM",
    "section": "Example: standard error",
    "text": "Example: standard error\nA classical example is estimating the standard error (SE) of a statistics. For example, we know that the SE of a sample mean is:\n\\[\n\\sigma_\\overline x = \\frac{s_x}{\\sqrt{n_x}}\n\\]\nWhere \\(s_x\\) is the standard deviation of \\(x\\) and \\(n_x\\) is the sample size.\n\nx &lt;- rnorm(100, mean = 10, sd = 5)\nmean(x) # mean\n\n#&gt; [1] 10.125\n\nsd(x) / sqrt(length(x)) # se\n\n#&gt; [1] 0.4699393\n\n5 / sqrt(length(x)) # analytically, assuming s = 5\n\n#&gt; [1] 0.5"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-1",
    "title": "Simulating GLM",
    "section": "Example: standard error",
    "text": "Example: standard error\nHowever we are not good in deriving the SE analytically. We know that the SE is the standard deviation of the sampling distribution of a statistics.\n\nThe sampling distribution is the distribution obtained by calculating the statistics (in this case the mean) on all possible (or a very big number) samples of size \\(n\\).\n\n\nWe can solve the problems creating a very simple Monte Carlo Simulation"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-2",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-2",
    "title": "Simulating GLM",
    "section": "Example: standard error",
    "text": "Example: standard error\nWe simulate 10000 samples of size \\(n\\) by a normal distribution with \\(\\mu = 10\\) and \\(\\sigma = 5\\). We calculate the mean \\(\\overline x\\) for each iteration and then we calculate the standard deviation of the vectors of means.\n\nnsim &lt;- 1e4\nmx &lt;- rep(0, 1e4)\n\nfor(i in 1:nsim){\n  x &lt;- rnorm(100, 10, 5)\n  mx[i] &lt;- mean(x)\n}"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-3",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-standard-error-3",
    "title": "Simulating GLM",
    "section": "Example: standard error",
    "text": "Example: standard error\n\nhist(mx)\n\nsd(mx) # the standard error\n\n#&gt; [1] 0.4948425"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#workflow",
    "href": "slides/05-extra-glm/binomial-glm-power.html#workflow",
    "title": "Simulating GLM",
    "section": "Workflow",
    "text": "Workflow\nThe general workflow is the following:\n\nDefine the experimental design:\n\nhow many variables?\nhow many participants/trials?\nwhich type of variables (categorical, numerical)?\n\n\nDefine the probability distribution of the response variable:\n\nGaussian\nPoisson\nBinomial\n…\n\n\nCreate the model matrix and define all parameters of the simulation: \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), etc.\nCompute the linear predictors \\(\\eta\\) on the link function scale\nApply the inverse of the link function \\(g^{-1}(\\eta)\\) obtaining values on the original scale\nSimulate the response variable by sampling from the appropriate distribution\nFit the appropriate model and check the result\nIn case of estimating statistical properties (e.g., power) repeat the simulation (1-7) several times (e.g., 10000) and summarize the results"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nLet’s simulate a simple linear model (i.e., GLM with a Gaussian random component and identity link function).\n\\[\n\\hat y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\n\\]\nIn this example we have:\n\n1 predictor \\(x\\) that is numeric\n1 response variable \\(y\\) that is numeric\n3 parameters: \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma_{\\epsilon}\\)\n\nGaussian random component and identity link function"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-1",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\n\nn &lt;- 100\nx &lt;- rnorm(n)\n\ndat &lt;- data.frame(x)\n\nX &lt;- model.matrix(~x, data = dat)\nhead(X)\n\n#&gt;   (Intercept)          x\n#&gt; 1           1 -1.1056094\n#&gt; 2           1  1.9433473\n#&gt; 3           1 -0.1645113\n#&gt; 4           1 -0.6830828\n#&gt; 5           1 -0.5124175\n#&gt; 6           1  0.2247971"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-2",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-2",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nThen let’s define the model parameters and compute the predicted values.\n\nb0 &lt;- 0\nb1 &lt;- 0.6\nsigma2 &lt;- 1\n\ndat$lp &lt;- b0 + b1*x\n\nplot(dat$x, dat$lp)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-3",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-3",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nNow, we are fitting a model with a Gaussian random component and an identity link function. Thus using the \\(g\\) function has no effect.\n\nfam &lt;- gaussian(link = \"identity\")\ndat$lp &lt;- fam$linkinv(dat$lp)\ndat$y &lt;- rnorm(nrow(dat), dat$lp, sqrt(sigma2))\nplot(dat$x, dat$y)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-4",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-4",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nNow we can fit the appropriate model using the glm function:\n\nfit &lt;- glm(y ~ x, family = gaussian(link = \"identity\"), data = dat)\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = gaussian(link = \"identity\"), data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.0988  -0.7783   0.0595   0.5912   3.1869  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.12112    0.09892  -1.224    0.224    \n#&gt; x            0.56165    0.09591   5.856 6.33e-08 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for gaussian family taken to be 0.9761855)\n#&gt; \n#&gt;     Null deviance: 129.143  on 99  degrees of freedom\n#&gt; Residual deviance:  95.666  on 98  degrees of freedom\n#&gt; AIC: 285.36\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-5",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-5",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nA faster way, especially with many parameters is using matrix multiplication between the \\(X\\) matrix and the vector of coefficients:\n\\[\\begin{equation}\n\\boldsymbol{y} =\n\\begin{bmatrix}\n1 & x_{1} \\\\\n1 & x_{2} \\\\\n1 & x_{3} \\\\\n1 & x_{4} \\\\\n\\vdots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\epsilon_3 \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-6",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-6",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\n\nB &lt;- c(b0, b1)\ny &lt;- X %*% B + rnorm(nrow(dat), 0, sqrt(sigma2))\nplot(dat$x, y)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-7",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-7",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nNow let’s add another effect, for example a binary variable group:\n\ngroup &lt;- c(\"a\", \"b\")\nx &lt;- rnorm(n*2)\n\ndat &lt;- data.frame(\n  x = x,\n  group = rep(group, each = n)\n)\n\nX &lt;- model.matrix(~ group + x, data = dat)\nhead(X)\n\n#&gt;   (Intercept) groupb          x\n#&gt; 1           1      0 -0.3301125\n#&gt; 2           1      0 -0.8321570\n#&gt; 3           1      0 -0.8517284\n#&gt; 4           1      0  0.1449133\n#&gt; 5           1      0 -1.7173194\n#&gt; 6           1      0 -0.4896183"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-8",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-8",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nNow the model matrix has another column groupb that is the dummy-coded version of the group variable. Now let’s set the parameters:\n\nb0 &lt;- 0 # y value when group = \"a\" and x = 0 \nb1 &lt;- 1 # difference between groups\nb2 &lt;- 0.6 # slope of the group\nsigma2 &lt;- 1 # residual variance\n\nThen we can compute the formula adding the new parameters:\n\ndat$y &lt;- b0 + b1 * ifelse(dat$group == \"a\", 0, 1) + b2 * dat$x + rnorm(nrow(dat), 0, sqrt(sigma2))"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-9",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-9",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\n\ndat |&gt; \n  ggplot(aes(x = x, y = y, color = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              formula = y ~ x,\n              se = FALSE)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-10",
    "href": "slides/05-extra-glm/binomial-glm-power.html#example-with-a-linear-model-10",
    "title": "Simulating GLM",
    "section": "Example with a linear model",
    "text": "Example with a linear model\nThe same using matrix formulation:\n\nB &lt;- c(b0, b1, b2)\ndat$y &lt;- X %*% B + rnorm(nrow(dat), 0, sqrt(sigma2))\n\nThen we can fit the model:\n\nfit &lt;- lm(y ~ group + x, data = dat)\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ group + x, data = dat)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -2.65835 -0.70263  0.02863  0.75388  1.89188 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -0.04132    0.10170  -0.406    0.685    \n#&gt; groupb       1.05955    0.14386   7.365 4.72e-12 ***\n#&gt; x            0.70415    0.07123   9.885  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.017 on 197 degrees of freedom\n#&gt; Multiple R-squared:  0.4302, Adjusted R-squared:  0.4244 \n#&gt; F-statistic: 74.37 on 2 and 197 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#generalized-linear-models-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#generalized-linear-models-1",
    "title": "Simulating GLM",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nThe workflow presented before can be applied to GLMs. The only extra steps is performing the link-function transformation.\nWe simulate data fixing coefficients and computing \\(\\eta\\), then we apply the inverse of the link function (4 and 5 from the workflow slide)."
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nLet’s simulate the effect of a continuous predictor on the probability of success, thus using a Binomial model.\n\nns &lt;- 100 # sample size\nx &lt;- runif(ns) # x predictor\nb0 &lt;- qlogis(0.001) # probability of correct response when x is 0\nb1 &lt;- 10 # increase in the logit of a correct response by unit increase in x\n\ndat &lt;- data.frame(id = 1:ns, x = x)\nhead(dat)\n\n#&gt; # A tibble: 6 × 2\n#&gt;      id      x\n#&gt;   &lt;int&gt;  &lt;dbl&gt;\n#&gt; 1     1 0.248 \n#&gt; 2     2 0.264 \n#&gt; 3     3 0.0202\n#&gt; 4     4 0.478 \n#&gt; 5     5 0.796 \n#&gt; 6     6 0.252"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example-1",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nLet’s compute the \\(\\eta\\) by doing the linear combination of predictors and coefficients:\n\ndat$lp &lt;- b0 + b1 * dat$x\nggplot(dat, aes(x = x, y = lp)) +\n  geom_line() +\n  ylab(latex(\"\\\\eta\")) +\n  xlab(\"x\")"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example-2",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example-2",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nThen we can compute \\(g^{-1}(\\eta)\\) applying the inverse of the link function. Let’s use the logit:\n\nfam &lt;- binomial(link = \"logit\")\ndat$p &lt;- fam$linkinv(dat$lp)\nggplot(dat, aes(x = x, y = p)) +\n  geom_line() +\n  ylim(c(0, 1)) +\n  ylab(latex(\"p\")) +\n  xlab(\"x\")"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example-3",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example-3",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nSo far we have the expected probability of success for each participant and \\(x\\), but we need to include the random component. We can use \\(p\\) or \\(g^{-1}(\\eta)\\) more generally to sample from the \\(\\mu\\) parameter of the probability distribution.\n\ndat$y &lt;- rbinom(n = nrow(dat), size = 1, prob = dat$p)\nhead(dat)\n\n#&gt; # A tibble: 6 × 5\n#&gt;      id      x    lp       p     y\n#&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     1 0.248  -4.42 0.0118      0\n#&gt; 2     2 0.264  -4.27 0.0138      0\n#&gt; 3     3 0.0202 -6.70 0.00122     0\n#&gt; 4     4 0.478  -2.13 0.106       0\n#&gt; 5     5 0.796   1.05 0.741       1\n#&gt; 6     6 0.252  -4.39 0.0122      0"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example-4",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example-4",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nNow we have simulated a vector of responses with the appropriate random component. We can plot the results.\n\ndat |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(position = position_jitter(height = 0.05)) +\n  stat_smooth(method = \"glm\", \n              method.args = list(family = fam),\n              se = FALSE)"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#glm-example-5",
    "href": "slides/05-extra-glm/binomial-glm-power.html#glm-example-5",
    "title": "Simulating GLM",
    "section": "GLM example",
    "text": "GLM example\nFinally we can fit the model and see if the parameters are estimated correctly. Of course, we know the true data generation process thus we are fitting the best model.\n\nfit &lt;- glm(y ~ x, data = dat, family = fam)\nsummary(fit)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x, family = fam, data = dat)\n#&gt; \n#&gt; Deviance Residuals: \n#&gt;     Min       1Q   Median       3Q      Max  \n#&gt; -2.1619  -0.2756  -0.1145   0.4282   2.0958  \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -7.476      1.529  -4.890 1.01e-06 ***\n#&gt; x             10.419      2.082   5.003 5.64e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 120.430  on 99  degrees of freedom\n#&gt; Residual deviance:  53.258  on 98  degrees of freedom\n#&gt; AIC: 57.258\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-1",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-1",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nOnce the data generation process and the model has been defined, the power analysis is straightforward.\nThe hardest part is fixing plausible values according to your knowledge and/or previous literature.\nFor example, there are methods to convert from odds ratio to Cohen’s \\(d\\) or other metrics.\nThe effectsize package is a great resource to understand and compute effect sizes.\n\nor &lt;- 1.5 # odds ratio\neffectsize::oddsratio_to_d(or)\n\n#&gt; [1] 0.2235446"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-2",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-2",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nWe can see the relationship between \\(d\\) and (log) Odds Ratio:"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-3",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-3",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nFor example we can a logistic regression with a binary predictor, fixing the effect size:\n\nn &lt;- 30 # number of subjects\nd &lt;- 0.5 # effect size in cohen's d\nor &lt;- effectsize::d_to_oddsratio(d) # this is beta1\nx &lt;- rep(c(\"a\", \"b\"), each = n)\nxc &lt;- ifelse(x == \"a\", 0, 1)\n\ndat &lt;- data.frame(x = x, xc = xc)\nb0 &lt;- qlogis(0.3) # probability of a\nb1 &lt;- log(or)\n\ndat$lp &lt;- b0 + b1 * dat$xc\ndat$y &lt;- rbinom(nrow(dat), 1, plogis(dat$lp))\n\nhead(dat)\n\n#&gt; # A tibble: 6 × 4\n#&gt;   x        xc     lp     y\n#&gt;   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 a         0 -0.847     0\n#&gt; 2 a         0 -0.847     0\n#&gt; 3 a         0 -0.847     1\n#&gt; 4 a         0 -0.847     0\n#&gt; 5 a         0 -0.847     0\n#&gt; 6 a         0 -0.847     0"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-4",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-4",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nClearly, we need to repeat the sampling process several times, store the results (e.g., the p-value of \\(\\beta_1\\)) and then compute the power.\n\nnsim &lt;- 1000\np &lt;- rep(0, nsim)\n\nfor(i in 1:nsim){\n  dat$y &lt;- rbinom(nrow(dat), 1, plogis(dat$lp))\n  fit &lt;- glm(y ~ x, data = dat, family = fam)\n  p[i] &lt;- summary(fit)$coefficients[\"xb\", \"Pr(&gt;|z|)\"]\n}\n\nmean(p &lt;= 0.05)\n\n#&gt; [1] 0.377"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-5",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-5",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nWith just one condition the power analysis is not really meaningful. We can compute the same for different sample sizes. Here my code is using a series of for loops but there could be a nicer implementation.\n\nns &lt;- c(30, 50, 100, 150)\n\npower &lt;- rep(0, length(ns))\n\nfor(i in 1:length(ns)){\n  p &lt;- rep(0, nsim)\n  for(j in 1:nsim){\n    dat &lt;- data.frame(id = 1:ns[i], x = rep(c(\"a\", \"b\"), each = ns[i]))\n    dat$xc &lt;- ifelse(dat$x == \"a\", 0, 1)\n    dat$lp &lt;- b0 + b1 * dat$xc\n    dat$y &lt;- rbinom(nrow(dat), 1, plogis(dat$lp))\n    fit &lt;- glm(y ~ x, data = dat, family = fam)\n    p[j] &lt;- summary(fit)$coefficients[\"xb\", \"Pr(&gt;|z|)\"]\n  }\n  power[[i]] &lt;- mean(p &lt;= 0.05)\n}"
  },
  {
    "objectID": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-6",
    "href": "slides/05-extra-glm/binomial-glm-power.html#power-analysis-6",
    "title": "Simulating GLM",
    "section": "Power analysis",
    "text": "Power analysis\nThen we can compute the results:\n\nplot(ns, power, type = \"b\", ylim = c(0, 1), pch = 19)"
  },
  {
    "objectID": "local/to-add.html",
    "href": "local/to-add.html",
    "title": "General",
    "section": "",
    "text": "General\n\ncss for code `` within bullet list\n\n\n\nIntro to GLM\n\nadd slide with tricks for the family object\n\n\n\nBinomial GLM\n\nintroduce the beroulli vs binomial model\ndifference between odds and probability\nadd some descriptive of the teddy child project\nda \\(\\pi\\) a \\(p\\) o \\(\\mu\\)\nadding the conversion to cohen’s d\nslide con lapply(predictions, plogis) metti anche esempio dove le differenze sono uguali in scala logit\nlittle bit more about the divide by 4 rule\ndivide by 4 rule as a special type of marginal effect\nmore on marginal effects\ncheck wald vs profile confidence interval\ncheck anova vs model comparison\nadd BIC/AIC weights\nsomething more about deviance and likelihood (maybe also the binomial example that is good)\nslide raw vs pearson residuals check the plots\ncheck binomial deviance residuals extra\nsomething more about hatvalues and check formula\n\nvedi libro della salvan sui residui\n\n\nPoisson\n\nvedi parametrizzazione della gamma negative binomial\ncheck R2\n\n\n\nGamma\n\ndefinisci meglio la parametrizzazione\nintepretazione dei parametri"
  }
]